{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80f05910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "from sklearn.model_selection import train_test_split,RepeatedStratifiedKFold,GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50981a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv(r\"Data\\UCI_Data\\final_X_train.txt\", delimiter=\",\",header=None)\n",
    "Y_train=pd.read_csv(r\"Data\\UCI_Data\\final_y_train.txt\", delimiter=\",\",header=None)\n",
    "data=X_train.copy(deep=True)\n",
    "data['class']=Y_train\n",
    "data=data.sample(frac=1,ignore_index=True)\n",
    "labels=data['class']\n",
    "data=data.drop('class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4dd281c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.328098</td>\n",
       "      <td>-0.016892</td>\n",
       "      <td>-0.105908</td>\n",
       "      <td>-0.657875</td>\n",
       "      <td>-0.545393</td>\n",
       "      <td>-0.568119</td>\n",
       "      <td>-0.692565</td>\n",
       "      <td>-0.555668</td>\n",
       "      <td>-0.567608</td>\n",
       "      <td>-0.470899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132243</td>\n",
       "      <td>-0.289747</td>\n",
       "      <td>-0.618739</td>\n",
       "      <td>-0.026148</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>-0.003853</td>\n",
       "      <td>-0.525921</td>\n",
       "      <td>0.070942</td>\n",
       "      <td>-0.022324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.342924</td>\n",
       "      <td>0.064407</td>\n",
       "      <td>0.116318</td>\n",
       "      <td>0.357233</td>\n",
       "      <td>0.428247</td>\n",
       "      <td>0.409752</td>\n",
       "      <td>0.331081</td>\n",
       "      <td>0.422234</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.510689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235170</td>\n",
       "      <td>0.294440</td>\n",
       "      <td>0.285924</td>\n",
       "      <td>0.327898</td>\n",
       "      <td>0.445712</td>\n",
       "      <td>0.595877</td>\n",
       "      <td>0.514740</td>\n",
       "      <td>0.434306</td>\n",
       "      <td>0.244521</td>\n",
       "      <td>0.310260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.558400</td>\n",
       "      <td>-0.885720</td>\n",
       "      <td>-1.006100</td>\n",
       "      <td>-1.000100</td>\n",
       "      <td>-0.999270</td>\n",
       "      <td>-1.005300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999280</td>\n",
       "      <td>-1.005400</td>\n",
       "      <td>-0.951790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.793830</td>\n",
       "      <td>-0.955590</td>\n",
       "      <td>-1.006300</td>\n",
       "      <td>-1.001000</td>\n",
       "      <td>-0.992450</td>\n",
       "      <td>-0.995690</td>\n",
       "      <td>-0.987830</td>\n",
       "      <td>-0.997540</td>\n",
       "      <td>-0.795520</td>\n",
       "      <td>-1.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.260400</td>\n",
       "      <td>-0.027013</td>\n",
       "      <td>-0.129390</td>\n",
       "      <td>-0.985930</td>\n",
       "      <td>-0.963715</td>\n",
       "      <td>-0.966993</td>\n",
       "      <td>-0.987775</td>\n",
       "      <td>-0.964815</td>\n",
       "      <td>-0.966835</td>\n",
       "      <td>-0.928032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007369</td>\n",
       "      <td>-0.500945</td>\n",
       "      <td>-0.826232</td>\n",
       "      <td>-0.120405</td>\n",
       "      <td>-0.240812</td>\n",
       "      <td>-0.554740</td>\n",
       "      <td>-0.450703</td>\n",
       "      <td>-0.800307</td>\n",
       "      <td>-0.062248</td>\n",
       "      <td>-0.135370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277880</td>\n",
       "      <td>-0.017045</td>\n",
       "      <td>-0.108480</td>\n",
       "      <td>-0.711575</td>\n",
       "      <td>-0.555900</td>\n",
       "      <td>-0.566125</td>\n",
       "      <td>-0.753750</td>\n",
       "      <td>-0.568120</td>\n",
       "      <td>-0.563235</td>\n",
       "      <td>-0.568920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148570</td>\n",
       "      <td>-0.323800</td>\n",
       "      <td>-0.695040</td>\n",
       "      <td>0.006337</td>\n",
       "      <td>0.027131</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>-0.005088</td>\n",
       "      <td>-0.676585</td>\n",
       "      <td>0.061055</td>\n",
       "      <td>0.040008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.300503</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>-0.090234</td>\n",
       "      <td>-0.413538</td>\n",
       "      <td>-0.187390</td>\n",
       "      <td>-0.235937</td>\n",
       "      <td>-0.484675</td>\n",
       "      <td>-0.207490</td>\n",
       "      <td>-0.241415</td>\n",
       "      <td>-0.133987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292135</td>\n",
       "      <td>-0.115175</td>\n",
       "      <td>-0.494525</td>\n",
       "      <td>0.101587</td>\n",
       "      <td>0.317148</td>\n",
       "      <td>0.551520</td>\n",
       "      <td>0.437635</td>\n",
       "      <td>-0.496592</td>\n",
       "      <td>0.192235</td>\n",
       "      <td>0.185645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.409900</td>\n",
       "      <td>0.769080</td>\n",
       "      <td>1.658900</td>\n",
       "      <td>0.947430</td>\n",
       "      <td>1.594100</td>\n",
       "      <td>2.041200</td>\n",
       "      <td>1.024800</td>\n",
       "      <td>1.864200</td>\n",
       "      <td>2.464300</td>\n",
       "      <td>1.746400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864680</td>\n",
       "      <td>0.942590</td>\n",
       "      <td>0.933850</td>\n",
       "      <td>0.995080</td>\n",
       "      <td>1.011000</td>\n",
       "      <td>0.996520</td>\n",
       "      <td>0.993710</td>\n",
       "      <td>0.937120</td>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.629590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  4252.000000  4252.000000  4252.000000  4252.000000  4252.000000   \n",
       "mean      0.328098    -0.016892    -0.105908    -0.657875    -0.545393   \n",
       "std       0.342924     0.064407     0.116318     0.357233     0.428247   \n",
       "min      -2.558400    -0.885720    -1.006100    -1.000100    -0.999270   \n",
       "25%       0.260400    -0.027013    -0.129390    -0.985930    -0.963715   \n",
       "50%       0.277880    -0.017045    -0.108480    -0.711575    -0.555900   \n",
       "75%       0.300503    -0.006720    -0.090234    -0.413538    -0.187390   \n",
       "max       2.409900     0.769080     1.658900     0.947430     1.594100   \n",
       "\n",
       "               5            6            7            8            9    ...  \\\n",
       "count  4252.000000  4252.000000  4252.000000  4252.000000  4252.000000  ...   \n",
       "mean     -0.568119    -0.692565    -0.555668    -0.567608    -0.470899  ...   \n",
       "std       0.409752     0.331081     0.422234     0.415663     0.510689  ...   \n",
       "min      -1.005300    -1.000000    -0.999280    -1.005400    -0.951790  ...   \n",
       "25%      -0.966993    -0.987775    -0.964815    -0.966835    -0.928032  ...   \n",
       "50%      -0.566125    -0.753750    -0.568120    -0.563235    -0.568920  ...   \n",
       "75%      -0.235937    -0.484675    -0.207490    -0.241415    -0.133987  ...   \n",
       "max       2.041200     1.024800     1.864200     2.464300     1.746400  ...   \n",
       "\n",
       "               551          552          553          554          555  \\\n",
       "count  4252.000000  4252.000000  4252.000000  4252.000000  4252.000000   \n",
       "mean      0.132243    -0.289747    -0.618739    -0.026148     0.032900   \n",
       "std       0.235170     0.294440     0.285924     0.327898     0.445712   \n",
       "min      -0.793830    -0.955590    -1.006300    -1.001000    -0.992450   \n",
       "25%      -0.007369    -0.500945    -0.826232    -0.120405    -0.240812   \n",
       "50%       0.148570    -0.323800    -0.695040     0.006337     0.027131   \n",
       "75%       0.292135    -0.115175    -0.494525     0.101587     0.317148   \n",
       "max       0.864680     0.942590     0.933850     0.995080     1.011000   \n",
       "\n",
       "               556          557          558          559          560  \n",
       "count  4252.000000  4252.000000  4252.000000  4252.000000  4252.000000  \n",
       "mean      0.002336    -0.003853    -0.525921     0.070942    -0.022324  \n",
       "std       0.595877     0.514740     0.434306     0.244521     0.310260  \n",
       "min      -0.995690    -0.987830    -0.997540    -0.795520    -1.093500  \n",
       "25%      -0.554740    -0.450703    -0.800307    -0.062248    -0.135370  \n",
       "50%       0.004438    -0.005088    -0.676585     0.061055     0.040008  \n",
       "75%       0.551520     0.437635    -0.496592     0.192235     0.185645  \n",
       "max       0.996520     0.993710     0.937120     0.825490     0.629590  \n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca791c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.487576</td>\n",
       "      <td>-0.137581</td>\n",
       "      <td>0.333963</td>\n",
       "      <td>0.818232</td>\n",
       "      <td>2.234521</td>\n",
       "      <td>1.455378</td>\n",
       "      <td>0.792271</td>\n",
       "      <td>2.414631</td>\n",
       "      <td>1.547660</td>\n",
       "      <td>0.959945</td>\n",
       "      <td>...</td>\n",
       "      <td>1.177058</td>\n",
       "      <td>-0.132479</td>\n",
       "      <td>-0.200218</td>\n",
       "      <td>-1.314561</td>\n",
       "      <td>-1.238592</td>\n",
       "      <td>-0.622460</td>\n",
       "      <td>-0.511516</td>\n",
       "      <td>0.175566</td>\n",
       "      <td>-0.694652</td>\n",
       "      <td>1.197989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.067713</td>\n",
       "      <td>-0.061338</td>\n",
       "      <td>-0.017813</td>\n",
       "      <td>0.635473</td>\n",
       "      <td>0.691560</td>\n",
       "      <td>0.691106</td>\n",
       "      <td>0.585409</td>\n",
       "      <td>0.789959</td>\n",
       "      <td>0.664193</td>\n",
       "      <td>0.516224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131806</td>\n",
       "      <td>0.511971</td>\n",
       "      <td>0.481616</td>\n",
       "      <td>-1.629665</td>\n",
       "      <td>-1.800479</td>\n",
       "      <td>0.887526</td>\n",
       "      <td>-0.680456</td>\n",
       "      <td>-0.003200</td>\n",
       "      <td>-0.453752</td>\n",
       "      <td>1.116628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.166931</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.026725</td>\n",
       "      <td>-0.930887</td>\n",
       "      <td>-1.010767</td>\n",
       "      <td>-1.000091</td>\n",
       "      <td>-0.900234</td>\n",
       "      <td>-1.007220</td>\n",
       "      <td>-0.992271</td>\n",
       "      <td>-0.913679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.656470</td>\n",
       "      <td>1.306803</td>\n",
       "      <td>1.360764</td>\n",
       "      <td>0.247143</td>\n",
       "      <td>0.536799</td>\n",
       "      <td>0.341342</td>\n",
       "      <td>0.302213</td>\n",
       "      <td>0.675987</td>\n",
       "      <td>1.521803</td>\n",
       "      <td>-1.095358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.232696</td>\n",
       "      <td>-0.013574</td>\n",
       "      <td>0.031283</td>\n",
       "      <td>-0.880802</td>\n",
       "      <td>-1.027535</td>\n",
       "      <td>-1.037533</td>\n",
       "      <td>-0.844834</td>\n",
       "      <td>-1.014728</td>\n",
       "      <td>-1.023213</td>\n",
       "      <td>-0.888828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.490199</td>\n",
       "      <td>-1.322163</td>\n",
       "      <td>-0.936724</td>\n",
       "      <td>0.208084</td>\n",
       "      <td>0.558991</td>\n",
       "      <td>-1.254058</td>\n",
       "      <td>1.380806</td>\n",
       "      <td>2.213450</td>\n",
       "      <td>-3.024812</td>\n",
       "      <td>-0.928285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.159435</td>\n",
       "      <td>1.782648</td>\n",
       "      <td>-0.860517</td>\n",
       "      <td>0.213122</td>\n",
       "      <td>0.670752</td>\n",
       "      <td>1.035502</td>\n",
       "      <td>0.111421</td>\n",
       "      <td>0.585830</td>\n",
       "      <td>0.886539</td>\n",
       "      <td>0.600218</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.536062</td>\n",
       "      <td>-0.331558</td>\n",
       "      <td>-0.667005</td>\n",
       "      <td>0.018097</td>\n",
       "      <td>0.234169</td>\n",
       "      <td>0.725259</td>\n",
       "      <td>-1.423267</td>\n",
       "      <td>-0.662514</td>\n",
       "      <td>0.208098</td>\n",
       "      <td>0.558708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>-0.755299</td>\n",
       "      <td>-0.157596</td>\n",
       "      <td>-0.129073</td>\n",
       "      <td>1.555232</td>\n",
       "      <td>1.362661</td>\n",
       "      <td>2.195779</td>\n",
       "      <td>1.436903</td>\n",
       "      <td>1.279002</td>\n",
       "      <td>1.847527</td>\n",
       "      <td>0.820948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.531834</td>\n",
       "      <td>0.046390</td>\n",
       "      <td>-0.085524</td>\n",
       "      <td>2.148535</td>\n",
       "      <td>-1.495470</td>\n",
       "      <td>-0.958828</td>\n",
       "      <td>-1.412523</td>\n",
       "      <td>-0.001288</td>\n",
       "      <td>-0.468510</td>\n",
       "      <td>1.112470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>5.627274</td>\n",
       "      <td>3.149115</td>\n",
       "      <td>-1.751027</td>\n",
       "      <td>3.022738</td>\n",
       "      <td>1.745936</td>\n",
       "      <td>1.100012</td>\n",
       "      <td>3.159315</td>\n",
       "      <td>1.782418</td>\n",
       "      <td>0.989784</td>\n",
       "      <td>3.554046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642192</td>\n",
       "      <td>-0.926553</td>\n",
       "      <td>-0.864913</td>\n",
       "      <td>-2.864949</td>\n",
       "      <td>-0.917742</td>\n",
       "      <td>1.302124</td>\n",
       "      <td>1.487125</td>\n",
       "      <td>-0.281585</td>\n",
       "      <td>-0.779093</td>\n",
       "      <td>0.504746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>-0.055814</td>\n",
       "      <td>-0.403141</td>\n",
       "      <td>-0.852779</td>\n",
       "      <td>0.416038</td>\n",
       "      <td>-0.027877</td>\n",
       "      <td>0.773727</td>\n",
       "      <td>0.360664</td>\n",
       "      <td>-0.032289</td>\n",
       "      <td>0.849173</td>\n",
       "      <td>0.401052</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211177</td>\n",
       "      <td>0.694678</td>\n",
       "      <td>0.281539</td>\n",
       "      <td>-0.549173</td>\n",
       "      <td>0.971391</td>\n",
       "      <td>-1.313171</td>\n",
       "      <td>1.331591</td>\n",
       "      <td>-0.189657</td>\n",
       "      <td>-0.637505</td>\n",
       "      <td>0.795376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>5.802843</td>\n",
       "      <td>-2.485699</td>\n",
       "      <td>-0.464917</td>\n",
       "      <td>2.627935</td>\n",
       "      <td>0.402580</td>\n",
       "      <td>1.037088</td>\n",
       "      <td>2.892762</td>\n",
       "      <td>0.273218</td>\n",
       "      <td>0.595596</td>\n",
       "      <td>2.602259</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571759</td>\n",
       "      <td>-0.776284</td>\n",
       "      <td>-0.773550</td>\n",
       "      <td>-2.849028</td>\n",
       "      <td>0.274648</td>\n",
       "      <td>1.612828</td>\n",
       "      <td>-1.683196</td>\n",
       "      <td>-0.742858</td>\n",
       "      <td>0.539357</td>\n",
       "      <td>0.115189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>-0.170255</td>\n",
       "      <td>0.620669</td>\n",
       "      <td>0.053964</td>\n",
       "      <td>0.817952</td>\n",
       "      <td>0.766852</td>\n",
       "      <td>0.648539</td>\n",
       "      <td>0.681077</td>\n",
       "      <td>0.777998</td>\n",
       "      <td>0.648169</td>\n",
       "      <td>0.803809</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.383542</td>\n",
       "      <td>1.206482</td>\n",
       "      <td>1.196610</td>\n",
       "      <td>-0.463588</td>\n",
       "      <td>-0.251444</td>\n",
       "      <td>0.970472</td>\n",
       "      <td>0.903403</td>\n",
       "      <td>0.156407</td>\n",
       "      <td>-1.066178</td>\n",
       "      <td>0.900429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0     0.487576 -0.137581  0.333963  0.818232  2.234521  1.455378  0.792271   \n",
       "1    -0.067713 -0.061338 -0.017813  0.635473  0.691560  0.691106  0.585409   \n",
       "2    -0.166931  0.000852  0.026725 -0.930887 -1.010767 -1.000091 -0.900234   \n",
       "3    -0.232696 -0.013574  0.031283 -0.880802 -1.027535 -1.037533 -0.844834   \n",
       "4    -0.159435  1.782648 -0.860517  0.213122  0.670752  1.035502  0.111421   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4247 -0.755299 -0.157596 -0.129073  1.555232  1.362661  2.195779  1.436903   \n",
       "4248  5.627274  3.149115 -1.751027  3.022738  1.745936  1.100012  3.159315   \n",
       "4249 -0.055814 -0.403141 -0.852779  0.416038 -0.027877  0.773727  0.360664   \n",
       "4250  5.802843 -2.485699 -0.464917  2.627935  0.402580  1.037088  2.892762   \n",
       "4251 -0.170255  0.620669  0.053964  0.817952  0.766852  0.648539  0.681077   \n",
       "\n",
       "           7         8         9    ...       551       552       553  \\\n",
       "0     2.414631  1.547660  0.959945  ...  1.177058 -0.132479 -0.200218   \n",
       "1     0.789959  0.664193  0.516224  ... -0.131806  0.511971  0.481616   \n",
       "2    -1.007220 -0.992271 -0.913679  ... -0.656470  1.306803  1.360764   \n",
       "3    -1.014728 -1.023213 -0.888828  ...  0.490199 -1.322163 -0.936724   \n",
       "4     0.585830  0.886539  0.600218  ... -0.536062 -0.331558 -0.667005   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4247  1.279002  1.847527  0.820948  ...  0.531834  0.046390 -0.085524   \n",
       "4248  1.782418  0.989784  3.554046  ...  0.642192 -0.926553 -0.864913   \n",
       "4249 -0.032289  0.849173  0.401052  ...  0.211177  0.694678  0.281539   \n",
       "4250  0.273218  0.595596  2.602259  ... -0.571759 -0.776284 -0.773550   \n",
       "4251  0.777998  0.648169  0.803809  ... -0.383542  1.206482  1.196610   \n",
       "\n",
       "           554       555       556       557       558       559       560  \n",
       "0    -1.314561 -1.238592 -0.622460 -0.511516  0.175566 -0.694652  1.197989  \n",
       "1    -1.629665 -1.800479  0.887526 -0.680456 -0.003200 -0.453752  1.116628  \n",
       "2     0.247143  0.536799  0.341342  0.302213  0.675987  1.521803 -1.095358  \n",
       "3     0.208084  0.558991 -1.254058  1.380806  2.213450 -3.024812 -0.928285  \n",
       "4     0.018097  0.234169  0.725259 -1.423267 -0.662514  0.208098  0.558708  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4247  2.148535 -1.495470 -0.958828 -1.412523 -0.001288 -0.468510  1.112470  \n",
       "4248 -2.864949 -0.917742  1.302124  1.487125 -0.281585 -0.779093  0.504746  \n",
       "4249 -0.549173  0.971391 -1.313171  1.331591 -0.189657 -0.637505  0.795376  \n",
       "4250 -2.849028  0.274648  1.612828 -1.683196 -0.742858  0.539357  0.115189  \n",
       "4251 -0.463588 -0.251444  0.970472  0.903403  0.156407 -1.066178  0.900429  \n",
       "\n",
       "[4252 rows x 561 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler()\n",
    "data_scaled = pd.DataFrame(scalar.fit_transform(data), columns=data.columns)\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa018a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension Reduction done\n"
     ]
    }
   ],
   "source": [
    "# Method 1 for dimension reduction\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model1=RandomForestRegressor(random_state=2,max_depth=10)\n",
    "model1.fit(data,labels)\n",
    "print(\"Dimension Reduction done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6251872d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features=data.columns\n",
    "importances=model1.feature_importances_\n",
    "indices = np.argsort(importances)[-20:]  # top 80 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "selected_features=[features[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d20b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_after_RF=data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1140169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>476</th>\n",
       "      <th>69</th>\n",
       "      <th>57</th>\n",
       "      <th>509</th>\n",
       "      <th>302</th>\n",
       "      <th>330</th>\n",
       "      <th>558</th>\n",
       "      <th>139</th>\n",
       "      <th>40</th>\n",
       "      <th>42</th>\n",
       "      <th>51</th>\n",
       "      <th>37</th>\n",
       "      <th>49</th>\n",
       "      <th>181</th>\n",
       "      <th>65</th>\n",
       "      <th>54</th>\n",
       "      <th>504</th>\n",
       "      <th>63</th>\n",
       "      <th>56</th>\n",
       "      <th>102</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.93920</td>\n",
       "      <td>-0.360030</td>\n",
       "      <td>-0.84522</td>\n",
       "      <td>-0.420100</td>\n",
       "      <td>-0.840000</td>\n",
       "      <td>-0.71673</td>\n",
       "      <td>-0.44968</td>\n",
       "      <td>-0.518050</td>\n",
       "      <td>0.76825</td>\n",
       "      <td>-0.453840</td>\n",
       "      <td>-0.422650</td>\n",
       "      <td>0.226180</td>\n",
       "      <td>0.76011</td>\n",
       "      <td>-0.30767</td>\n",
       "      <td>-0.46445</td>\n",
       "      <td>-0.488420</td>\n",
       "      <td>-0.35167</td>\n",
       "      <td>-0.089152</td>\n",
       "      <td>0.42072</td>\n",
       "      <td>0.76576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.98414</td>\n",
       "      <td>-0.385190</td>\n",
       "      <td>-0.92695</td>\n",
       "      <td>-0.606070</td>\n",
       "      <td>-0.879270</td>\n",
       "      <td>-0.74628</td>\n",
       "      <td>-0.52731</td>\n",
       "      <td>-0.491170</td>\n",
       "      <td>0.79390</td>\n",
       "      <td>-0.411740</td>\n",
       "      <td>-0.396410</td>\n",
       "      <td>0.432900</td>\n",
       "      <td>0.74271</td>\n",
       "      <td>-0.54437</td>\n",
       "      <td>-0.49796</td>\n",
       "      <td>-0.424580</td>\n",
       "      <td>-0.37166</td>\n",
       "      <td>-0.240820</td>\n",
       "      <td>0.47817</td>\n",
       "      <td>0.60630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.99990</td>\n",
       "      <td>-0.148650</td>\n",
       "      <td>-0.60028</td>\n",
       "      <td>-0.990830</td>\n",
       "      <td>-0.999910</td>\n",
       "      <td>-0.99964</td>\n",
       "      <td>-0.23237</td>\n",
       "      <td>-0.994250</td>\n",
       "      <td>0.50827</td>\n",
       "      <td>0.491060</td>\n",
       "      <td>0.481790</td>\n",
       "      <td>0.341230</td>\n",
       "      <td>0.43831</td>\n",
       "      <td>-0.99245</td>\n",
       "      <td>-0.33663</td>\n",
       "      <td>0.490890</td>\n",
       "      <td>-0.98703</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.11256</td>\n",
       "      <td>-0.69983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.99992</td>\n",
       "      <td>-0.119280</td>\n",
       "      <td>0.60993</td>\n",
       "      <td>-0.990390</td>\n",
       "      <td>-0.999340</td>\n",
       "      <td>-0.99987</td>\n",
       "      <td>0.43528</td>\n",
       "      <td>-0.994410</td>\n",
       "      <td>-0.25050</td>\n",
       "      <td>0.461070</td>\n",
       "      <td>0.450480</td>\n",
       "      <td>0.386420</td>\n",
       "      <td>-0.31351</td>\n",
       "      <td>-0.99768</td>\n",
       "      <td>-0.89598</td>\n",
       "      <td>0.461180</td>\n",
       "      <td>-0.98073</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.95382</td>\n",
       "      <td>-0.65005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.78730</td>\n",
       "      <td>-0.351000</td>\n",
       "      <td>-0.99467</td>\n",
       "      <td>-0.448230</td>\n",
       "      <td>-0.953290</td>\n",
       "      <td>-0.70840</td>\n",
       "      <td>-0.81362</td>\n",
       "      <td>0.084361</td>\n",
       "      <td>0.92421</td>\n",
       "      <td>-0.180530</td>\n",
       "      <td>-0.138210</td>\n",
       "      <td>0.088951</td>\n",
       "      <td>0.86557</td>\n",
       "      <td>-0.70401</td>\n",
       "      <td>-0.24201</td>\n",
       "      <td>-0.233620</td>\n",
       "      <td>-0.32155</td>\n",
       "      <td>-0.627590</td>\n",
       "      <td>0.79741</td>\n",
       "      <td>0.55310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>-0.92617</td>\n",
       "      <td>0.066643</td>\n",
       "      <td>-0.91803</td>\n",
       "      <td>0.174380</td>\n",
       "      <td>-0.556080</td>\n",
       "      <td>-0.33937</td>\n",
       "      <td>-0.52648</td>\n",
       "      <td>-0.052289</td>\n",
       "      <td>0.83497</td>\n",
       "      <td>-0.423730</td>\n",
       "      <td>-0.378490</td>\n",
       "      <td>0.144240</td>\n",
       "      <td>0.79832</td>\n",
       "      <td>-0.33187</td>\n",
       "      <td>-0.64483</td>\n",
       "      <td>-0.453870</td>\n",
       "      <td>0.21002</td>\n",
       "      <td>-0.430500</td>\n",
       "      <td>0.57605</td>\n",
       "      <td>0.36228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>-0.97095</td>\n",
       "      <td>-0.484220</td>\n",
       "      <td>-0.91406</td>\n",
       "      <td>-0.023797</td>\n",
       "      <td>0.243010</td>\n",
       "      <td>-0.57473</td>\n",
       "      <td>-0.64820</td>\n",
       "      <td>-0.440740</td>\n",
       "      <td>0.28219</td>\n",
       "      <td>-0.086592</td>\n",
       "      <td>-0.003381</td>\n",
       "      <td>0.677380</td>\n",
       "      <td>0.90108</td>\n",
       "      <td>-0.59649</td>\n",
       "      <td>-0.82353</td>\n",
       "      <td>-0.185300</td>\n",
       "      <td>0.87749</td>\n",
       "      <td>0.621290</td>\n",
       "      <td>-0.21359</td>\n",
       "      <td>0.71140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>-0.87863</td>\n",
       "      <td>-0.176820</td>\n",
       "      <td>-0.86717</td>\n",
       "      <td>-0.613510</td>\n",
       "      <td>-0.963600</td>\n",
       "      <td>-0.76530</td>\n",
       "      <td>-0.60828</td>\n",
       "      <td>-0.318650</td>\n",
       "      <td>0.86238</td>\n",
       "      <td>-0.285170</td>\n",
       "      <td>-0.253760</td>\n",
       "      <td>0.016757</td>\n",
       "      <td>0.79221</td>\n",
       "      <td>-0.72603</td>\n",
       "      <td>-0.12901</td>\n",
       "      <td>-0.328760</td>\n",
       "      <td>-0.50134</td>\n",
       "      <td>0.057368</td>\n",
       "      <td>0.64200</td>\n",
       "      <td>0.70025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>-0.98043</td>\n",
       "      <td>-0.226050</td>\n",
       "      <td>-0.97890</td>\n",
       "      <td>-0.242080</td>\n",
       "      <td>0.018924</td>\n",
       "      <td>-0.77287</td>\n",
       "      <td>-0.84851</td>\n",
       "      <td>-0.322300</td>\n",
       "      <td>0.27427</td>\n",
       "      <td>0.008927</td>\n",
       "      <td>0.026156</td>\n",
       "      <td>-0.577310</td>\n",
       "      <td>0.93898</td>\n",
       "      <td>-0.60876</td>\n",
       "      <td>-0.80642</td>\n",
       "      <td>-0.029883</td>\n",
       "      <td>0.57803</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.20572</td>\n",
       "      <td>0.52263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>-0.94960</td>\n",
       "      <td>-0.370050</td>\n",
       "      <td>-0.67996</td>\n",
       "      <td>-0.465120</td>\n",
       "      <td>-0.816150</td>\n",
       "      <td>-0.72834</td>\n",
       "      <td>-0.45800</td>\n",
       "      <td>-0.545580</td>\n",
       "      <td>0.77121</td>\n",
       "      <td>-0.330620</td>\n",
       "      <td>-0.327590</td>\n",
       "      <td>0.635400</td>\n",
       "      <td>0.71264</td>\n",
       "      <td>-0.60466</td>\n",
       "      <td>-0.53805</td>\n",
       "      <td>-0.335190</td>\n",
       "      <td>-0.24471</td>\n",
       "      <td>-0.217150</td>\n",
       "      <td>0.42575</td>\n",
       "      <td>0.57381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          476        69       57       509       302      330      558  \\\n",
       "0    -0.93920 -0.360030 -0.84522 -0.420100 -0.840000 -0.71673 -0.44968   \n",
       "1    -0.98414 -0.385190 -0.92695 -0.606070 -0.879270 -0.74628 -0.52731   \n",
       "2    -0.99990 -0.148650 -0.60028 -0.990830 -0.999910 -0.99964 -0.23237   \n",
       "3    -0.99992 -0.119280  0.60993 -0.990390 -0.999340 -0.99987  0.43528   \n",
       "4    -0.78730 -0.351000 -0.99467 -0.448230 -0.953290 -0.70840 -0.81362   \n",
       "...       ...       ...      ...       ...       ...      ...      ...   \n",
       "4247 -0.92617  0.066643 -0.91803  0.174380 -0.556080 -0.33937 -0.52648   \n",
       "4248 -0.97095 -0.484220 -0.91406 -0.023797  0.243010 -0.57473 -0.64820   \n",
       "4249 -0.87863 -0.176820 -0.86717 -0.613510 -0.963600 -0.76530 -0.60828   \n",
       "4250 -0.98043 -0.226050 -0.97890 -0.242080  0.018924 -0.77287 -0.84851   \n",
       "4251 -0.94960 -0.370050 -0.67996 -0.465120 -0.816150 -0.72834 -0.45800   \n",
       "\n",
       "           139       40        42        51        37       49      181  \\\n",
       "0    -0.518050  0.76825 -0.453840 -0.422650  0.226180  0.76011 -0.30767   \n",
       "1    -0.491170  0.79390 -0.411740 -0.396410  0.432900  0.74271 -0.54437   \n",
       "2    -0.994250  0.50827  0.491060  0.481790  0.341230  0.43831 -0.99245   \n",
       "3    -0.994410 -0.25050  0.461070  0.450480  0.386420 -0.31351 -0.99768   \n",
       "4     0.084361  0.92421 -0.180530 -0.138210  0.088951  0.86557 -0.70401   \n",
       "...        ...      ...       ...       ...       ...      ...      ...   \n",
       "4247 -0.052289  0.83497 -0.423730 -0.378490  0.144240  0.79832 -0.33187   \n",
       "4248 -0.440740  0.28219 -0.086592 -0.003381  0.677380  0.90108 -0.59649   \n",
       "4249 -0.318650  0.86238 -0.285170 -0.253760  0.016757  0.79221 -0.72603   \n",
       "4250 -0.322300  0.27427  0.008927  0.026156 -0.577310  0.93898 -0.60876   \n",
       "4251 -0.545580  0.77121 -0.330620 -0.327590  0.635400  0.71264 -0.60466   \n",
       "\n",
       "           65        54      504        63       56      102  \n",
       "0    -0.46445 -0.488420 -0.35167 -0.089152  0.42072  0.76576  \n",
       "1    -0.49796 -0.424580 -0.37166 -0.240820  0.47817  0.60630  \n",
       "2    -0.33663  0.490890 -0.98703 -1.000000 -0.11256 -0.69983  \n",
       "3    -0.89598  0.461180 -0.98073 -1.000000 -0.95382 -0.65005  \n",
       "4    -0.24201 -0.233620 -0.32155 -0.627590  0.79741  0.55310  \n",
       "...       ...       ...      ...       ...      ...      ...  \n",
       "4247 -0.64483 -0.453870  0.21002 -0.430500  0.57605  0.36228  \n",
       "4248 -0.82353 -0.185300  0.87749  0.621290 -0.21359  0.71140  \n",
       "4249 -0.12901 -0.328760 -0.50134  0.057368  0.64200  0.70025  \n",
       "4250 -0.80642 -0.029883  0.57803 -1.000000 -0.20572  0.52263  \n",
       "4251 -0.53805 -0.335190 -0.24471 -0.217150  0.42575  0.57381  \n",
       "\n",
       "[4252 rows x 20 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_after_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5da024e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       2\n",
       "2       4\n",
       "3       6\n",
       "4       1\n",
       "       ..\n",
       "4247    3\n",
       "4248    2\n",
       "4249    2\n",
       "4250    1\n",
       "4251    3\n",
       "Name: class, Length: 4252, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94959865",
   "metadata": {},
   "source": [
    "<h1 > Halt and Stop </h1>\n",
    "<h2> PCA Code </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1288828d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.629531</td>\n",
       "      <td>14.856823</td>\n",
       "      <td>10.593409</td>\n",
       "      <td>-3.781079</td>\n",
       "      <td>-0.230631</td>\n",
       "      <td>-2.629830</td>\n",
       "      <td>2.825996</td>\n",
       "      <td>-8.828905</td>\n",
       "      <td>-3.269102</td>\n",
       "      <td>-6.578480</td>\n",
       "      <td>2.159592</td>\n",
       "      <td>1.837202</td>\n",
       "      <td>-1.987490</td>\n",
       "      <td>1.322127</td>\n",
       "      <td>2.970779</td>\n",
       "      <td>-3.654699</td>\n",
       "      <td>2.091649</td>\n",
       "      <td>-3.082109</td>\n",
       "      <td>0.733009</td>\n",
       "      <td>-0.820087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.315133</td>\n",
       "      <td>4.921955</td>\n",
       "      <td>-3.995595</td>\n",
       "      <td>-2.255632</td>\n",
       "      <td>-0.461161</td>\n",
       "      <td>0.916194</td>\n",
       "      <td>1.667741</td>\n",
       "      <td>-3.197429</td>\n",
       "      <td>1.044379</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.168932</td>\n",
       "      <td>-0.695841</td>\n",
       "      <td>-1.567962</td>\n",
       "      <td>0.494077</td>\n",
       "      <td>-0.723823</td>\n",
       "      <td>-0.442762</td>\n",
       "      <td>1.914127</td>\n",
       "      <td>2.452142</td>\n",
       "      <td>2.941270</td>\n",
       "      <td>-0.875028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.155627</td>\n",
       "      <td>2.154239</td>\n",
       "      <td>-5.819888</td>\n",
       "      <td>-1.102799</td>\n",
       "      <td>0.287754</td>\n",
       "      <td>1.054957</td>\n",
       "      <td>-0.567482</td>\n",
       "      <td>-2.257072</td>\n",
       "      <td>-1.038818</td>\n",
       "      <td>-1.656368</td>\n",
       "      <td>-0.147627</td>\n",
       "      <td>-0.803911</td>\n",
       "      <td>-0.978908</td>\n",
       "      <td>-1.965637</td>\n",
       "      <td>-0.601632</td>\n",
       "      <td>-1.967115</td>\n",
       "      <td>-2.640024</td>\n",
       "      <td>2.022526</td>\n",
       "      <td>2.801107</td>\n",
       "      <td>-0.959965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.103288</td>\n",
       "      <td>3.767917</td>\n",
       "      <td>-6.018159</td>\n",
       "      <td>-1.677049</td>\n",
       "      <td>1.064371</td>\n",
       "      <td>0.691963</td>\n",
       "      <td>-1.510585</td>\n",
       "      <td>-1.111558</td>\n",
       "      <td>-1.349424</td>\n",
       "      <td>-2.358492</td>\n",
       "      <td>0.466362</td>\n",
       "      <td>-1.070167</td>\n",
       "      <td>0.433423</td>\n",
       "      <td>-2.167386</td>\n",
       "      <td>-0.530678</td>\n",
       "      <td>-2.469406</td>\n",
       "      <td>-3.085323</td>\n",
       "      <td>-0.314081</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>-0.751699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.893014</td>\n",
       "      <td>6.439557</td>\n",
       "      <td>-5.857893</td>\n",
       "      <td>-1.931942</td>\n",
       "      <td>0.319152</td>\n",
       "      <td>-0.173189</td>\n",
       "      <td>-0.837526</td>\n",
       "      <td>-1.445935</td>\n",
       "      <td>0.938064</td>\n",
       "      <td>-0.741610</td>\n",
       "      <td>-1.252487</td>\n",
       "      <td>0.630344</td>\n",
       "      <td>-1.756081</td>\n",
       "      <td>-1.811645</td>\n",
       "      <td>0.272141</td>\n",
       "      <td>-1.653960</td>\n",
       "      <td>-1.236539</td>\n",
       "      <td>-0.055979</td>\n",
       "      <td>0.671251</td>\n",
       "      <td>-2.306505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>-15.346321</td>\n",
       "      <td>-5.223540</td>\n",
       "      <td>2.066638</td>\n",
       "      <td>-0.071040</td>\n",
       "      <td>-2.737702</td>\n",
       "      <td>2.245572</td>\n",
       "      <td>-2.540565</td>\n",
       "      <td>-2.358935</td>\n",
       "      <td>-0.237690</td>\n",
       "      <td>-1.986813</td>\n",
       "      <td>1.721676</td>\n",
       "      <td>0.451884</td>\n",
       "      <td>0.742498</td>\n",
       "      <td>-0.463987</td>\n",
       "      <td>4.093653</td>\n",
       "      <td>4.451495</td>\n",
       "      <td>-1.226662</td>\n",
       "      <td>-5.124464</td>\n",
       "      <td>2.369892</td>\n",
       "      <td>2.796131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>-15.632898</td>\n",
       "      <td>-7.127824</td>\n",
       "      <td>2.817008</td>\n",
       "      <td>0.017201</td>\n",
       "      <td>-2.730833</td>\n",
       "      <td>2.092721</td>\n",
       "      <td>-3.115867</td>\n",
       "      <td>-3.415086</td>\n",
       "      <td>-1.001908</td>\n",
       "      <td>-2.261312</td>\n",
       "      <td>1.836727</td>\n",
       "      <td>0.698259</td>\n",
       "      <td>0.570752</td>\n",
       "      <td>-0.425441</td>\n",
       "      <td>4.355570</td>\n",
       "      <td>5.076866</td>\n",
       "      <td>-2.319139</td>\n",
       "      <td>-4.613177</td>\n",
       "      <td>3.123821</td>\n",
       "      <td>1.958682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>-17.016423</td>\n",
       "      <td>-9.349355</td>\n",
       "      <td>5.325734</td>\n",
       "      <td>1.912351</td>\n",
       "      <td>-1.780125</td>\n",
       "      <td>1.598338</td>\n",
       "      <td>-3.942679</td>\n",
       "      <td>-2.457444</td>\n",
       "      <td>-1.664441</td>\n",
       "      <td>1.860920</td>\n",
       "      <td>0.076592</td>\n",
       "      <td>1.036340</td>\n",
       "      <td>-5.423483</td>\n",
       "      <td>-1.701017</td>\n",
       "      <td>4.847681</td>\n",
       "      <td>2.904304</td>\n",
       "      <td>-1.793181</td>\n",
       "      <td>-3.872349</td>\n",
       "      <td>1.418040</td>\n",
       "      <td>0.675944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>-17.054935</td>\n",
       "      <td>-8.611769</td>\n",
       "      <td>3.950561</td>\n",
       "      <td>1.225732</td>\n",
       "      <td>0.202216</td>\n",
       "      <td>2.609193</td>\n",
       "      <td>-3.498617</td>\n",
       "      <td>-0.149453</td>\n",
       "      <td>-2.515303</td>\n",
       "      <td>-0.557511</td>\n",
       "      <td>1.498262</td>\n",
       "      <td>0.026294</td>\n",
       "      <td>-3.384069</td>\n",
       "      <td>-3.548249</td>\n",
       "      <td>3.816402</td>\n",
       "      <td>2.331627</td>\n",
       "      <td>-2.298082</td>\n",
       "      <td>-3.585890</td>\n",
       "      <td>-0.505159</td>\n",
       "      <td>1.327229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>-16.708425</td>\n",
       "      <td>-10.328057</td>\n",
       "      <td>4.876636</td>\n",
       "      <td>0.959383</td>\n",
       "      <td>-3.124137</td>\n",
       "      <td>1.836876</td>\n",
       "      <td>-3.873189</td>\n",
       "      <td>-2.402532</td>\n",
       "      <td>-1.433123</td>\n",
       "      <td>-0.705269</td>\n",
       "      <td>2.633819</td>\n",
       "      <td>1.115383</td>\n",
       "      <td>-3.397099</td>\n",
       "      <td>-3.938887</td>\n",
       "      <td>3.231070</td>\n",
       "      <td>3.083693</td>\n",
       "      <td>-0.378234</td>\n",
       "      <td>-1.364739</td>\n",
       "      <td>0.576016</td>\n",
       "      <td>-1.010143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2         3         4         5         6   \\\n",
       "0     17.629531  14.856823  10.593409 -3.781079 -0.230631 -2.629830  2.825996   \n",
       "1      8.315133   4.921955  -3.995595 -2.255632 -0.461161  0.916194  1.667741   \n",
       "2      5.155627   2.154239  -5.819888 -1.102799  0.287754  1.054957 -0.567482   \n",
       "3      3.103288   3.767917  -6.018159 -1.677049  1.064371  0.691963 -1.510585   \n",
       "4      1.893014   6.439557  -5.857893 -1.931942  0.319152 -0.173189 -0.837526   \n",
       "...         ...        ...        ...       ...       ...       ...       ...   \n",
       "4247 -15.346321  -5.223540   2.066638 -0.071040 -2.737702  2.245572 -2.540565   \n",
       "4248 -15.632898  -7.127824   2.817008  0.017201 -2.730833  2.092721 -3.115867   \n",
       "4249 -17.016423  -9.349355   5.325734  1.912351 -1.780125  1.598338 -3.942679   \n",
       "4250 -17.054935  -8.611769   3.950561  1.225732  0.202216  2.609193 -3.498617   \n",
       "4251 -16.708425 -10.328057   4.876636  0.959383 -3.124137  1.836876 -3.873189   \n",
       "\n",
       "            7         8         9         10        11        12        13  \\\n",
       "0    -8.828905 -3.269102 -6.578480  2.159592  1.837202 -1.987490  1.322127   \n",
       "1    -3.197429  1.044379  0.097800  0.168932 -0.695841 -1.567962  0.494077   \n",
       "2    -2.257072 -1.038818 -1.656368 -0.147627 -0.803911 -0.978908 -1.965637   \n",
       "3    -1.111558 -1.349424 -2.358492  0.466362 -1.070167  0.433423 -2.167386   \n",
       "4    -1.445935  0.938064 -0.741610 -1.252487  0.630344 -1.756081 -1.811645   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4247 -2.358935 -0.237690 -1.986813  1.721676  0.451884  0.742498 -0.463987   \n",
       "4248 -3.415086 -1.001908 -2.261312  1.836727  0.698259  0.570752 -0.425441   \n",
       "4249 -2.457444 -1.664441  1.860920  0.076592  1.036340 -5.423483 -1.701017   \n",
       "4250 -0.149453 -2.515303 -0.557511  1.498262  0.026294 -3.384069 -3.548249   \n",
       "4251 -2.402532 -1.433123 -0.705269  2.633819  1.115383 -3.397099 -3.938887   \n",
       "\n",
       "            14        15        16        17        18        19  \n",
       "0     2.970779 -3.654699  2.091649 -3.082109  0.733009 -0.820087  \n",
       "1    -0.723823 -0.442762  1.914127  2.452142  2.941270 -0.875028  \n",
       "2    -0.601632 -1.967115 -2.640024  2.022526  2.801107 -0.959965  \n",
       "3    -0.530678 -2.469406 -3.085323 -0.314081  0.525884 -0.751699  \n",
       "4     0.272141 -1.653960 -1.236539 -0.055979  0.671251 -2.306505  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "4247  4.093653  4.451495 -1.226662 -5.124464  2.369892  2.796131  \n",
       "4248  4.355570  5.076866 -2.319139 -4.613177  3.123821  1.958682  \n",
       "4249  4.847681  2.904304 -1.793181 -3.872349  1.418040  0.675944  \n",
       "4250  3.816402  2.331627 -2.298082 -3.585890 -0.505159  1.327229  \n",
       "4251  3.231070  3.083693 -0.378234 -1.364739  0.576016 -1.010143  \n",
       "\n",
       "[4252 rows x 20 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_1 = PCA(n_components=20)\n",
    "principalComponents = pca_1.fit_transform(X_train)\n",
    "data_after_RF=pd.DataFrame(principalComponents)\n",
    "data_after_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258a1eb",
   "metadata": {},
   "source": [
    "<h2> PCA STOP </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64dfddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Percentages are: [5, 10, 15, 20, 25]\n",
      "Splitting Position are: [0, 213, 638, 1276, 2126, 3189]\n"
     ]
    }
   ],
   "source": [
    "# Defining Splitter for the dataset (Common for all)\n",
    "def splitter(len_array,lower_bound, incrementor, start_idx):\n",
    "    final=0\n",
    "    per=[]\n",
    "    sp=[start_idx]\n",
    "    for i in range(lower_bound,100,incrementor):\n",
    "        if final+i>100:\n",
    "            break\n",
    "        per.append(i)\n",
    "        final+=i\n",
    "    for i in per:\n",
    "        len_of_data=round((i/100)*len_array)\n",
    "        sp.append(sp[-1]+len_of_data)\n",
    "    return per,sp\n",
    "\n",
    "lower_bound=5\n",
    "incrementor=5\n",
    "start_idx=0\n",
    "\n",
    "        \n",
    "spliting_percentage, spliting_position=splitter(len(data),lower_bound,incrementor,start_idx)\n",
    "print(\"Splitting Percentages are: {}\".format(spliting_percentage))\n",
    "print(\"Splitting Position are: {}\".format(spliting_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ced02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_experiment=data_after_RF.copy(deep=True)\n",
    "labels_for_experiment=labels.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2339956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>476</th>\n",
       "      <th>69</th>\n",
       "      <th>57</th>\n",
       "      <th>509</th>\n",
       "      <th>302</th>\n",
       "      <th>330</th>\n",
       "      <th>558</th>\n",
       "      <th>139</th>\n",
       "      <th>40</th>\n",
       "      <th>42</th>\n",
       "      <th>51</th>\n",
       "      <th>37</th>\n",
       "      <th>49</th>\n",
       "      <th>181</th>\n",
       "      <th>65</th>\n",
       "      <th>54</th>\n",
       "      <th>504</th>\n",
       "      <th>63</th>\n",
       "      <th>56</th>\n",
       "      <th>102</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.93920</td>\n",
       "      <td>-0.360030</td>\n",
       "      <td>-0.84522</td>\n",
       "      <td>-0.420100</td>\n",
       "      <td>-0.840000</td>\n",
       "      <td>-0.71673</td>\n",
       "      <td>-0.44968</td>\n",
       "      <td>-0.518050</td>\n",
       "      <td>0.76825</td>\n",
       "      <td>-0.453840</td>\n",
       "      <td>-0.422650</td>\n",
       "      <td>0.226180</td>\n",
       "      <td>0.76011</td>\n",
       "      <td>-0.30767</td>\n",
       "      <td>-0.46445</td>\n",
       "      <td>-0.488420</td>\n",
       "      <td>-0.35167</td>\n",
       "      <td>-0.089152</td>\n",
       "      <td>0.42072</td>\n",
       "      <td>0.76576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.98414</td>\n",
       "      <td>-0.385190</td>\n",
       "      <td>-0.92695</td>\n",
       "      <td>-0.606070</td>\n",
       "      <td>-0.879270</td>\n",
       "      <td>-0.74628</td>\n",
       "      <td>-0.52731</td>\n",
       "      <td>-0.491170</td>\n",
       "      <td>0.79390</td>\n",
       "      <td>-0.411740</td>\n",
       "      <td>-0.396410</td>\n",
       "      <td>0.432900</td>\n",
       "      <td>0.74271</td>\n",
       "      <td>-0.54437</td>\n",
       "      <td>-0.49796</td>\n",
       "      <td>-0.424580</td>\n",
       "      <td>-0.37166</td>\n",
       "      <td>-0.240820</td>\n",
       "      <td>0.47817</td>\n",
       "      <td>0.60630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.99990</td>\n",
       "      <td>-0.148650</td>\n",
       "      <td>-0.60028</td>\n",
       "      <td>-0.990830</td>\n",
       "      <td>-0.999910</td>\n",
       "      <td>-0.99964</td>\n",
       "      <td>-0.23237</td>\n",
       "      <td>-0.994250</td>\n",
       "      <td>0.50827</td>\n",
       "      <td>0.491060</td>\n",
       "      <td>0.481790</td>\n",
       "      <td>0.341230</td>\n",
       "      <td>0.43831</td>\n",
       "      <td>-0.99245</td>\n",
       "      <td>-0.33663</td>\n",
       "      <td>0.490890</td>\n",
       "      <td>-0.98703</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.11256</td>\n",
       "      <td>-0.69983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.99992</td>\n",
       "      <td>-0.119280</td>\n",
       "      <td>0.60993</td>\n",
       "      <td>-0.990390</td>\n",
       "      <td>-0.999340</td>\n",
       "      <td>-0.99987</td>\n",
       "      <td>0.43528</td>\n",
       "      <td>-0.994410</td>\n",
       "      <td>-0.25050</td>\n",
       "      <td>0.461070</td>\n",
       "      <td>0.450480</td>\n",
       "      <td>0.386420</td>\n",
       "      <td>-0.31351</td>\n",
       "      <td>-0.99768</td>\n",
       "      <td>-0.89598</td>\n",
       "      <td>0.461180</td>\n",
       "      <td>-0.98073</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.95382</td>\n",
       "      <td>-0.65005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.78730</td>\n",
       "      <td>-0.351000</td>\n",
       "      <td>-0.99467</td>\n",
       "      <td>-0.448230</td>\n",
       "      <td>-0.953290</td>\n",
       "      <td>-0.70840</td>\n",
       "      <td>-0.81362</td>\n",
       "      <td>0.084361</td>\n",
       "      <td>0.92421</td>\n",
       "      <td>-0.180530</td>\n",
       "      <td>-0.138210</td>\n",
       "      <td>0.088951</td>\n",
       "      <td>0.86557</td>\n",
       "      <td>-0.70401</td>\n",
       "      <td>-0.24201</td>\n",
       "      <td>-0.233620</td>\n",
       "      <td>-0.32155</td>\n",
       "      <td>-0.627590</td>\n",
       "      <td>0.79741</td>\n",
       "      <td>0.55310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>-0.92617</td>\n",
       "      <td>0.066643</td>\n",
       "      <td>-0.91803</td>\n",
       "      <td>0.174380</td>\n",
       "      <td>-0.556080</td>\n",
       "      <td>-0.33937</td>\n",
       "      <td>-0.52648</td>\n",
       "      <td>-0.052289</td>\n",
       "      <td>0.83497</td>\n",
       "      <td>-0.423730</td>\n",
       "      <td>-0.378490</td>\n",
       "      <td>0.144240</td>\n",
       "      <td>0.79832</td>\n",
       "      <td>-0.33187</td>\n",
       "      <td>-0.64483</td>\n",
       "      <td>-0.453870</td>\n",
       "      <td>0.21002</td>\n",
       "      <td>-0.430500</td>\n",
       "      <td>0.57605</td>\n",
       "      <td>0.36228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>-0.97095</td>\n",
       "      <td>-0.484220</td>\n",
       "      <td>-0.91406</td>\n",
       "      <td>-0.023797</td>\n",
       "      <td>0.243010</td>\n",
       "      <td>-0.57473</td>\n",
       "      <td>-0.64820</td>\n",
       "      <td>-0.440740</td>\n",
       "      <td>0.28219</td>\n",
       "      <td>-0.086592</td>\n",
       "      <td>-0.003381</td>\n",
       "      <td>0.677380</td>\n",
       "      <td>0.90108</td>\n",
       "      <td>-0.59649</td>\n",
       "      <td>-0.82353</td>\n",
       "      <td>-0.185300</td>\n",
       "      <td>0.87749</td>\n",
       "      <td>0.621290</td>\n",
       "      <td>-0.21359</td>\n",
       "      <td>0.71140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>-0.87863</td>\n",
       "      <td>-0.176820</td>\n",
       "      <td>-0.86717</td>\n",
       "      <td>-0.613510</td>\n",
       "      <td>-0.963600</td>\n",
       "      <td>-0.76530</td>\n",
       "      <td>-0.60828</td>\n",
       "      <td>-0.318650</td>\n",
       "      <td>0.86238</td>\n",
       "      <td>-0.285170</td>\n",
       "      <td>-0.253760</td>\n",
       "      <td>0.016757</td>\n",
       "      <td>0.79221</td>\n",
       "      <td>-0.72603</td>\n",
       "      <td>-0.12901</td>\n",
       "      <td>-0.328760</td>\n",
       "      <td>-0.50134</td>\n",
       "      <td>0.057368</td>\n",
       "      <td>0.64200</td>\n",
       "      <td>0.70025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>-0.98043</td>\n",
       "      <td>-0.226050</td>\n",
       "      <td>-0.97890</td>\n",
       "      <td>-0.242080</td>\n",
       "      <td>0.018924</td>\n",
       "      <td>-0.77287</td>\n",
       "      <td>-0.84851</td>\n",
       "      <td>-0.322300</td>\n",
       "      <td>0.27427</td>\n",
       "      <td>0.008927</td>\n",
       "      <td>0.026156</td>\n",
       "      <td>-0.577310</td>\n",
       "      <td>0.93898</td>\n",
       "      <td>-0.60876</td>\n",
       "      <td>-0.80642</td>\n",
       "      <td>-0.029883</td>\n",
       "      <td>0.57803</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.20572</td>\n",
       "      <td>0.52263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>-0.94960</td>\n",
       "      <td>-0.370050</td>\n",
       "      <td>-0.67996</td>\n",
       "      <td>-0.465120</td>\n",
       "      <td>-0.816150</td>\n",
       "      <td>-0.72834</td>\n",
       "      <td>-0.45800</td>\n",
       "      <td>-0.545580</td>\n",
       "      <td>0.77121</td>\n",
       "      <td>-0.330620</td>\n",
       "      <td>-0.327590</td>\n",
       "      <td>0.635400</td>\n",
       "      <td>0.71264</td>\n",
       "      <td>-0.60466</td>\n",
       "      <td>-0.53805</td>\n",
       "      <td>-0.335190</td>\n",
       "      <td>-0.24471</td>\n",
       "      <td>-0.217150</td>\n",
       "      <td>0.42575</td>\n",
       "      <td>0.57381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          476        69       57       509       302      330      558  \\\n",
       "0    -0.93920 -0.360030 -0.84522 -0.420100 -0.840000 -0.71673 -0.44968   \n",
       "1    -0.98414 -0.385190 -0.92695 -0.606070 -0.879270 -0.74628 -0.52731   \n",
       "2    -0.99990 -0.148650 -0.60028 -0.990830 -0.999910 -0.99964 -0.23237   \n",
       "3    -0.99992 -0.119280  0.60993 -0.990390 -0.999340 -0.99987  0.43528   \n",
       "4    -0.78730 -0.351000 -0.99467 -0.448230 -0.953290 -0.70840 -0.81362   \n",
       "...       ...       ...      ...       ...       ...      ...      ...   \n",
       "4247 -0.92617  0.066643 -0.91803  0.174380 -0.556080 -0.33937 -0.52648   \n",
       "4248 -0.97095 -0.484220 -0.91406 -0.023797  0.243010 -0.57473 -0.64820   \n",
       "4249 -0.87863 -0.176820 -0.86717 -0.613510 -0.963600 -0.76530 -0.60828   \n",
       "4250 -0.98043 -0.226050 -0.97890 -0.242080  0.018924 -0.77287 -0.84851   \n",
       "4251 -0.94960 -0.370050 -0.67996 -0.465120 -0.816150 -0.72834 -0.45800   \n",
       "\n",
       "           139       40        42        51        37       49      181  \\\n",
       "0    -0.518050  0.76825 -0.453840 -0.422650  0.226180  0.76011 -0.30767   \n",
       "1    -0.491170  0.79390 -0.411740 -0.396410  0.432900  0.74271 -0.54437   \n",
       "2    -0.994250  0.50827  0.491060  0.481790  0.341230  0.43831 -0.99245   \n",
       "3    -0.994410 -0.25050  0.461070  0.450480  0.386420 -0.31351 -0.99768   \n",
       "4     0.084361  0.92421 -0.180530 -0.138210  0.088951  0.86557 -0.70401   \n",
       "...        ...      ...       ...       ...       ...      ...      ...   \n",
       "4247 -0.052289  0.83497 -0.423730 -0.378490  0.144240  0.79832 -0.33187   \n",
       "4248 -0.440740  0.28219 -0.086592 -0.003381  0.677380  0.90108 -0.59649   \n",
       "4249 -0.318650  0.86238 -0.285170 -0.253760  0.016757  0.79221 -0.72603   \n",
       "4250 -0.322300  0.27427  0.008927  0.026156 -0.577310  0.93898 -0.60876   \n",
       "4251 -0.545580  0.77121 -0.330620 -0.327590  0.635400  0.71264 -0.60466   \n",
       "\n",
       "           65        54      504        63       56      102  \n",
       "0    -0.46445 -0.488420 -0.35167 -0.089152  0.42072  0.76576  \n",
       "1    -0.49796 -0.424580 -0.37166 -0.240820  0.47817  0.60630  \n",
       "2    -0.33663  0.490890 -0.98703 -1.000000 -0.11256 -0.69983  \n",
       "3    -0.89598  0.461180 -0.98073 -1.000000 -0.95382 -0.65005  \n",
       "4    -0.24201 -0.233620 -0.32155 -0.627590  0.79741  0.55310  \n",
       "...       ...       ...      ...       ...      ...      ...  \n",
       "4247 -0.64483 -0.453870  0.21002 -0.430500  0.57605  0.36228  \n",
       "4248 -0.82353 -0.185300  0.87749  0.621290 -0.21359  0.71140  \n",
       "4249 -0.12901 -0.328760 -0.50134  0.057368  0.64200  0.70025  \n",
       "4250 -0.80642 -0.029883  0.57803 -1.000000 -0.20572  0.52263  \n",
       "4251 -0.53805 -0.335190 -0.24471 -0.217150  0.42575  0.57381  \n",
       "\n",
       "[4252 rows x 20 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_after_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13c87ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       2\n",
       "2       4\n",
       "3       6\n",
       "4       1\n",
       "       ..\n",
       "4247    3\n",
       "4248    2\n",
       "4249    2\n",
       "4250    1\n",
       "4251    3\n",
       "Name: class, Length: 4252, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_for_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27d89c",
   "metadata": {},
   "source": [
    "# Apply ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aff76ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best: 0.000000 using {'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[126  12  14   0   0   0]\n",
      " [ 14  92  21   1   0   5]\n",
      " [ 12  14 105   0   0   1]\n",
      " [  0   3   0 106  53   4]\n",
      " [  1   2   1  28 126   0]\n",
      " [  0   3   0   2   0 105]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.83      0.83       152\n",
      "           2       0.73      0.69      0.71       133\n",
      "           3       0.74      0.80      0.77       132\n",
      "           4       0.77      0.64      0.70       166\n",
      "           5       0.70      0.80      0.75       158\n",
      "           6       0.91      0.95      0.93       110\n",
      "\n",
      "    accuracy                           0.78       851\n",
      "   macro avg       0.78      0.78      0.78       851\n",
      "weighted avg       0.78      0.78      0.77       851\n",
      "\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SImple Algorithm\n",
    "# splitting dataset into training and testing part\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    solvers = [\"lbfgs\",\"liblinear\"]\n",
    "    penalty = ['l2']\n",
    "    c_values = [5]\n",
    "    # define grid search\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0,)\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "#     print(\"Training set score for logreg_model: %f\" % grid_search.score(X_train , y_train))\n",
    "#     print(\"Testing  set score for logreg_model: %f\" % grid_search.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b46c1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[24  3  4  0  0  0]\n",
      " [ 3 22  4  1  0  1]\n",
      " [ 5  6 19  0  0  0]\n",
      " [ 0  0  0 28  9  1]\n",
      " [ 0  1  0 22 24  0]\n",
      " [ 0  2  0  2  0 32]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.77      0.76        31\n",
      "           2       0.65      0.71      0.68        31\n",
      "           3       0.70      0.63      0.67        30\n",
      "           4       0.53      0.74      0.62        38\n",
      "           5       0.73      0.51      0.60        47\n",
      "           6       0.94      0.89      0.91        36\n",
      "\n",
      "    accuracy                           0.70       213\n",
      "   macro avg       0.72      0.71      0.71       213\n",
      "weighted avg       0.72      0.70      0.70       213\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (425,)\n",
      "Shape of testing output Y_test: (425, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[56 10  8  0  0  0]\n",
      " [ 6 31  8  1  0  1]\n",
      " [ 5  9 57  0  1  0]\n",
      " [ 0  3  0 45 25  1]\n",
      " [ 0  0  1 17 70  0]\n",
      " [ 0  4  0  1  0 65]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.76      0.79        74\n",
      "           2       0.54      0.66      0.60        47\n",
      "           3       0.77      0.79      0.78        72\n",
      "           4       0.70      0.61      0.65        74\n",
      "           5       0.73      0.80      0.76        88\n",
      "           6       0.97      0.93      0.95        70\n",
      "\n",
      "    accuracy                           0.76       425\n",
      "   macro avg       0.76      0.76      0.76       425\n",
      "weighted avg       0.77      0.76      0.76       425\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (638,)\n",
      "Shape of testing output Y_test: (638, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[96 14 15  0  0  0]\n",
      " [14 68 13  4  1  2]\n",
      " [18  8 69  0  0  1]\n",
      " [ 0  2  1 84 34  1]\n",
      " [ 1  2  1 26 81  0]\n",
      " [ 0  0  1  4  0 77]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.77      0.76       125\n",
      "           2       0.72      0.67      0.69       102\n",
      "           3       0.69      0.72      0.70        96\n",
      "           4       0.71      0.69      0.70       122\n",
      "           5       0.70      0.73      0.71       111\n",
      "           6       0.95      0.94      0.94        82\n",
      "\n",
      "    accuracy                           0.74       638\n",
      "   macro avg       0.75      0.75      0.75       638\n",
      "weighted avg       0.75      0.74      0.74       638\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (850,)\n",
      "Shape of testing output Y_test: (850, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[159   5   7   0   0   1]\n",
      " [ 25  79  19   0   0   5]\n",
      " [ 15  20  99   0   1   0]\n",
      " [  0   2   0 106  55   5]\n",
      " [  0   1   1  25 109   0]\n",
      " [  0   3   0   1   0 107]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.92      0.86       172\n",
      "           2       0.72      0.62      0.66       128\n",
      "           3       0.79      0.73      0.76       135\n",
      "           4       0.80      0.63      0.71       168\n",
      "           5       0.66      0.80      0.72       136\n",
      "           6       0.91      0.96      0.93       111\n",
      "\n",
      "    accuracy                           0.78       850\n",
      "   macro avg       0.78      0.78      0.77       850\n",
      "weighted avg       0.78      0.78      0.77       850\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[162  16  22   0   1   0]\n",
      " [ 22 107  16   2   1   6]\n",
      " [ 22  18 129   4   0   0]\n",
      " [  0   2   0 136  51  11]\n",
      " [  1   2   1  64 132   0]\n",
      " [  0   4   0   3   0 128]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.81      0.79       201\n",
      "           2       0.72      0.69      0.71       154\n",
      "           3       0.77      0.75      0.76       173\n",
      "           4       0.65      0.68      0.67       200\n",
      "           5       0.71      0.66      0.69       200\n",
      "           6       0.88      0.95      0.91       135\n",
      "\n",
      "    accuracy                           0.75      1063\n",
      "   macro avg       0.75      0.76      0.75      1063\n",
      "weighted avg       0.75      0.75      0.75      1063\n",
      "\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    solvers = [\"lbfgs\",\"liblinear\"]\n",
    "    penalty = ['l2','l1']\n",
    "    c_values = [10]\n",
    "    # define grid search\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=2, cv=cv, scoring='f1',error_score=0,)\n",
    "    grid_result = grid_search.fit(X_train, Y_train)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = grid_search.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ccd98",
   "metadata": {},
   "source": [
    "# ****KNN Classifer ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "480dacab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8776785868532435 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[122   3   4   0   0   0]\n",
      " [ 13 121   7   0   0   0]\n",
      " [ 17   7 126   0   0   0]\n",
      " [  0   1   0 137  32   2]\n",
      " [  0   2   0  17 128   0]\n",
      " [  0   3   0   1   0 108]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.95      0.87       129\n",
      "           2       0.88      0.86      0.87       141\n",
      "           3       0.92      0.84      0.88       150\n",
      "           4       0.88      0.80      0.84       172\n",
      "           5       0.80      0.87      0.83       147\n",
      "           6       0.98      0.96      0.97       112\n",
      "\n",
      "    accuracy                           0.87       851\n",
      "   macro avg       0.88      0.88      0.88       851\n",
      "weighted avg       0.88      0.87      0.87       851\n",
      "\n",
      "Training set score for knn_model: 0.943840\n",
      "Testing  set score for knn_model: 0.871915\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = KNeighborsClassifier()\n",
    "    params_grid = [{'n_neighbors': [2,3,4], 'weights' :['uniform'],'leaf_size':[4,5,6,7,8,9],'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "    knn_model = GridSearchCV(model, params_grid, cv=5)\n",
    "    knn_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', knn_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best #neighbors:',knn_model.best_estimator_.n_neighbors,\"\\n\") \n",
    "    print('Best weights:',knn_model.best_estimator_.weights,\"\\n\")\n",
    "    print('Best leaf_size:',knn_model.best_estimator_.leaf_size,\"\\n\")\n",
    "    print('Best algorithm:',knn_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = knn_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for knn_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for knn_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "acc79f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.7132890365448505 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[29  1  3  0  0  0]\n",
      " [ 7 24  5  0  0  0]\n",
      " [ 6  4 21  0  0  0]\n",
      " [ 0  2  0 37 11  0]\n",
      " [ 2  0  0  8 32  0]\n",
      " [ 0  3  0  0  0 18]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.92      0.91       168\n",
      "           2       0.87      0.90      0.89       124\n",
      "           3       0.90      0.91      0.90       141\n",
      "           4       0.87      0.77      0.82       157\n",
      "           5       0.81      0.86      0.84       147\n",
      "           6       0.97      0.96      0.97       114\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.89      0.89       851\n",
      "weighted avg       0.89      0.88      0.88       851\n",
      "\n",
      "Training set score for knn_model: 0.863850\n",
      "Testing  set score for knn_model: 0.755869\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (425,)\n",
      "Shape of testing output Y_test: (425, 1)\n",
      "Best score for training data: 0.7670588235294117 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[71  3  5  0  0  0]\n",
      " [ 9 43  6  0  0  2]\n",
      " [ 6  7 69  0  0  0]\n",
      " [ 0  2  0 44 19  2]\n",
      " [ 0  4  0 11 70  0]\n",
      " [ 0  0  0  0  2 50]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.92      0.91       168\n",
      "           2       0.87      0.90      0.89       124\n",
      "           3       0.90      0.91      0.90       141\n",
      "           4       0.87      0.77      0.82       157\n",
      "           5       0.81      0.86      0.84       147\n",
      "           6       0.97      0.96      0.97       114\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.89      0.89       851\n",
      "weighted avg       0.89      0.88      0.88       851\n",
      "\n",
      "Training set score for knn_model: 0.884706\n",
      "Testing  set score for knn_model: 0.816471\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (638,)\n",
      "Shape of testing output Y_test: (638, 1)\n",
      "Best score for training data: 0.7883858267716535 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[109   4  16   0   0   0]\n",
      " [ 14  61   9   0   0   0]\n",
      " [  8   4  87   0   0   0]\n",
      " [  0   6   2  76  35   3]\n",
      " [  1   3   2  20 100   0]\n",
      " [  0   1   0   3   0  74]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.92      0.91       168\n",
      "           2       0.87      0.90      0.89       124\n",
      "           3       0.90      0.91      0.90       141\n",
      "           4       0.87      0.77      0.82       157\n",
      "           5       0.81      0.86      0.84       147\n",
      "           6       0.97      0.96      0.97       114\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.89      0.89       851\n",
      "weighted avg       0.89      0.88      0.88       851\n",
      "\n",
      "Training set score for knn_model: 0.901254\n",
      "Testing  set score for knn_model: 0.794671\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (850,)\n",
      "Shape of testing output Y_test: (850, 1)\n",
      "Best score for training data: 0.8058823529411765 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[146   3   4   0   0   0]\n",
      " [  9 109   4   3   0   3]\n",
      " [ 19   7 110   0   0   0]\n",
      " [  1   2   1 133  31   1]\n",
      " [  2   2   1  41 115   0]\n",
      " [  1   2   0   4   0  96]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.92      0.91       168\n",
      "           2       0.87      0.90      0.89       124\n",
      "           3       0.90      0.91      0.90       141\n",
      "           4       0.87      0.77      0.82       157\n",
      "           5       0.81      0.86      0.84       147\n",
      "           6       0.97      0.96      0.97       114\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.89      0.89       851\n",
      "weighted avg       0.89      0.88      0.88       851\n",
      "\n",
      "Training set score for knn_model: 0.920000\n",
      "Testing  set score for knn_model: 0.834118\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.8081229515457526 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[176   2  15   0   0   0]\n",
      " [ 13 140   8   0   0   2]\n",
      " [ 23   5 151   0   0   0]\n",
      " [  1   4   0 145  40   2]\n",
      " [  1   2   0  43 142   0]\n",
      " [  0   7   0   3   0 138]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.92      0.91       168\n",
      "           2       0.87      0.90      0.89       124\n",
      "           3       0.90      0.91      0.90       141\n",
      "           4       0.87      0.77      0.82       157\n",
      "           5       0.81      0.86      0.84       147\n",
      "           6       0.97      0.96      0.97       114\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.89      0.89       851\n",
      "weighted avg       0.89      0.88      0.88       851\n",
      "\n",
      "Training set score for knn_model: 0.917215\n",
      "Testing  set score for knn_model: 0.839135\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    model = KNeighborsClassifier()\n",
    "    params_grid = [{'n_neighbors': [2,3,4], 'weights' :['uniform'],'leaf_size':[4,5,6,7,8,9],'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "    knn_model = GridSearchCV(model, params_grid, cv=5)\n",
    "    knn_model.fit(X_train,Y_train)\n",
    "    print('Best score for training data:', knn_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best #neighbors:',knn_model.best_estimator_.n_neighbors,\"\\n\") \n",
    "    print('Best weights:',knn_model.best_estimator_.weights,\"\\n\")\n",
    "    print('Best leaf_size:',knn_model.best_estimator_.leaf_size,\"\\n\")\n",
    "    print('Best algorithm:',knn_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = knn_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for knn_model: %f\" % final_model.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for knn_model: %f\" % final_model.score(X_test  , Y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5541e508",
   "metadata": {},
   "source": [
    "# ** Support Vector Machine **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c82f4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.777416378818752 \n",
      "\n",
      "Best C: 2 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[134   9  15   0   0   0]\n",
      " [ 18  76  18   1   0   7]\n",
      " [ 11  15  98   0   0   0]\n",
      " [  0   3   0 110  69   7]\n",
      " [  2   0   1  18 143   0]\n",
      " [  0   1   0   1   0  94]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.85      0.83       158\n",
      "           2       0.73      0.63      0.68       120\n",
      "           3       0.74      0.79      0.77       124\n",
      "           4       0.85      0.58      0.69       189\n",
      "           5       0.67      0.87      0.76       164\n",
      "           6       0.87      0.98      0.92        96\n",
      "\n",
      "    accuracy                           0.77       851\n",
      "   macro avg       0.78      0.78      0.77       851\n",
      "weighted avg       0.78      0.77      0.77       851\n",
      "\n",
      "Training set score for svm_model: 0.792708\n",
      "Testing  set score for svm_model: 0.769683\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = SVC()\n",
    "    params_grid = [{'kernel': ['linear'], 'C': [2]}]\n",
    "    svm_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    svm_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "    print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "    print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "    final_model = svm_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for svm_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for svm_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2ef4eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.6713615023474179 \n",
      "\n",
      "Best C: 100 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[32  6  0  0  0  0]\n",
      " [ 3 15  5  0  0  0]\n",
      " [ 8  5 30  0  0  0]\n",
      " [ 0  0  0 20 18  2]\n",
      " [ 0  0  0  3 35  0]\n",
      " [ 0  3  0  0  0 28]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.84      0.79        38\n",
      "           2       0.52      0.65      0.58        23\n",
      "           3       0.86      0.70      0.77        43\n",
      "           4       0.87      0.50      0.63        40\n",
      "           5       0.66      0.92      0.77        38\n",
      "           6       0.93      0.90      0.92        31\n",
      "\n",
      "    accuracy                           0.75       213\n",
      "   macro avg       0.76      0.75      0.74       213\n",
      "weighted avg       0.78      0.75      0.75       213\n",
      "\n",
      "Training set score for svm_model: 0.868545\n",
      "Testing  set score for svm_model: 0.751174\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.7411846968334831 \n",
      "\n",
      "Best C: 100 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[68  8  7  0  0  0]\n",
      " [ 8 41  8  1  0  0]\n",
      " [ 6  4 54  0  0  0]\n",
      " [ 0  2  1 45 23  1]\n",
      " [ 0  0  0 20 63  0]\n",
      " [ 0  4  0  2  0 59]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.82      0.82        83\n",
      "           2       0.69      0.71      0.70        58\n",
      "           3       0.77      0.84      0.81        64\n",
      "           4       0.66      0.62      0.64        72\n",
      "           5       0.73      0.76      0.75        83\n",
      "           6       0.98      0.91      0.94        65\n",
      "\n",
      "    accuracy                           0.78       425\n",
      "   macro avg       0.78      0.78      0.78       425\n",
      "weighted avg       0.78      0.78      0.78       425\n",
      "\n",
      "Training set score for svm_model: 0.861176\n",
      "Testing  set score for svm_model: 0.776471\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.7586441078335843 \n",
      "\n",
      "Best C: 100 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[94 14 12  0  0  0]\n",
      " [16 65  7  0  0  2]\n",
      " [11  7 86  0  1  0]\n",
      " [ 0  1  2 74 43  2]\n",
      " [ 2  0  1 19 84  0]\n",
      " [ 0  1  0  1  0 93]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.78      0.77       120\n",
      "           2       0.74      0.72      0.73        90\n",
      "           3       0.80      0.82      0.81       105\n",
      "           4       0.79      0.61      0.69       122\n",
      "           5       0.66      0.79      0.72       106\n",
      "           6       0.96      0.98      0.97        95\n",
      "\n",
      "    accuracy                           0.78       638\n",
      "   macro avg       0.78      0.78      0.78       638\n",
      "weighted avg       0.78      0.78      0.78       638\n",
      "\n",
      "Training set score for svm_model: 0.849530\n",
      "Testing  set score for svm_model: 0.777429\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.750593075532109 \n",
      "\n",
      "Best C: 100 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[136   7   9   0   0   0]\n",
      " [ 22  92   2   1   1   0]\n",
      " [ 26  12 107   0   1   0]\n",
      " [  1   1   1 114  46   1]\n",
      " [  0   0   0  35 119   0]\n",
      " [  0   7   0   5   1 103]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.89      0.81       152\n",
      "           2       0.77      0.78      0.78       118\n",
      "           3       0.90      0.73      0.81       146\n",
      "           4       0.74      0.70      0.71       164\n",
      "           5       0.71      0.77      0.74       154\n",
      "           6       0.99      0.89      0.94       116\n",
      "\n",
      "    accuracy                           0.79       850\n",
      "   macro avg       0.81      0.79      0.80       850\n",
      "weighted avg       0.80      0.79      0.79       850\n",
      "\n",
      "Training set score for svm_model: 0.821176\n",
      "Testing  set score for svm_model: 0.789412\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.7488342484284236 \n",
      "\n",
      "Best C: 100 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[193  11   6   0   0   0]\n",
      " [ 22 108  22   2   0   2]\n",
      " [ 21  22 117   0   2   0]\n",
      " [  1   2   2 119  73   0]\n",
      " [  2   0   2  45 149   1]\n",
      " [  0   5   0   5   0 129]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.92      0.86       210\n",
      "           2       0.73      0.69      0.71       156\n",
      "           3       0.79      0.72      0.75       162\n",
      "           4       0.70      0.60      0.65       197\n",
      "           5       0.67      0.75      0.70       199\n",
      "           6       0.98      0.93      0.95       139\n",
      "\n",
      "    accuracy                           0.77      1063\n",
      "   macro avg       0.78      0.77      0.77      1063\n",
      "weighted avg       0.77      0.77      0.77      1063\n",
      "\n",
      "Training set score for svm_model: 0.811853\n",
      "Testing  set score for svm_model: 0.766698\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = SVC()\n",
    "    params_grid = [{'kernel': ['linear'], 'C': [100,150]}]\n",
    "    svm_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    svm_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "    print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "    print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "    final_model = svm_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for svm_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for svm_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b2853",
   "metadata": {},
   "source": [
    "# ** Decision Tree **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1543568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8494593025337362 \n",
      "\n",
      "Best depth: 10 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[116   7  15   0   2   0]\n",
      " [ 26 101   9   0   1   3]\n",
      " [ 16   9 103   0   0   0]\n",
      " [  0   3   0 154  19   1]\n",
      " [  1   2   0   4 156   1]\n",
      " [  0   1   0   0   0 101]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.83      0.78       140\n",
      "           2       0.82      0.72      0.77       140\n",
      "           3       0.81      0.80      0.81       128\n",
      "           4       0.97      0.87      0.92       177\n",
      "           5       0.88      0.95      0.91       164\n",
      "           6       0.95      0.99      0.97       102\n",
      "\n",
      "    accuracy                           0.86       851\n",
      "   macro avg       0.86      0.86      0.86       851\n",
      "weighted avg       0.86      0.86      0.86       851\n",
      "\n",
      "Training set score for dc_model: 0.965892\n",
      "Testing  set score for dc_model: 0.858989\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = DecisionTreeClassifier()\n",
    "    params_grid = [{'max_depth': [8,9,10,11,12],'random_state':[42]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "37ebe3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.7230046948356806 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[19  6  3  0  0  0]\n",
      " [ 3 16  5  0  0  1]\n",
      " [ 7 16 24  0  0  0]\n",
      " [ 0  0  0 29  8  1]\n",
      " [ 1  0  0 14 32  0]\n",
      " [ 0  6  0  1  0 21]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.68      0.66        28\n",
      "           2       0.36      0.64      0.46        25\n",
      "           3       0.75      0.51      0.61        47\n",
      "           4       0.66      0.76      0.71        38\n",
      "           5       0.80      0.68      0.74        47\n",
      "           6       0.91      0.75      0.82        28\n",
      "\n",
      "    accuracy                           0.66       213\n",
      "   macro avg       0.69      0.67      0.67       213\n",
      "weighted avg       0.71      0.66      0.67       213\n",
      "\n",
      "Training set score for dc_model: 0.990610\n",
      "Testing  set score for dc_model: 0.661972\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (425,)\n",
      "Shape of testing output Y_test: (425, 1)\n",
      "Best score for training data: 0.729464255985083 \n",
      "\n",
      "Best depth: 10 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[56 15  5  0  0  0]\n",
      " [13 31  6  0  1  4]\n",
      " [16 16 38  0  0  0]\n",
      " [ 1  2  0 62 19  3]\n",
      " [ 0  0  0 17 61  0]\n",
      " [ 0  3  1  1  0 54]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.74      0.69        76\n",
      "           2       0.46      0.56      0.51        55\n",
      "           3       0.76      0.54      0.63        70\n",
      "           4       0.78      0.71      0.74        87\n",
      "           5       0.75      0.78      0.77        78\n",
      "           6       0.89      0.92      0.90        59\n",
      "\n",
      "    accuracy                           0.71       425\n",
      "   macro avg       0.71      0.71      0.71       425\n",
      "weighted avg       0.72      0.71      0.71       425\n",
      "\n",
      "Training set score for dc_model: 0.995294\n",
      "Testing  set score for dc_model: 0.710588\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (638,)\n",
      "Shape of testing output Y_test: (638, 1)\n",
      "Best score for training data: 0.7397983287566067 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[79 17 24  0  0  0]\n",
      " [16 59 13  0  0  3]\n",
      " [ 8  9 85  0  0  1]\n",
      " [ 0  0  1 87 22  0]\n",
      " [ 1  0  0 22 97  0]\n",
      " [ 0  3  0  3  0 88]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.66      0.71       120\n",
      "           2       0.67      0.65      0.66        91\n",
      "           3       0.69      0.83      0.75       103\n",
      "           4       0.78      0.79      0.78       110\n",
      "           5       0.82      0.81      0.81       120\n",
      "           6       0.96      0.94      0.95        94\n",
      "\n",
      "    accuracy                           0.78       638\n",
      "   macro avg       0.78      0.78      0.78       638\n",
      "weighted avg       0.78      0.78      0.78       638\n",
      "\n",
      "Training set score for dc_model: 0.960815\n",
      "Testing  set score for dc_model: 0.775862\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (850,)\n",
      "Shape of testing output Y_test: (850, 1)\n",
      "Best score for training data: 0.7623591964033909 \n",
      "\n",
      "Best depth: 12 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[119  18  15   0   0   0]\n",
      " [ 19  98  17   3   1   0]\n",
      " [ 20  18  75   0   2   0]\n",
      " [  0   2   1 157  14   4]\n",
      " [  1   0   0  25 144   0]\n",
      " [  0   1   0   6   3  87]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.78      0.77       152\n",
      "           2       0.72      0.71      0.71       138\n",
      "           3       0.69      0.65      0.67       115\n",
      "           4       0.82      0.88      0.85       178\n",
      "           5       0.88      0.85      0.86       170\n",
      "           6       0.96      0.90      0.93        97\n",
      "\n",
      "    accuracy                           0.80       850\n",
      "   macro avg       0.80      0.80      0.80       850\n",
      "weighted avg       0.80      0.80      0.80       850\n",
      "\n",
      "Training set score for dc_model: 0.995294\n",
      "Testing  set score for dc_model: 0.800000\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7892814514203867 \n",
      "\n",
      "Best depth: 11 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[139  24  26   1   0   0]\n",
      " [ 19 110  27   4   0   2]\n",
      " [ 16  15 129   1   0   0]\n",
      " [  0   2   1 182  22   0]\n",
      " [  1   0   1  21 166   0]\n",
      " [  0   8   1   4   0 141]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.73      0.76       190\n",
      "           2       0.69      0.68      0.69       162\n",
      "           3       0.70      0.80      0.75       161\n",
      "           4       0.85      0.88      0.87       207\n",
      "           5       0.88      0.88      0.88       189\n",
      "           6       0.99      0.92      0.95       154\n",
      "\n",
      "    accuracy                           0.82      1063\n",
      "   macro avg       0.82      0.81      0.81      1063\n",
      "weighted avg       0.82      0.82      0.82      1063\n",
      "\n",
      "Training set score for dc_model: 0.994356\n",
      "Testing  set score for dc_model: 0.815616\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = DecisionTreeClassifier()\n",
    "    params_grid = [{'max_depth': [8,9,10,11,12],'random_state':[42]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,Y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , Y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0b5b6",
   "metadata": {},
   "source": [
    "<h1> **************************************************************************************************************</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e22d6",
   "metadata": {},
   "source": [
    " <h1> AdaBoost </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18f300b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.3\n",
      "Best score for training data: 0.8316532258064516 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[186  12  16   0   0   0]\n",
      " [ 21 148  22   0   0   2]\n",
      " [ 26  19 170   0   0   2]\n",
      " [  1   2   1 207  38   2]\n",
      " [  3   0   0  13 205   1]\n",
      " [  0   0   0   6   1 172]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.87      0.82       214\n",
      "           2       0.82      0.77      0.79       193\n",
      "           3       0.81      0.78      0.80       217\n",
      "           4       0.92      0.82      0.87       251\n",
      "           5       0.84      0.92      0.88       222\n",
      "           6       0.96      0.96      0.96       179\n",
      "\n",
      "    accuracy                           0.85      1276\n",
      "   macro avg       0.86      0.85      0.85      1276\n",
      "weighted avg       0.85      0.85      0.85      1276\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.852665\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.25\n",
      "Best score for training data: 0.8425838820947006 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[141   9  23   1   1   0]\n",
      " [ 17 111  15   3   3   0]\n",
      " [ 12  12 153   0   1   0]\n",
      " [  0   2   1 200  12   5]\n",
      " [  1   0   1  20 186   1]\n",
      " [  0   2   1   4   0 125]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.81      0.82       175\n",
      "           2       0.82      0.74      0.78       149\n",
      "           3       0.79      0.86      0.82       178\n",
      "           4       0.88      0.91      0.89       220\n",
      "           5       0.92      0.89      0.90       209\n",
      "           6       0.95      0.95      0.95       132\n",
      "\n",
      "    accuracy                           0.86      1063\n",
      "   macro avg       0.86      0.86      0.86      1063\n",
      "weighted avg       0.86      0.86      0.86      1063\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.861712\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8462243537755944 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[126   8  13   0   1   0]\n",
      " [ 21  97   9   0   0   5]\n",
      " [ 20  14 100   1   1   0]\n",
      " [  0   2   0 161   9   3]\n",
      " [  1   0   0  13 143   0]\n",
      " [  0   2   0   1   0 100]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.85      0.80       148\n",
      "           2       0.79      0.73      0.76       132\n",
      "           3       0.82      0.74      0.78       136\n",
      "           4       0.91      0.92      0.92       175\n",
      "           5       0.93      0.91      0.92       157\n",
      "           6       0.93      0.97      0.95       103\n",
      "\n",
      "    accuracy                           0.85       851\n",
      "   macro avg       0.85      0.85      0.85       851\n",
      "weighted avg       0.86      0.85      0.85       851\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.854289\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.15\n",
      "Best score for training data: 0.8563915578776141 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[ 95   6  17   0   1   0]\n",
      " [ 14  72  12   0   0   1]\n",
      " [ 18   9  84   0   1   0]\n",
      " [  0   0   0 123   4   3]\n",
      " [  0   0   0   9  94   0]\n",
      " [  0   0   0   1   2  72]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.80      0.77       119\n",
      "           2       0.83      0.73      0.77        99\n",
      "           3       0.74      0.75      0.75       112\n",
      "           4       0.92      0.95      0.94       130\n",
      "           5       0.92      0.91      0.92       103\n",
      "           6       0.95      0.96      0.95        75\n",
      "\n",
      "    accuracy                           0.85       638\n",
      "   macro avg       0.85      0.85      0.85       638\n",
      "weighted avg       0.85      0.85      0.85       638\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.846395\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.1\n",
      "Best score for training data: 0.8507582928678263 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[66  3  6  0  0  0]\n",
      " [ 4 42  7  0  0  0]\n",
      " [10  3 64  0  0  0]\n",
      " [ 0  0  0 88  3  0]\n",
      " [ 0  0  2  4 64  0]\n",
      " [ 0  1  0  0  0 59]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.88      0.85        75\n",
      "           2       0.86      0.79      0.82        53\n",
      "           3       0.81      0.83      0.82        77\n",
      "           4       0.96      0.97      0.96        91\n",
      "           5       0.96      0.91      0.93        70\n",
      "           6       1.00      0.98      0.99        60\n",
      "\n",
      "    accuracy                           0.90       426\n",
      "   macro avg       0.90      0.89      0.90       426\n",
      "weighted avg       0.90      0.90      0.90       426\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.899061\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.05\n",
      "Best score for training data: 0.8606052449759946 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 0.01 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[26  1  4  0  1  0]\n",
      " [ 3 33  2  0  0  1]\n",
      " [ 2  3 28  0  0  0]\n",
      " [ 0  0  0 38  1  1]\n",
      " [ 0  0  0  6 37  0]\n",
      " [ 0  0  0  0  1 25]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.81      0.83        32\n",
      "           2       0.89      0.85      0.87        39\n",
      "           3       0.82      0.85      0.84        33\n",
      "           4       0.86      0.95      0.90        40\n",
      "           5       0.93      0.86      0.89        43\n",
      "           6       0.93      0.96      0.94        26\n",
      "\n",
      "    accuracy                           0.88       213\n",
      "   macro avg       0.88      0.88      0.88       213\n",
      "weighted avg       0.88      0.88      0.88       213\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.877934\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = DecisionTreeClassifier(max_features=None)\n",
    "    params_grid = [{'n_estimators':[2,4],'learning_rate':[1.0,1e-1,1e-2],'algorithm':['SAMME.R','SAMME']}]\n",
    "    model_ada=AdaBoostClassifier(model,)\n",
    "    dc_model = GridSearchCV(model_ada, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best estimator:',dc_model.best_estimator_.n_estimators,\"\\n\") \n",
    "    print('Best #learning rate:',dc_model.best_estimator_.learning_rate,\"\\n\")\n",
    "    print('Best algorithm:',dc_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d4501268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7417840375586855 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[27  9  8  0  0  0]\n",
      " [ 2 23  8  1  0  1]\n",
      " [ 5  9 20  0  0  0]\n",
      " [ 0  0  0 32  8  1]\n",
      " [ 0  1  0  6 30  0]\n",
      " [ 0  3  0  0  0 19]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.61      0.69        44\n",
      "           2       0.51      0.66      0.57        35\n",
      "           3       0.56      0.59      0.57        34\n",
      "           4       0.82      0.78      0.80        41\n",
      "           5       0.79      0.81      0.80        37\n",
      "           6       0.90      0.86      0.88        22\n",
      "\n",
      "    accuracy                           0.71       213\n",
      "   macro avg       0.73      0.72      0.72       213\n",
      "weighted avg       0.73      0.71      0.71       213\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.708920\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7388872240535411 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[51 11 12  0  0  0]\n",
      " [10 39  7  0  0  3]\n",
      " [17 16 45  0  0  1]\n",
      " [ 0  4  0 60 19  2]\n",
      " [ 0  0  1 21 47  0]\n",
      " [ 0  3  2  1  0 53]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.69      0.67        74\n",
      "           2       0.53      0.66      0.59        59\n",
      "           3       0.67      0.57      0.62        79\n",
      "           4       0.73      0.71      0.72        85\n",
      "           5       0.71      0.68      0.70        69\n",
      "           6       0.90      0.90      0.90        59\n",
      "\n",
      "    accuracy                           0.69       425\n",
      "   macro avg       0.70      0.70      0.70       425\n",
      "weighted avg       0.70      0.69      0.69       425\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.694118\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7664835976023858 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.01 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[ 71  11  21   0   0   0]\n",
      " [ 13  60  15   2   0   3]\n",
      " [ 10  21  64   0   0   0]\n",
      " [  0   1   0 100  25   2]\n",
      " [  0   2   1  19  97   0]\n",
      " [  0   4   0   2   0  94]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.69      0.72       103\n",
      "           2       0.61      0.65      0.62        93\n",
      "           3       0.63      0.67      0.65        95\n",
      "           4       0.81      0.78      0.80       128\n",
      "           5       0.80      0.82      0.80       119\n",
      "           6       0.95      0.94      0.94       100\n",
      "\n",
      "    accuracy                           0.76       638\n",
      "   macro avg       0.76      0.76      0.76       638\n",
      "weighted avg       0.76      0.76      0.76       638\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.761755\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7623674911660778 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[103  14  14   0   1   0]\n",
      " [ 12 100  17   2   0   0]\n",
      " [ 29  16  95   0   1   0]\n",
      " [  0   3   3 139  17   2]\n",
      " [  0   0   1  25 138   0]\n",
      " [  0   0   1  12   3 102]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.78      0.75       132\n",
      "           2       0.75      0.76      0.76       131\n",
      "           3       0.73      0.67      0.70       141\n",
      "           4       0.78      0.85      0.81       164\n",
      "           5       0.86      0.84      0.85       164\n",
      "           6       0.98      0.86      0.92       118\n",
      "\n",
      "    accuracy                           0.80       850\n",
      "   macro avg       0.80      0.80      0.80       850\n",
      "weighted avg       0.80      0.80      0.80       850\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.796471\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7892628842736267 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[125  26  36   1   0   0]\n",
      " [ 11 109  32   3   0   5]\n",
      " [ 19  14 133   0   1   1]\n",
      " [  0   2   0 176  21   1]\n",
      " [  1   0   0  26 189   0]\n",
      " [  0   5   0   6   1 119]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.66      0.73       188\n",
      "           2       0.70      0.68      0.69       160\n",
      "           3       0.66      0.79      0.72       168\n",
      "           4       0.83      0.88      0.85       200\n",
      "           5       0.89      0.88      0.88       216\n",
      "           6       0.94      0.91      0.93       131\n",
      "\n",
      "    accuracy                           0.80      1063\n",
      "   macro avg       0.80      0.80      0.80      1063\n",
      "weighted avg       0.81      0.80      0.80      1063\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.800564\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = DecisionTreeClassifier(max_features=None)\n",
    "    params_grid = [{'n_estimators':[2,4],'learning_rate':[1.0,1e-1,1e-2],'algorithm':['SAMME.R','SAMME']}]\n",
    "    model_ada=AdaBoostClassifier(model,)\n",
    "    dc_model = GridSearchCV(model_ada, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best estimator:',dc_model.best_estimator_.n_estimators,\"\\n\") \n",
    "    print('Best #learning rate:',dc_model.best_estimator_.learning_rate,\"\\n\")\n",
    "    print('Best algorithm:',dc_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013bf29",
   "metadata": {},
   "source": [
    "# ** Ensemble Learning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8c91abd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "[[134  11  14   0   0   0]\n",
      " [ 15  81  14   1   1   3]\n",
      " [ 18  12  92   0   0   0]\n",
      " [  0   3   0 121  56   2]\n",
      " [  3   1   2  24 140   0]\n",
      " [  0   2   0   0   0 101]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.84      0.81       159\n",
      "           2       0.74      0.70      0.72       115\n",
      "           3       0.75      0.75      0.75       122\n",
      "           4       0.83      0.66      0.74       182\n",
      "           5       0.71      0.82      0.76       170\n",
      "           6       0.95      0.98      0.97       103\n",
      "\n",
      "    accuracy                           0.79       851\n",
      "   macro avg       0.80      0.80      0.79       851\n",
      "weighted avg       0.79      0.79      0.78       851\n",
      "\n",
      "Training set score for EL: 0.790650\n",
      "Testing  set score for EL: 0.786134\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_ratio=[0.20]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    estimator = []\n",
    "    # make different combination in the ensemble classfier\n",
    "    estimator.append(('lr',LogisticRegression(solver='saga',penalty='l1',C=1,n_jobs=2)))\n",
    "    estimator.append(('SVC', SVC(gamma ='scale',probability=True,C=100,kernel='linear',)))\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=14,criterion='gini', splitter='best', min_samples_split=17,random_state=42)\n",
    "    model = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=250 ,learning_rate=1,algorithm='SAMME',random_state=7)\n",
    "    estimator.append(('DTC', model))\n",
    "    #estimator.append(('svc_rbf',SVC(gamma ='auto',C=4,kernel='rbf',)))\n",
    "\n",
    "\n",
    "    vot_hard = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "    vot_hard.fit(X_train,y_train)\n",
    "    Y_pred = vot_hard.predict(X_test)\n",
    "    \n",
    "    print(confusion_matrix(y_test,Y_pred))\n",
    "    print(classification_report(y_test,Y_pred))\n",
    "    print(\"Training set score for EL: %f\" % vot_hard.score(X_train , y_train))\n",
    "    print(\"Testing  set score for EL: %f\" % vot_hard.score(X_test  , y_test ))\n",
    "    \n",
    "\n",
    "    score = accuracy_score(y_test, Y_pred)\n",
    "    print(\"Hard Voting Score % d\" % score)\n",
    "    \n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a56fad3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "[[28  8  1  0  0  0]\n",
      " [ 2 21  2  0  0  1]\n",
      " [ 5 10 26  0  0  0]\n",
      " [ 0  1  0 31 15  1]\n",
      " [ 0  0  0  4 27  0]\n",
      " [ 0  1  0  0  0 29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.76      0.78        37\n",
      "           2       0.51      0.81      0.63        26\n",
      "           3       0.90      0.63      0.74        41\n",
      "           4       0.89      0.65      0.75        48\n",
      "           5       0.64      0.87      0.74        31\n",
      "           6       0.94      0.97      0.95        30\n",
      "\n",
      "    accuracy                           0.76       213\n",
      "   macro avg       0.78      0.78      0.76       213\n",
      "weighted avg       0.80      0.76      0.76       213\n",
      "\n",
      "Training set score for EL: 0.779343\n",
      "Testing  set score for EL: 0.760563\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (425,)\n",
      "Shape of testing output Y_test: (425, 1)\n",
      "[[63  6  7  0  0  0]\n",
      " [11 44 10  1  0  2]\n",
      " [10  6 54  0  0  0]\n",
      " [ 0  1  0 51 29  4]\n",
      " [ 1  0  0 14 63  0]\n",
      " [ 0  0  0  0  0 48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.83      0.78        76\n",
      "           2       0.77      0.65      0.70        68\n",
      "           3       0.76      0.77      0.77        70\n",
      "           4       0.77      0.60      0.68        85\n",
      "           5       0.68      0.81      0.74        78\n",
      "           6       0.89      1.00      0.94        48\n",
      "\n",
      "    accuracy                           0.76       425\n",
      "   macro avg       0.77      0.78      0.77       425\n",
      "weighted avg       0.76      0.76      0.76       425\n",
      "\n",
      "Training set score for EL: 0.825882\n",
      "Testing  set score for EL: 0.760000\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (638,)\n",
      "Shape of testing output Y_test: (638, 1)\n",
      "[[ 90  11  13   0   0   0]\n",
      " [ 12  72   6   0   1   3]\n",
      " [ 11   9  72   1   0   0]\n",
      " [  1   2   0  76  47   3]\n",
      " [  0   1   0  23 106   0]\n",
      " [  0   0   0   1   0  77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.79      0.79       114\n",
      "           2       0.76      0.77      0.76        94\n",
      "           3       0.79      0.77      0.78        93\n",
      "           4       0.75      0.59      0.66       129\n",
      "           5       0.69      0.82      0.75       130\n",
      "           6       0.93      0.99      0.96        78\n",
      "\n",
      "    accuracy                           0.77       638\n",
      "   macro avg       0.78      0.79      0.78       638\n",
      "weighted avg       0.77      0.77      0.77       638\n",
      "\n",
      "Training set score for EL: 0.838558\n",
      "Testing  set score for EL: 0.772727\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (850,)\n",
      "Shape of testing output Y_test: (850, 1)\n",
      "[[138  10   6   0   0   1]\n",
      " [ 18  80  15   3   0   2]\n",
      " [ 23   9 106   1   0   0]\n",
      " [  0   2   1 113  47   2]\n",
      " [  1   0   0  38 112   1]\n",
      " [  0   5   0   4   0 112]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.89      0.82       155\n",
      "           2       0.75      0.68      0.71       118\n",
      "           3       0.83      0.76      0.79       139\n",
      "           4       0.71      0.68      0.70       165\n",
      "           5       0.70      0.74      0.72       152\n",
      "           6       0.95      0.93      0.94       121\n",
      "\n",
      "    accuracy                           0.78       850\n",
      "   macro avg       0.79      0.78      0.78       850\n",
      "weighted avg       0.78      0.78      0.78       850\n",
      "\n",
      "Training set score for EL: 0.802353\n",
      "Testing  set score for EL: 0.777647\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "[[187   5  18   0   0   0]\n",
      " [ 29  89  18   2   0   7]\n",
      " [ 21  10 124   1   0   0]\n",
      " [  0   2   1 123  71   7]\n",
      " [  0   1   0  63 137   0]\n",
      " [  0   3   0   4   0 140]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.89      0.84       210\n",
      "           2       0.81      0.61      0.70       145\n",
      "           3       0.77      0.79      0.78       156\n",
      "           4       0.64      0.60      0.62       204\n",
      "           5       0.66      0.68      0.67       201\n",
      "           6       0.91      0.95      0.93       147\n",
      "\n",
      "    accuracy                           0.75      1063\n",
      "   macro avg       0.76      0.76      0.76      1063\n",
      "weighted avg       0.75      0.75      0.75      1063\n",
      "\n",
      "Training set score for EL: 0.787394\n",
      "Testing  set score for EL: 0.752587\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    estimator = []\n",
    "    # make different combination in the ensemble classfier\n",
    "    estimator.append(('lr',LogisticRegression(solver='saga',penalty='l1',C=1,n_jobs=2)))\n",
    "    estimator.append(('SVC', SVC(gamma ='scale',probability=True,C=100,kernel='linear',)))\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=14,criterion='gini', splitter='best', min_samples_split=17,random_state=42)\n",
    "    model = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=250 ,learning_rate=1,algorithm='SAMME',random_state=7)\n",
    "    estimator.append(('DTC', model))\n",
    "    #estimator.append(('svc_rbf',SVC(gamma ='auto',C=4,kernel='rbf',)))\n",
    "\n",
    "\n",
    "    vot_hard = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "    vot_hard.fit(X_train,Y_train)\n",
    "    Y_pred = vot_hard.predict(X_test)\n",
    "    \n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "    print(\"Training set score for EL: %f\" % vot_hard.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for EL: %f\" % vot_hard.score(X_test  , Y_test ))\n",
    "    \n",
    "\n",
    "    score = accuracy_score(Y_test, Y_pred)\n",
    "    print(\"Hard Voting Score % d\" % score)\n",
    "    \n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0b5b6",
   "metadata": {},
   "source": [
    "# ** Random Forest **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0c9587e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8691587887920141 \n",
      "\n",
      "Best depth: 7 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 10 \n",
      "\n",
      "[[139   7   9   0   0   0]\n",
      " [ 18 115   6   1   0   0]\n",
      " [ 15   7 129   0   0   0]\n",
      " [  0   2   0 132  20   0]\n",
      " [  1   0   0  18 136   0]\n",
      " [  0   0   0   0   0  96]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.90      0.85       155\n",
      "           2       0.88      0.82      0.85       140\n",
      "           3       0.90      0.85      0.87       151\n",
      "           4       0.87      0.86      0.87       154\n",
      "           5       0.87      0.88      0.87       155\n",
      "           6       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.88      0.89       851\n",
      "weighted avg       0.88      0.88      0.88       851\n",
      "\n",
      "Training set score for dc_model: 0.935901\n",
      "Testing  set score for dc_model: 0.877791\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = RandomForestClassifier()\n",
    "    params_grid = [{'max_depth': [5,6,7],'random_state':[42],'n_jobs':[10,15,20]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #estimators:',dc_model.best_estimator_.n_estimators,\"\\n\")\n",
    "    print('Best jobs:',dc_model.best_estimator_.n_jobs,\"\\n\") \n",
    "    #print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "eb66d8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7887323943661971 \n",
      "\n",
      "Best depth: 6 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[31  5  1  0  0  0]\n",
      " [ 9 19  4  0  0  0]\n",
      " [13  7 21  1  0  0]\n",
      " [ 0  2  0 32  7  1]\n",
      " [ 0  1  0  3 26  0]\n",
      " [ 0  7  0  1  0 22]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.84      0.69        37\n",
      "           2       0.46      0.59      0.52        32\n",
      "           3       0.81      0.50      0.62        42\n",
      "           4       0.86      0.76      0.81        42\n",
      "           5       0.79      0.87      0.83        30\n",
      "           6       0.96      0.73      0.83        30\n",
      "\n",
      "    accuracy                           0.71       213\n",
      "   macro avg       0.74      0.72      0.72       213\n",
      "weighted avg       0.75      0.71      0.71       213\n",
      "\n",
      "Training set score for dc_model: 0.995305\n",
      "Testing  set score for dc_model: 0.708920\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.8024006925715046 \n",
      "\n",
      "Best depth: 7 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[56  2  5  0  0  0]\n",
      " [ 5 48  3  0  0  5]\n",
      " [11  7 55  0  0  0]\n",
      " [ 0  1  1 59 26  0]\n",
      " [ 1  0  0  6 72  0]\n",
      " [ 0  2  0  0  0 60]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.89      0.82        63\n",
      "           2       0.80      0.79      0.79        61\n",
      "           3       0.86      0.75      0.80        73\n",
      "           4       0.91      0.68      0.78        87\n",
      "           5       0.73      0.91      0.81        79\n",
      "           6       0.92      0.97      0.94        62\n",
      "\n",
      "    accuracy                           0.82       425\n",
      "   macro avg       0.83      0.83      0.83       425\n",
      "weighted avg       0.83      0.82      0.82       425\n",
      "\n",
      "Training set score for dc_model: 0.983529\n",
      "Testing  set score for dc_model: 0.823529\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.8197729353057549 \n",
      "\n",
      "Best depth: 6 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[ 94  11   9   0   0   0]\n",
      " [ 13  66   9   0   0   2]\n",
      " [ 10  10  85   0   0   0]\n",
      " [  0   1   0 104  16   3]\n",
      " [  2   0   0  18 102   0]\n",
      " [  0   3   0   0   0  80]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.82      0.81       114\n",
      "           2       0.73      0.73      0.73        90\n",
      "           3       0.83      0.81      0.82       105\n",
      "           4       0.85      0.84      0.85       124\n",
      "           5       0.86      0.84      0.85       122\n",
      "           6       0.94      0.96      0.95        83\n",
      "\n",
      "    accuracy                           0.83       638\n",
      "   macro avg       0.83      0.83      0.83       638\n",
      "weighted avg       0.83      0.83      0.83       638\n",
      "\n",
      "Training set score for dc_model: 0.949843\n",
      "Testing  set score for dc_model: 0.832288\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.8258721942965211 \n",
      "\n",
      "Best depth: 7 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[137   8   8   0   0   0]\n",
      " [ 14 103   9   2   0   1]\n",
      " [ 20   4 111   0   0   0]\n",
      " [  0   1   0 126  15   1]\n",
      " [  1   1   0  25 137   1]\n",
      " [  0   6   1   5   0 113]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.90      0.84       153\n",
      "           2       0.84      0.80      0.82       129\n",
      "           3       0.86      0.82      0.84       135\n",
      "           4       0.80      0.88      0.84       143\n",
      "           5       0.90      0.83      0.86       165\n",
      "           6       0.97      0.90      0.94       125\n",
      "\n",
      "    accuracy                           0.86       850\n",
      "   macro avg       0.86      0.86      0.86       850\n",
      "weighted avg       0.86      0.86      0.86       850\n",
      "\n",
      "Training set score for dc_model: 0.972941\n",
      "Testing  set score for dc_model: 0.855294\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.8344340998912495 \n",
      "\n",
      "Best depth: 7 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[186   3   7   0   0   0]\n",
      " [ 24 137  13   0   0   3]\n",
      " [ 21  10 140   1   0   0]\n",
      " [  0   0   0 172  14   1]\n",
      " [  2   0   0  27 150   0]\n",
      " [  0   5   0   3   0 144]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.95      0.87       196\n",
      "           2       0.88      0.77      0.83       177\n",
      "           3       0.88      0.81      0.84       172\n",
      "           4       0.85      0.92      0.88       187\n",
      "           5       0.91      0.84      0.87       179\n",
      "           6       0.97      0.95      0.96       152\n",
      "\n",
      "    accuracy                           0.87      1063\n",
      "   macro avg       0.88      0.87      0.88      1063\n",
      "weighted avg       0.88      0.87      0.87      1063\n",
      "\n",
      "Training set score for dc_model: 0.954845\n",
      "Testing  set score for dc_model: 0.873942\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    params_grid = [{'max_depth': [5,6,7],'random_state':[42],'n_jobs':[2]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #estimators:',dc_model.best_estimator_.n_estimators,\"\\n\")\n",
    "    print('Best jobs:',dc_model.best_estimator_.n_jobs,\"\\n\") \n",
    "    #print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2fe81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "414dcb54",
   "metadata": {},
   "source": [
    "# ...........THANK YOU.........HAPPY CODING......."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
