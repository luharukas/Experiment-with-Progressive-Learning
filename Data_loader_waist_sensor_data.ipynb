{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80f05910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "from sklearn.model_selection import train_test_split,RepeatedStratifiedKFold,GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c50981a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=pd.read_csv(r\"Data\\UCI_Data\\final_X_train.txt\", delimiter=\",\",header=None)\n",
    "Y_train=pd.read_csv(r\"Data\\UCI_Data\\final_y_train.txt\", delimiter=\",\",header=None)\n",
    "data=X_train.copy(deep=True)\n",
    "data['class']=Y_train\n",
    "data=data.sample(frac=1,ignore_index=True)\n",
    "labels=data['class']\n",
    "data=data.drop('class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b4dd281c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "      <td>4252.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.328098</td>\n",
       "      <td>-0.016892</td>\n",
       "      <td>-0.105908</td>\n",
       "      <td>-0.657875</td>\n",
       "      <td>-0.545393</td>\n",
       "      <td>-0.568119</td>\n",
       "      <td>-0.692565</td>\n",
       "      <td>-0.555668</td>\n",
       "      <td>-0.567608</td>\n",
       "      <td>-0.470899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132243</td>\n",
       "      <td>-0.289747</td>\n",
       "      <td>-0.618739</td>\n",
       "      <td>-0.026148</td>\n",
       "      <td>0.032900</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>-0.003853</td>\n",
       "      <td>-0.525921</td>\n",
       "      <td>0.070942</td>\n",
       "      <td>-0.022324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.342924</td>\n",
       "      <td>0.064407</td>\n",
       "      <td>0.116318</td>\n",
       "      <td>0.357233</td>\n",
       "      <td>0.428247</td>\n",
       "      <td>0.409752</td>\n",
       "      <td>0.331081</td>\n",
       "      <td>0.422234</td>\n",
       "      <td>0.415663</td>\n",
       "      <td>0.510689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.235170</td>\n",
       "      <td>0.294440</td>\n",
       "      <td>0.285924</td>\n",
       "      <td>0.327898</td>\n",
       "      <td>0.445712</td>\n",
       "      <td>0.595877</td>\n",
       "      <td>0.514740</td>\n",
       "      <td>0.434306</td>\n",
       "      <td>0.244521</td>\n",
       "      <td>0.310260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.558400</td>\n",
       "      <td>-0.885720</td>\n",
       "      <td>-1.006100</td>\n",
       "      <td>-1.000100</td>\n",
       "      <td>-0.999270</td>\n",
       "      <td>-1.005300</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.999280</td>\n",
       "      <td>-1.005400</td>\n",
       "      <td>-0.951790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.793830</td>\n",
       "      <td>-0.955590</td>\n",
       "      <td>-1.006300</td>\n",
       "      <td>-1.001000</td>\n",
       "      <td>-0.992450</td>\n",
       "      <td>-0.995690</td>\n",
       "      <td>-0.987830</td>\n",
       "      <td>-0.997540</td>\n",
       "      <td>-0.795520</td>\n",
       "      <td>-1.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.260400</td>\n",
       "      <td>-0.027013</td>\n",
       "      <td>-0.129390</td>\n",
       "      <td>-0.985930</td>\n",
       "      <td>-0.963715</td>\n",
       "      <td>-0.966993</td>\n",
       "      <td>-0.987775</td>\n",
       "      <td>-0.964815</td>\n",
       "      <td>-0.966835</td>\n",
       "      <td>-0.928032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007369</td>\n",
       "      <td>-0.500945</td>\n",
       "      <td>-0.826232</td>\n",
       "      <td>-0.120405</td>\n",
       "      <td>-0.240812</td>\n",
       "      <td>-0.554740</td>\n",
       "      <td>-0.450703</td>\n",
       "      <td>-0.800307</td>\n",
       "      <td>-0.062248</td>\n",
       "      <td>-0.135370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.277880</td>\n",
       "      <td>-0.017045</td>\n",
       "      <td>-0.108480</td>\n",
       "      <td>-0.711575</td>\n",
       "      <td>-0.555900</td>\n",
       "      <td>-0.566125</td>\n",
       "      <td>-0.753750</td>\n",
       "      <td>-0.568120</td>\n",
       "      <td>-0.563235</td>\n",
       "      <td>-0.568920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148570</td>\n",
       "      <td>-0.323800</td>\n",
       "      <td>-0.695040</td>\n",
       "      <td>0.006337</td>\n",
       "      <td>0.027131</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>-0.005088</td>\n",
       "      <td>-0.676585</td>\n",
       "      <td>0.061055</td>\n",
       "      <td>0.040008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.300503</td>\n",
       "      <td>-0.006720</td>\n",
       "      <td>-0.090234</td>\n",
       "      <td>-0.413538</td>\n",
       "      <td>-0.187390</td>\n",
       "      <td>-0.235937</td>\n",
       "      <td>-0.484675</td>\n",
       "      <td>-0.207490</td>\n",
       "      <td>-0.241415</td>\n",
       "      <td>-0.133987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292135</td>\n",
       "      <td>-0.115175</td>\n",
       "      <td>-0.494525</td>\n",
       "      <td>0.101587</td>\n",
       "      <td>0.317148</td>\n",
       "      <td>0.551520</td>\n",
       "      <td>0.437635</td>\n",
       "      <td>-0.496592</td>\n",
       "      <td>0.192235</td>\n",
       "      <td>0.185645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.409900</td>\n",
       "      <td>0.769080</td>\n",
       "      <td>1.658900</td>\n",
       "      <td>0.947430</td>\n",
       "      <td>1.594100</td>\n",
       "      <td>2.041200</td>\n",
       "      <td>1.024800</td>\n",
       "      <td>1.864200</td>\n",
       "      <td>2.464300</td>\n",
       "      <td>1.746400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864680</td>\n",
       "      <td>0.942590</td>\n",
       "      <td>0.933850</td>\n",
       "      <td>0.995080</td>\n",
       "      <td>1.011000</td>\n",
       "      <td>0.996520</td>\n",
       "      <td>0.993710</td>\n",
       "      <td>0.937120</td>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.629590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1            2            3            4    \\\n",
       "count  4252.000000  4252.000000  4252.000000  4252.000000  4252.000000   \n",
       "mean      0.328098    -0.016892    -0.105908    -0.657875    -0.545393   \n",
       "std       0.342924     0.064407     0.116318     0.357233     0.428247   \n",
       "min      -2.558400    -0.885720    -1.006100    -1.000100    -0.999270   \n",
       "25%       0.260400    -0.027013    -0.129390    -0.985930    -0.963715   \n",
       "50%       0.277880    -0.017045    -0.108480    -0.711575    -0.555900   \n",
       "75%       0.300503    -0.006720    -0.090234    -0.413538    -0.187390   \n",
       "max       2.409900     0.769080     1.658900     0.947430     1.594100   \n",
       "\n",
       "               5            6            7            8            9    ...  \\\n",
       "count  4252.000000  4252.000000  4252.000000  4252.000000  4252.000000  ...   \n",
       "mean     -0.568119    -0.692565    -0.555668    -0.567608    -0.470899  ...   \n",
       "std       0.409752     0.331081     0.422234     0.415663     0.510689  ...   \n",
       "min      -1.005300    -1.000000    -0.999280    -1.005400    -0.951790  ...   \n",
       "25%      -0.966993    -0.987775    -0.964815    -0.966835    -0.928032  ...   \n",
       "50%      -0.566125    -0.753750    -0.568120    -0.563235    -0.568920  ...   \n",
       "75%      -0.235937    -0.484675    -0.207490    -0.241415    -0.133987  ...   \n",
       "max       2.041200     1.024800     1.864200     2.464300     1.746400  ...   \n",
       "\n",
       "               551          552          553          554          555  \\\n",
       "count  4252.000000  4252.000000  4252.000000  4252.000000  4252.000000   \n",
       "mean      0.132243    -0.289747    -0.618739    -0.026148     0.032900   \n",
       "std       0.235170     0.294440     0.285924     0.327898     0.445712   \n",
       "min      -0.793830    -0.955590    -1.006300    -1.001000    -0.992450   \n",
       "25%      -0.007369    -0.500945    -0.826232    -0.120405    -0.240812   \n",
       "50%       0.148570    -0.323800    -0.695040     0.006337     0.027131   \n",
       "75%       0.292135    -0.115175    -0.494525     0.101587     0.317148   \n",
       "max       0.864680     0.942590     0.933850     0.995080     1.011000   \n",
       "\n",
       "               556          557          558          559          560  \n",
       "count  4252.000000  4252.000000  4252.000000  4252.000000  4252.000000  \n",
       "mean      0.002336    -0.003853    -0.525921     0.070942    -0.022324  \n",
       "std       0.595877     0.514740     0.434306     0.244521     0.310260  \n",
       "min      -0.995690    -0.987830    -0.997540    -0.795520    -1.093500  \n",
       "25%      -0.554740    -0.450703    -0.800307    -0.062248    -0.135370  \n",
       "50%       0.004438    -0.005088    -0.676585     0.061055     0.040008  \n",
       "75%       0.551520     0.437635    -0.496592     0.192235     0.185645  \n",
       "max       0.996520     0.993710     0.937120     0.825490     0.629590  \n",
       "\n",
       "[8 rows x 561 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ca791c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.181804</td>\n",
       "      <td>-0.378514</td>\n",
       "      <td>-0.183929</td>\n",
       "      <td>1.211411</td>\n",
       "      <td>1.309825</td>\n",
       "      <td>1.408020</td>\n",
       "      <td>1.188475</td>\n",
       "      <td>1.328900</td>\n",
       "      <td>1.215921</td>\n",
       "      <td>0.757288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.989513</td>\n",
       "      <td>-0.975635</td>\n",
       "      <td>-0.985204</td>\n",
       "      <td>0.429172</td>\n",
       "      <td>0.645784</td>\n",
       "      <td>-1.400297</td>\n",
       "      <td>-0.840128</td>\n",
       "      <td>-0.601489</td>\n",
       "      <td>-0.170491</td>\n",
       "      <td>0.573890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.184150</td>\n",
       "      <td>1.633734</td>\n",
       "      <td>0.180667</td>\n",
       "      <td>0.600393</td>\n",
       "      <td>0.695436</td>\n",
       "      <td>0.848391</td>\n",
       "      <td>0.547861</td>\n",
       "      <td>0.657340</td>\n",
       "      <td>0.903333</td>\n",
       "      <td>0.732822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.927380</td>\n",
       "      <td>-0.438180</td>\n",
       "      <td>-0.538949</td>\n",
       "      <td>-0.301384</td>\n",
       "      <td>-0.942178</td>\n",
       "      <td>-0.593827</td>\n",
       "      <td>-0.244962</td>\n",
       "      <td>-0.647292</td>\n",
       "      <td>0.645332</td>\n",
       "      <td>0.220493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.267839</td>\n",
       "      <td>-0.370594</td>\n",
       "      <td>-0.035439</td>\n",
       "      <td>0.828451</td>\n",
       "      <td>0.583292</td>\n",
       "      <td>0.959129</td>\n",
       "      <td>0.741130</td>\n",
       "      <td>0.679084</td>\n",
       "      <td>0.950926</td>\n",
       "      <td>1.067337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.418753</td>\n",
       "      <td>-0.089817</td>\n",
       "      <td>0.049807</td>\n",
       "      <td>1.188002</td>\n",
       "      <td>-1.640244</td>\n",
       "      <td>-1.474045</td>\n",
       "      <td>0.099351</td>\n",
       "      <td>-0.923559</td>\n",
       "      <td>0.221146</td>\n",
       "      <td>0.276369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.143716</td>\n",
       "      <td>-0.120531</td>\n",
       "      <td>-0.144120</td>\n",
       "      <td>-0.931447</td>\n",
       "      <td>-0.976858</td>\n",
       "      <td>-0.990084</td>\n",
       "      <td>-0.907031</td>\n",
       "      <td>-0.967569</td>\n",
       "      <td>-0.977931</td>\n",
       "      <td>-0.897562</td>\n",
       "      <td>...</td>\n",
       "      <td>1.119604</td>\n",
       "      <td>0.694706</td>\n",
       "      <td>0.909472</td>\n",
       "      <td>-0.047102</td>\n",
       "      <td>-0.200315</td>\n",
       "      <td>-1.302698</td>\n",
       "      <td>0.068707</td>\n",
       "      <td>-0.857238</td>\n",
       "      <td>0.380047</td>\n",
       "      <td>0.043309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.139441</td>\n",
       "      <td>0.110749</td>\n",
       "      <td>-0.060632</td>\n",
       "      <td>0.080504</td>\n",
       "      <td>-0.714361</td>\n",
       "      <td>-1.026818</td>\n",
       "      <td>0.053301</td>\n",
       "      <td>-0.679992</td>\n",
       "      <td>-1.011207</td>\n",
       "      <td>0.218865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007769</td>\n",
       "      <td>-0.795034</td>\n",
       "      <td>-0.817553</td>\n",
       "      <td>-2.340884</td>\n",
       "      <td>2.168291</td>\n",
       "      <td>0.529692</td>\n",
       "      <td>-1.168138</td>\n",
       "      <td>-0.796306</td>\n",
       "      <td>0.473219</td>\n",
       "      <td>0.150950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>-0.139924</td>\n",
       "      <td>0.114893</td>\n",
       "      <td>0.059235</td>\n",
       "      <td>-0.895528</td>\n",
       "      <td>-0.996218</td>\n",
       "      <td>-0.995258</td>\n",
       "      <td>-0.868063</td>\n",
       "      <td>-0.993955</td>\n",
       "      <td>-0.988470</td>\n",
       "      <td>-0.846488</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.907530</td>\n",
       "      <td>0.652067</td>\n",
       "      <td>0.452444</td>\n",
       "      <td>0.080805</td>\n",
       "      <td>-1.447631</td>\n",
       "      <td>-0.490337</td>\n",
       "      <td>0.471289</td>\n",
       "      <td>2.027729</td>\n",
       "      <td>1.959036</td>\n",
       "      <td>-1.911189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>5.913668</td>\n",
       "      <td>0.086536</td>\n",
       "      <td>-2.597257</td>\n",
       "      <td>2.339098</td>\n",
       "      <td>0.352206</td>\n",
       "      <td>1.767348</td>\n",
       "      <td>2.673032</td>\n",
       "      <td>0.333665</td>\n",
       "      <td>1.476666</td>\n",
       "      <td>2.333845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.885190</td>\n",
       "      <td>0.113202</td>\n",
       "      <td>-0.130891</td>\n",
       "      <td>-2.915093</td>\n",
       "      <td>0.072979</td>\n",
       "      <td>-1.279066</td>\n",
       "      <td>-1.244399</td>\n",
       "      <td>-0.594489</td>\n",
       "      <td>0.163025</td>\n",
       "      <td>0.633267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>-0.268510</td>\n",
       "      <td>0.415814</td>\n",
       "      <td>-0.721743</td>\n",
       "      <td>0.074709</td>\n",
       "      <td>0.742705</td>\n",
       "      <td>0.772067</td>\n",
       "      <td>0.046293</td>\n",
       "      <td>0.854623</td>\n",
       "      <td>0.761880</td>\n",
       "      <td>0.211149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184470</td>\n",
       "      <td>-0.011659</td>\n",
       "      <td>-0.136628</td>\n",
       "      <td>-0.294338</td>\n",
       "      <td>1.065633</td>\n",
       "      <td>0.801777</td>\n",
       "      <td>0.856811</td>\n",
       "      <td>0.026506</td>\n",
       "      <td>-0.321213</td>\n",
       "      <td>1.195797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>-0.167310</td>\n",
       "      <td>0.539721</td>\n",
       "      <td>-0.441100</td>\n",
       "      <td>-0.610247</td>\n",
       "      <td>-0.369146</td>\n",
       "      <td>-0.559602</td>\n",
       "      <td>-0.666185</td>\n",
       "      <td>-0.345636</td>\n",
       "      <td>-0.545414</td>\n",
       "      <td>-0.410868</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.376156</td>\n",
       "      <td>0.807390</td>\n",
       "      <td>0.675117</td>\n",
       "      <td>-0.127627</td>\n",
       "      <td>1.318607</td>\n",
       "      <td>-0.408331</td>\n",
       "      <td>0.338566</td>\n",
       "      <td>-0.572820</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.650609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>0.043111</td>\n",
       "      <td>-0.309243</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.726208</td>\n",
       "      <td>0.931006</td>\n",
       "      <td>0.707655</td>\n",
       "      <td>0.706512</td>\n",
       "      <td>0.834466</td>\n",
       "      <td>0.776822</td>\n",
       "      <td>0.837032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.729416</td>\n",
       "      <td>2.034394</td>\n",
       "      <td>1.850140</td>\n",
       "      <td>-1.447148</td>\n",
       "      <td>-0.815242</td>\n",
       "      <td>-1.298082</td>\n",
       "      <td>-1.397329</td>\n",
       "      <td>-0.658023</td>\n",
       "      <td>-0.093245</td>\n",
       "      <td>0.540753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252 rows × 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.181804 -0.378514 -0.183929  1.211411  1.309825  1.408020  1.188475   \n",
       "1     0.184150  1.633734  0.180667  0.600393  0.695436  0.848391  0.547861   \n",
       "2    -0.267839 -0.370594 -0.035439  0.828451  0.583292  0.959129  0.741130   \n",
       "3    -0.143716 -0.120531 -0.144120 -0.931447 -0.976858 -0.990084 -0.907031   \n",
       "4     0.139441  0.110749 -0.060632  0.080504 -0.714361 -1.026818  0.053301   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4247 -0.139924  0.114893  0.059235 -0.895528 -0.996218 -0.995258 -0.868063   \n",
       "4248  5.913668  0.086536 -2.597257  2.339098  0.352206  1.767348  2.673032   \n",
       "4249 -0.268510  0.415814 -0.721743  0.074709  0.742705  0.772067  0.046293   \n",
       "4250 -0.167310  0.539721 -0.441100 -0.610247 -0.369146 -0.559602 -0.666185   \n",
       "4251  0.043111 -0.309243  0.034722  0.726208  0.931006  0.707655  0.706512   \n",
       "\n",
       "           7         8         9    ...       551       552       553  \\\n",
       "0     1.328900  1.215921  0.757288  ...  0.989513 -0.975635 -0.985204   \n",
       "1     0.657340  0.903333  0.732822  ...  0.927380 -0.438180 -0.538949   \n",
       "2     0.679084  0.950926  1.067337  ...  0.418753 -0.089817  0.049807   \n",
       "3    -0.967569 -0.977931 -0.897562  ...  1.119604  0.694706  0.909472   \n",
       "4    -0.679992 -1.011207  0.218865  ...  0.007769 -0.795034 -0.817553   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4247 -0.993955 -0.988470 -0.846488  ... -0.907530  0.652067  0.452444   \n",
       "4248  0.333665  1.476666  2.333845  ... -0.885190  0.113202 -0.130891   \n",
       "4249  0.854623  0.761880  0.211149  ...  0.184470 -0.011659 -0.136628   \n",
       "4250 -0.345636 -0.545414 -0.410868  ... -1.376156  0.807390  0.675117   \n",
       "4251  0.834466  0.776822  0.837032  ...  0.729416  2.034394  1.850140   \n",
       "\n",
       "           554       555       556       557       558       559       560  \n",
       "0     0.429172  0.645784 -1.400297 -0.840128 -0.601489 -0.170491  0.573890  \n",
       "1    -0.301384 -0.942178 -0.593827 -0.244962 -0.647292  0.645332  0.220493  \n",
       "2     1.188002 -1.640244 -1.474045  0.099351 -0.923559  0.221146  0.276369  \n",
       "3    -0.047102 -0.200315 -1.302698  0.068707 -0.857238  0.380047  0.043309  \n",
       "4    -2.340884  2.168291  0.529692 -1.168138 -0.796306  0.473219  0.150950  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4247  0.080805 -1.447631 -0.490337  0.471289  2.027729  1.959036 -1.911189  \n",
       "4248 -2.915093  0.072979 -1.279066 -1.244399 -0.594489  0.163025  0.633267  \n",
       "4249 -0.294338  1.065633  0.801777  0.856811  0.026506 -0.321213  1.195797  \n",
       "4250 -0.127627  1.318607 -0.408331  0.338566 -0.572820  0.002124  0.650609  \n",
       "4251 -1.447148 -0.815242 -1.298082 -1.397329 -0.658023 -0.093245  0.540753  \n",
       "\n",
       "[4252 rows x 561 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler()\n",
    "data_scaled = pd.DataFrame(scalar.fit_transform(data), columns=data.columns)\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fa018a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension Reduction done\n"
     ]
    }
   ],
   "source": [
    "# Method 1 for dimension reduction\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model1=RandomForestRegressor(random_state=2,max_depth=10)\n",
    "model1.fit(data,labels)\n",
    "print(\"Dimension Reduction done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6251872d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features=data.columns\n",
    "importances=model1.feature_importances_\n",
    "indices = np.argsort(importances)[-20:]  # top 80 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "selected_features=[features[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1d20b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_after_RF=data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a1140169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>302</th>\n",
       "      <th>476</th>\n",
       "      <th>69</th>\n",
       "      <th>57</th>\n",
       "      <th>509</th>\n",
       "      <th>330</th>\n",
       "      <th>560</th>\n",
       "      <th>303</th>\n",
       "      <th>139</th>\n",
       "      <th>51</th>\n",
       "      <th>42</th>\n",
       "      <th>65</th>\n",
       "      <th>37</th>\n",
       "      <th>54</th>\n",
       "      <th>49</th>\n",
       "      <th>504</th>\n",
       "      <th>181</th>\n",
       "      <th>63</th>\n",
       "      <th>56</th>\n",
       "      <th>102</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.812300</td>\n",
       "      <td>-0.79845</td>\n",
       "      <td>0.183860</td>\n",
       "      <td>-0.98223</td>\n",
       "      <td>-0.357970</td>\n",
       "      <td>-0.59567</td>\n",
       "      <td>0.155710</td>\n",
       "      <td>-0.67741</td>\n",
       "      <td>0.043679</td>\n",
       "      <td>-0.170100</td>\n",
       "      <td>-0.185710</td>\n",
       "      <td>-0.28371</td>\n",
       "      <td>0.060380</td>\n",
       "      <td>-0.196020</td>\n",
       "      <td>0.85742</td>\n",
       "      <td>-0.24683</td>\n",
       "      <td>-0.34622</td>\n",
       "      <td>-0.192380</td>\n",
       "      <td>0.75438</td>\n",
       "      <td>0.66965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.859070</td>\n",
       "      <td>-0.89067</td>\n",
       "      <td>-0.317770</td>\n",
       "      <td>-0.93661</td>\n",
       "      <td>-0.360600</td>\n",
       "      <td>-0.76223</td>\n",
       "      <td>0.046078</td>\n",
       "      <td>-0.85571</td>\n",
       "      <td>-0.492890</td>\n",
       "      <td>-0.006883</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>-0.62215</td>\n",
       "      <td>-0.142340</td>\n",
       "      <td>-0.050869</td>\n",
       "      <td>0.79233</td>\n",
       "      <td>-0.26603</td>\n",
       "      <td>-0.57348</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.60876</td>\n",
       "      <td>0.53391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.779560</td>\n",
       "      <td>-0.97099</td>\n",
       "      <td>-0.676350</td>\n",
       "      <td>-0.99624</td>\n",
       "      <td>-0.498650</td>\n",
       "      <td>-0.61913</td>\n",
       "      <td>0.063412</td>\n",
       "      <td>-0.94452</td>\n",
       "      <td>-0.396320</td>\n",
       "      <td>-0.050900</td>\n",
       "      <td>-0.055481</td>\n",
       "      <td>-0.64045</td>\n",
       "      <td>0.175420</td>\n",
       "      <td>-0.065852</td>\n",
       "      <td>0.83856</td>\n",
       "      <td>-0.38402</td>\n",
       "      <td>-0.65520</td>\n",
       "      <td>-0.966820</td>\n",
       "      <td>0.68541</td>\n",
       "      <td>0.74086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.999950</td>\n",
       "      <td>-0.99989</td>\n",
       "      <td>-0.275170</td>\n",
       "      <td>-0.98303</td>\n",
       "      <td>-0.984400</td>\n",
       "      <td>-0.99924</td>\n",
       "      <td>-0.008889</td>\n",
       "      <td>-0.99994</td>\n",
       "      <td>-0.967240</td>\n",
       "      <td>0.040792</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>-0.29605</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.037238</td>\n",
       "      <td>0.81959</td>\n",
       "      <td>-0.97698</td>\n",
       "      <td>-0.98761</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.71984</td>\n",
       "      <td>-0.60071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.908560</td>\n",
       "      <td>-0.99994</td>\n",
       "      <td>-0.943640</td>\n",
       "      <td>-0.97048</td>\n",
       "      <td>-0.867150</td>\n",
       "      <td>-0.99979</td>\n",
       "      <td>0.024504</td>\n",
       "      <td>-0.99151</td>\n",
       "      <td>-0.959710</td>\n",
       "      <td>-0.006231</td>\n",
       "      <td>-0.003264</td>\n",
       "      <td>-1.00400</td>\n",
       "      <td>-0.766310</td>\n",
       "      <td>-0.003740</td>\n",
       "      <td>0.92372</td>\n",
       "      <td>-0.58337</td>\n",
       "      <td>-0.99146</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.66802</td>\n",
       "      <td>-0.62306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>-0.999680</td>\n",
       "      <td>-0.99989</td>\n",
       "      <td>-0.324440</td>\n",
       "      <td>-0.36750</td>\n",
       "      <td>-0.973970</td>\n",
       "      <td>-0.99957</td>\n",
       "      <td>-0.615220</td>\n",
       "      <td>-0.99971</td>\n",
       "      <td>-0.990710</td>\n",
       "      <td>0.731970</td>\n",
       "      <td>0.742610</td>\n",
       "      <td>-0.38181</td>\n",
       "      <td>0.087882</td>\n",
       "      <td>0.740840</td>\n",
       "      <td>-0.22874</td>\n",
       "      <td>-0.97070</td>\n",
       "      <td>-0.99403</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.91100</td>\n",
       "      <td>-0.43793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>-0.080192</td>\n",
       "      <td>-0.94526</td>\n",
       "      <td>-0.295270</td>\n",
       "      <td>-0.99872</td>\n",
       "      <td>0.075548</td>\n",
       "      <td>-0.36811</td>\n",
       "      <td>0.174130</td>\n",
       "      <td>-0.91368</td>\n",
       "      <td>-0.434570</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>-0.112690</td>\n",
       "      <td>-0.88804</td>\n",
       "      <td>-0.373220</td>\n",
       "      <td>-0.273200</td>\n",
       "      <td>0.94308</td>\n",
       "      <td>0.56609</td>\n",
       "      <td>-0.83534</td>\n",
       "      <td>-0.493930</td>\n",
       "      <td>-0.20025</td>\n",
       "      <td>0.26091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>-0.961840</td>\n",
       "      <td>-0.99159</td>\n",
       "      <td>-0.605760</td>\n",
       "      <td>-0.95529</td>\n",
       "      <td>-0.598010</td>\n",
       "      <td>-0.75308</td>\n",
       "      <td>0.348640</td>\n",
       "      <td>-0.90054</td>\n",
       "      <td>-0.419870</td>\n",
       "      <td>-0.428130</td>\n",
       "      <td>-0.443980</td>\n",
       "      <td>-0.36786</td>\n",
       "      <td>0.294750</td>\n",
       "      <td>-0.461840</td>\n",
       "      <td>0.72774</td>\n",
       "      <td>-0.56634</td>\n",
       "      <td>-0.66900</td>\n",
       "      <td>0.228160</td>\n",
       "      <td>0.46553</td>\n",
       "      <td>0.51279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>-0.993120</td>\n",
       "      <td>-0.99833</td>\n",
       "      <td>-0.666740</td>\n",
       "      <td>-0.99603</td>\n",
       "      <td>-0.823690</td>\n",
       "      <td>-0.97822</td>\n",
       "      <td>0.179510</td>\n",
       "      <td>-0.99067</td>\n",
       "      <td>-0.908980</td>\n",
       "      <td>-0.220640</td>\n",
       "      <td>-0.222060</td>\n",
       "      <td>-0.55032</td>\n",
       "      <td>-0.246870</td>\n",
       "      <td>-0.238240</td>\n",
       "      <td>0.85393</td>\n",
       "      <td>-0.79750</td>\n",
       "      <td>-0.96242</td>\n",
       "      <td>0.086704</td>\n",
       "      <td>0.80464</td>\n",
       "      <td>-0.04497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>-0.876390</td>\n",
       "      <td>-0.91202</td>\n",
       "      <td>0.099425</td>\n",
       "      <td>-0.99068</td>\n",
       "      <td>-0.555830</td>\n",
       "      <td>-0.77128</td>\n",
       "      <td>0.145430</td>\n",
       "      <td>-0.70946</td>\n",
       "      <td>0.052827</td>\n",
       "      <td>-0.168870</td>\n",
       "      <td>-0.171460</td>\n",
       "      <td>-0.46310</td>\n",
       "      <td>0.087885</td>\n",
       "      <td>-0.184020</td>\n",
       "      <td>0.84474</td>\n",
       "      <td>-0.41651</td>\n",
       "      <td>-0.72850</td>\n",
       "      <td>-0.194380</td>\n",
       "      <td>0.76881</td>\n",
       "      <td>0.69839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           302      476        69       57       509      330       560  \\\n",
       "0    -0.812300 -0.79845  0.183860 -0.98223 -0.357970 -0.59567  0.155710   \n",
       "1    -0.859070 -0.89067 -0.317770 -0.93661 -0.360600 -0.76223  0.046078   \n",
       "2    -0.779560 -0.97099 -0.676350 -0.99624 -0.498650 -0.61913  0.063412   \n",
       "3    -0.999950 -0.99989 -0.275170 -0.98303 -0.984400 -0.99924 -0.008889   \n",
       "4    -0.908560 -0.99994 -0.943640 -0.97048 -0.867150 -0.99979  0.024504   \n",
       "...        ...      ...       ...      ...       ...      ...       ...   \n",
       "4247 -0.999680 -0.99989 -0.324440 -0.36750 -0.973970 -0.99957 -0.615220   \n",
       "4248 -0.080192 -0.94526 -0.295270 -0.99872  0.075548 -0.36811  0.174130   \n",
       "4249 -0.961840 -0.99159 -0.605760 -0.95529 -0.598010 -0.75308  0.348640   \n",
       "4250 -0.993120 -0.99833 -0.666740 -0.99603 -0.823690 -0.97822  0.179510   \n",
       "4251 -0.876390 -0.91202  0.099425 -0.99068 -0.555830 -0.77128  0.145430   \n",
       "\n",
       "          303       139        51        42       65        37        54  \\\n",
       "0    -0.67741  0.043679 -0.170100 -0.185710 -0.28371  0.060380 -0.196020   \n",
       "1    -0.85571 -0.492890 -0.006883 -0.031985 -0.62215 -0.142340 -0.050869   \n",
       "2    -0.94452 -0.396320 -0.050900 -0.055481 -0.64045  0.175420 -0.065852   \n",
       "3    -0.99994 -0.967240  0.040792  0.041905 -0.29605  0.202020  0.037238   \n",
       "4    -0.99151 -0.959710 -0.006231 -0.003264 -1.00400 -0.766310 -0.003740   \n",
       "...       ...       ...       ...       ...      ...       ...       ...   \n",
       "4247 -0.99971 -0.990710  0.731970  0.742610 -0.38181  0.087882  0.740840   \n",
       "4248 -0.91368 -0.434570  0.005599 -0.112690 -0.88804 -0.373220 -0.273200   \n",
       "4249 -0.90054 -0.419870 -0.428130 -0.443980 -0.36786  0.294750 -0.461840   \n",
       "4250 -0.99067 -0.908980 -0.220640 -0.222060 -0.55032 -0.246870 -0.238240   \n",
       "4251 -0.70946  0.052827 -0.168870 -0.171460 -0.46310  0.087885 -0.184020   \n",
       "\n",
       "           49      504      181        63       56      102  \n",
       "0     0.85742 -0.24683 -0.34622 -0.192380  0.75438  0.66965  \n",
       "1     0.79233 -0.26603 -0.57348 -1.000000  0.60876  0.53391  \n",
       "2     0.83856 -0.38402 -0.65520 -0.966820  0.68541  0.74086  \n",
       "3     0.81959 -0.97698 -0.98761 -1.000000  0.71984 -0.60071  \n",
       "4     0.92372 -0.58337 -0.99146 -1.000000  0.66802 -0.62306  \n",
       "...       ...      ...      ...       ...      ...      ...  \n",
       "4247 -0.22874 -0.97070 -0.99403 -1.000000 -0.91100 -0.43793  \n",
       "4248  0.94308  0.56609 -0.83534 -0.493930 -0.20025  0.26091  \n",
       "4249  0.72774 -0.56634 -0.66900  0.228160  0.46553  0.51279  \n",
       "4250  0.85393 -0.79750 -0.96242  0.086704  0.80464 -0.04497  \n",
       "4251  0.84474 -0.41651 -0.72850 -0.194380  0.76881  0.69839  \n",
       "\n",
       "[4252 rows x 20 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_after_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e5da024e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       2\n",
       "2       2\n",
       "3       5\n",
       "4       5\n",
       "       ..\n",
       "4247    6\n",
       "4248    1\n",
       "4249    2\n",
       "4250    5\n",
       "4251    1\n",
       "Name: class, Length: 4252, dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94959865",
   "metadata": {},
   "source": [
    "<h1 > Halt and Stop </h1>\n",
    "<h2> PCA Code </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1288828d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.629531</td>\n",
       "      <td>14.856823</td>\n",
       "      <td>10.593409</td>\n",
       "      <td>-3.781079</td>\n",
       "      <td>-0.230631</td>\n",
       "      <td>-2.629830</td>\n",
       "      <td>2.825996</td>\n",
       "      <td>-8.828905</td>\n",
       "      <td>-3.269102</td>\n",
       "      <td>-6.578480</td>\n",
       "      <td>2.159592</td>\n",
       "      <td>1.837202</td>\n",
       "      <td>-1.987490</td>\n",
       "      <td>1.322127</td>\n",
       "      <td>2.970779</td>\n",
       "      <td>-3.654699</td>\n",
       "      <td>2.091649</td>\n",
       "      <td>-3.082109</td>\n",
       "      <td>0.733009</td>\n",
       "      <td>-0.820087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.315133</td>\n",
       "      <td>4.921955</td>\n",
       "      <td>-3.995595</td>\n",
       "      <td>-2.255632</td>\n",
       "      <td>-0.461161</td>\n",
       "      <td>0.916194</td>\n",
       "      <td>1.667741</td>\n",
       "      <td>-3.197429</td>\n",
       "      <td>1.044379</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.168932</td>\n",
       "      <td>-0.695841</td>\n",
       "      <td>-1.567962</td>\n",
       "      <td>0.494077</td>\n",
       "      <td>-0.723823</td>\n",
       "      <td>-0.442762</td>\n",
       "      <td>1.914127</td>\n",
       "      <td>2.452142</td>\n",
       "      <td>2.941270</td>\n",
       "      <td>-0.875028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.155627</td>\n",
       "      <td>2.154239</td>\n",
       "      <td>-5.819888</td>\n",
       "      <td>-1.102799</td>\n",
       "      <td>0.287754</td>\n",
       "      <td>1.054957</td>\n",
       "      <td>-0.567482</td>\n",
       "      <td>-2.257072</td>\n",
       "      <td>-1.038818</td>\n",
       "      <td>-1.656368</td>\n",
       "      <td>-0.147627</td>\n",
       "      <td>-0.803911</td>\n",
       "      <td>-0.978908</td>\n",
       "      <td>-1.965637</td>\n",
       "      <td>-0.601632</td>\n",
       "      <td>-1.967115</td>\n",
       "      <td>-2.640024</td>\n",
       "      <td>2.022526</td>\n",
       "      <td>2.801107</td>\n",
       "      <td>-0.959965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.103288</td>\n",
       "      <td>3.767917</td>\n",
       "      <td>-6.018159</td>\n",
       "      <td>-1.677049</td>\n",
       "      <td>1.064371</td>\n",
       "      <td>0.691963</td>\n",
       "      <td>-1.510585</td>\n",
       "      <td>-1.111558</td>\n",
       "      <td>-1.349424</td>\n",
       "      <td>-2.358492</td>\n",
       "      <td>0.466362</td>\n",
       "      <td>-1.070167</td>\n",
       "      <td>0.433423</td>\n",
       "      <td>-2.167386</td>\n",
       "      <td>-0.530678</td>\n",
       "      <td>-2.469406</td>\n",
       "      <td>-3.085323</td>\n",
       "      <td>-0.314081</td>\n",
       "      <td>0.525884</td>\n",
       "      <td>-0.751699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.893014</td>\n",
       "      <td>6.439557</td>\n",
       "      <td>-5.857893</td>\n",
       "      <td>-1.931942</td>\n",
       "      <td>0.319152</td>\n",
       "      <td>-0.173189</td>\n",
       "      <td>-0.837526</td>\n",
       "      <td>-1.445935</td>\n",
       "      <td>0.938064</td>\n",
       "      <td>-0.741610</td>\n",
       "      <td>-1.252487</td>\n",
       "      <td>0.630344</td>\n",
       "      <td>-1.756081</td>\n",
       "      <td>-1.811645</td>\n",
       "      <td>0.272141</td>\n",
       "      <td>-1.653960</td>\n",
       "      <td>-1.236539</td>\n",
       "      <td>-0.055979</td>\n",
       "      <td>0.671251</td>\n",
       "      <td>-2.306505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>-15.346321</td>\n",
       "      <td>-5.223540</td>\n",
       "      <td>2.066638</td>\n",
       "      <td>-0.071040</td>\n",
       "      <td>-2.737702</td>\n",
       "      <td>2.245572</td>\n",
       "      <td>-2.540565</td>\n",
       "      <td>-2.358935</td>\n",
       "      <td>-0.237690</td>\n",
       "      <td>-1.986813</td>\n",
       "      <td>1.721676</td>\n",
       "      <td>0.451884</td>\n",
       "      <td>0.742498</td>\n",
       "      <td>-0.463987</td>\n",
       "      <td>4.093653</td>\n",
       "      <td>4.451495</td>\n",
       "      <td>-1.226662</td>\n",
       "      <td>-5.124464</td>\n",
       "      <td>2.369892</td>\n",
       "      <td>2.796131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>-15.632898</td>\n",
       "      <td>-7.127824</td>\n",
       "      <td>2.817008</td>\n",
       "      <td>0.017201</td>\n",
       "      <td>-2.730833</td>\n",
       "      <td>2.092721</td>\n",
       "      <td>-3.115867</td>\n",
       "      <td>-3.415086</td>\n",
       "      <td>-1.001908</td>\n",
       "      <td>-2.261312</td>\n",
       "      <td>1.836727</td>\n",
       "      <td>0.698259</td>\n",
       "      <td>0.570752</td>\n",
       "      <td>-0.425441</td>\n",
       "      <td>4.355570</td>\n",
       "      <td>5.076866</td>\n",
       "      <td>-2.319139</td>\n",
       "      <td>-4.613177</td>\n",
       "      <td>3.123821</td>\n",
       "      <td>1.958682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>-17.016423</td>\n",
       "      <td>-9.349355</td>\n",
       "      <td>5.325734</td>\n",
       "      <td>1.912351</td>\n",
       "      <td>-1.780125</td>\n",
       "      <td>1.598338</td>\n",
       "      <td>-3.942679</td>\n",
       "      <td>-2.457444</td>\n",
       "      <td>-1.664441</td>\n",
       "      <td>1.860920</td>\n",
       "      <td>0.076592</td>\n",
       "      <td>1.036340</td>\n",
       "      <td>-5.423483</td>\n",
       "      <td>-1.701017</td>\n",
       "      <td>4.847681</td>\n",
       "      <td>2.904304</td>\n",
       "      <td>-1.793181</td>\n",
       "      <td>-3.872349</td>\n",
       "      <td>1.418040</td>\n",
       "      <td>0.675944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>-17.054935</td>\n",
       "      <td>-8.611769</td>\n",
       "      <td>3.950561</td>\n",
       "      <td>1.225732</td>\n",
       "      <td>0.202216</td>\n",
       "      <td>2.609193</td>\n",
       "      <td>-3.498617</td>\n",
       "      <td>-0.149453</td>\n",
       "      <td>-2.515303</td>\n",
       "      <td>-0.557511</td>\n",
       "      <td>1.498262</td>\n",
       "      <td>0.026294</td>\n",
       "      <td>-3.384069</td>\n",
       "      <td>-3.548249</td>\n",
       "      <td>3.816402</td>\n",
       "      <td>2.331627</td>\n",
       "      <td>-2.298082</td>\n",
       "      <td>-3.585890</td>\n",
       "      <td>-0.505159</td>\n",
       "      <td>1.327229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>-16.708425</td>\n",
       "      <td>-10.328057</td>\n",
       "      <td>4.876636</td>\n",
       "      <td>0.959383</td>\n",
       "      <td>-3.124137</td>\n",
       "      <td>1.836876</td>\n",
       "      <td>-3.873189</td>\n",
       "      <td>-2.402532</td>\n",
       "      <td>-1.433123</td>\n",
       "      <td>-0.705269</td>\n",
       "      <td>2.633819</td>\n",
       "      <td>1.115383</td>\n",
       "      <td>-3.397099</td>\n",
       "      <td>-3.938887</td>\n",
       "      <td>3.231070</td>\n",
       "      <td>3.083693</td>\n",
       "      <td>-0.378234</td>\n",
       "      <td>-1.364739</td>\n",
       "      <td>0.576016</td>\n",
       "      <td>-1.010143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2         3         4         5         6   \\\n",
       "0     17.629531  14.856823  10.593409 -3.781079 -0.230631 -2.629830  2.825996   \n",
       "1      8.315133   4.921955  -3.995595 -2.255632 -0.461161  0.916194  1.667741   \n",
       "2      5.155627   2.154239  -5.819888 -1.102799  0.287754  1.054957 -0.567482   \n",
       "3      3.103288   3.767917  -6.018159 -1.677049  1.064371  0.691963 -1.510585   \n",
       "4      1.893014   6.439557  -5.857893 -1.931942  0.319152 -0.173189 -0.837526   \n",
       "...         ...        ...        ...       ...       ...       ...       ...   \n",
       "4247 -15.346321  -5.223540   2.066638 -0.071040 -2.737702  2.245572 -2.540565   \n",
       "4248 -15.632898  -7.127824   2.817008  0.017201 -2.730833  2.092721 -3.115867   \n",
       "4249 -17.016423  -9.349355   5.325734  1.912351 -1.780125  1.598338 -3.942679   \n",
       "4250 -17.054935  -8.611769   3.950561  1.225732  0.202216  2.609193 -3.498617   \n",
       "4251 -16.708425 -10.328057   4.876636  0.959383 -3.124137  1.836876 -3.873189   \n",
       "\n",
       "            7         8         9         10        11        12        13  \\\n",
       "0    -8.828905 -3.269102 -6.578480  2.159592  1.837202 -1.987490  1.322127   \n",
       "1    -3.197429  1.044379  0.097800  0.168932 -0.695841 -1.567962  0.494077   \n",
       "2    -2.257072 -1.038818 -1.656368 -0.147627 -0.803911 -0.978908 -1.965637   \n",
       "3    -1.111558 -1.349424 -2.358492  0.466362 -1.070167  0.433423 -2.167386   \n",
       "4    -1.445935  0.938064 -0.741610 -1.252487  0.630344 -1.756081 -1.811645   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4247 -2.358935 -0.237690 -1.986813  1.721676  0.451884  0.742498 -0.463987   \n",
       "4248 -3.415086 -1.001908 -2.261312  1.836727  0.698259  0.570752 -0.425441   \n",
       "4249 -2.457444 -1.664441  1.860920  0.076592  1.036340 -5.423483 -1.701017   \n",
       "4250 -0.149453 -2.515303 -0.557511  1.498262  0.026294 -3.384069 -3.548249   \n",
       "4251 -2.402532 -1.433123 -0.705269  2.633819  1.115383 -3.397099 -3.938887   \n",
       "\n",
       "            14        15        16        17        18        19  \n",
       "0     2.970779 -3.654699  2.091649 -3.082109  0.733009 -0.820087  \n",
       "1    -0.723823 -0.442762  1.914127  2.452142  2.941270 -0.875028  \n",
       "2    -0.601632 -1.967115 -2.640024  2.022526  2.801107 -0.959965  \n",
       "3    -0.530678 -2.469406 -3.085323 -0.314081  0.525884 -0.751699  \n",
       "4     0.272141 -1.653960 -1.236539 -0.055979  0.671251 -2.306505  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "4247  4.093653  4.451495 -1.226662 -5.124464  2.369892  2.796131  \n",
       "4248  4.355570  5.076866 -2.319139 -4.613177  3.123821  1.958682  \n",
       "4249  4.847681  2.904304 -1.793181 -3.872349  1.418040  0.675944  \n",
       "4250  3.816402  2.331627 -2.298082 -3.585890 -0.505159  1.327229  \n",
       "4251  3.231070  3.083693 -0.378234 -1.364739  0.576016 -1.010143  \n",
       "\n",
       "[4252 rows x 20 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_1 = PCA(n_components=20)\n",
    "principalComponents = pca_1.fit_transform(X_train)\n",
    "data_after_RF=pd.DataFrame(principalComponents)\n",
    "data_after_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258a1eb",
   "metadata": {},
   "source": [
    "<h2> PCA STOP </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "64dfddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Percentages are: [5, 10, 15, 20, 25]\n",
      "Splitting Position are: [0, 213, 638, 1276, 2126, 3189]\n"
     ]
    }
   ],
   "source": [
    "# Defining Splitter for the dataset (Common for all)\n",
    "def splitter(len_array,lower_bound, incrementor, start_idx):\n",
    "    final=0\n",
    "    per=[]\n",
    "    sp=[start_idx]\n",
    "    for i in range(lower_bound,100,incrementor):\n",
    "        if final+i>100:\n",
    "            break\n",
    "        per.append(i)\n",
    "        final+=i\n",
    "    for i in per:\n",
    "        len_of_data=round((i/100)*len_array)\n",
    "        sp.append(sp[-1]+len_of_data)\n",
    "    return per,sp\n",
    "\n",
    "lower_bound=5\n",
    "incrementor=5\n",
    "start_idx=0\n",
    "\n",
    "        \n",
    "spliting_percentage, spliting_position=splitter(len(data),lower_bound,incrementor,start_idx)\n",
    "print(\"Splitting Percentages are: {}\".format(spliting_percentage))\n",
    "print(\"Splitting Position are: {}\".format(spliting_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4ced02df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_experiment=data_after_RF.copy(deep=True)\n",
    "labels_for_experiment=labels.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c2339956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>302</th>\n",
       "      <th>476</th>\n",
       "      <th>69</th>\n",
       "      <th>57</th>\n",
       "      <th>509</th>\n",
       "      <th>330</th>\n",
       "      <th>560</th>\n",
       "      <th>303</th>\n",
       "      <th>139</th>\n",
       "      <th>51</th>\n",
       "      <th>42</th>\n",
       "      <th>65</th>\n",
       "      <th>37</th>\n",
       "      <th>54</th>\n",
       "      <th>49</th>\n",
       "      <th>504</th>\n",
       "      <th>181</th>\n",
       "      <th>63</th>\n",
       "      <th>56</th>\n",
       "      <th>102</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.812300</td>\n",
       "      <td>-0.79845</td>\n",
       "      <td>0.183860</td>\n",
       "      <td>-0.98223</td>\n",
       "      <td>-0.357970</td>\n",
       "      <td>-0.59567</td>\n",
       "      <td>0.155710</td>\n",
       "      <td>-0.67741</td>\n",
       "      <td>0.043679</td>\n",
       "      <td>-0.170100</td>\n",
       "      <td>-0.185710</td>\n",
       "      <td>-0.28371</td>\n",
       "      <td>0.060380</td>\n",
       "      <td>-0.196020</td>\n",
       "      <td>0.85742</td>\n",
       "      <td>-0.24683</td>\n",
       "      <td>-0.34622</td>\n",
       "      <td>-0.192380</td>\n",
       "      <td>0.75438</td>\n",
       "      <td>0.66965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.859070</td>\n",
       "      <td>-0.89067</td>\n",
       "      <td>-0.317770</td>\n",
       "      <td>-0.93661</td>\n",
       "      <td>-0.360600</td>\n",
       "      <td>-0.76223</td>\n",
       "      <td>0.046078</td>\n",
       "      <td>-0.85571</td>\n",
       "      <td>-0.492890</td>\n",
       "      <td>-0.006883</td>\n",
       "      <td>-0.031985</td>\n",
       "      <td>-0.62215</td>\n",
       "      <td>-0.142340</td>\n",
       "      <td>-0.050869</td>\n",
       "      <td>0.79233</td>\n",
       "      <td>-0.26603</td>\n",
       "      <td>-0.57348</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.60876</td>\n",
       "      <td>0.53391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.779560</td>\n",
       "      <td>-0.97099</td>\n",
       "      <td>-0.676350</td>\n",
       "      <td>-0.99624</td>\n",
       "      <td>-0.498650</td>\n",
       "      <td>-0.61913</td>\n",
       "      <td>0.063412</td>\n",
       "      <td>-0.94452</td>\n",
       "      <td>-0.396320</td>\n",
       "      <td>-0.050900</td>\n",
       "      <td>-0.055481</td>\n",
       "      <td>-0.64045</td>\n",
       "      <td>0.175420</td>\n",
       "      <td>-0.065852</td>\n",
       "      <td>0.83856</td>\n",
       "      <td>-0.38402</td>\n",
       "      <td>-0.65520</td>\n",
       "      <td>-0.966820</td>\n",
       "      <td>0.68541</td>\n",
       "      <td>0.74086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.999950</td>\n",
       "      <td>-0.99989</td>\n",
       "      <td>-0.275170</td>\n",
       "      <td>-0.98303</td>\n",
       "      <td>-0.984400</td>\n",
       "      <td>-0.99924</td>\n",
       "      <td>-0.008889</td>\n",
       "      <td>-0.99994</td>\n",
       "      <td>-0.967240</td>\n",
       "      <td>0.040792</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>-0.29605</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.037238</td>\n",
       "      <td>0.81959</td>\n",
       "      <td>-0.97698</td>\n",
       "      <td>-0.98761</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.71984</td>\n",
       "      <td>-0.60071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.908560</td>\n",
       "      <td>-0.99994</td>\n",
       "      <td>-0.943640</td>\n",
       "      <td>-0.97048</td>\n",
       "      <td>-0.867150</td>\n",
       "      <td>-0.99979</td>\n",
       "      <td>0.024504</td>\n",
       "      <td>-0.99151</td>\n",
       "      <td>-0.959710</td>\n",
       "      <td>-0.006231</td>\n",
       "      <td>-0.003264</td>\n",
       "      <td>-1.00400</td>\n",
       "      <td>-0.766310</td>\n",
       "      <td>-0.003740</td>\n",
       "      <td>0.92372</td>\n",
       "      <td>-0.58337</td>\n",
       "      <td>-0.99146</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.66802</td>\n",
       "      <td>-0.62306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247</th>\n",
       "      <td>-0.999680</td>\n",
       "      <td>-0.99989</td>\n",
       "      <td>-0.324440</td>\n",
       "      <td>-0.36750</td>\n",
       "      <td>-0.973970</td>\n",
       "      <td>-0.99957</td>\n",
       "      <td>-0.615220</td>\n",
       "      <td>-0.99971</td>\n",
       "      <td>-0.990710</td>\n",
       "      <td>0.731970</td>\n",
       "      <td>0.742610</td>\n",
       "      <td>-0.38181</td>\n",
       "      <td>0.087882</td>\n",
       "      <td>0.740840</td>\n",
       "      <td>-0.22874</td>\n",
       "      <td>-0.97070</td>\n",
       "      <td>-0.99403</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.91100</td>\n",
       "      <td>-0.43793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4248</th>\n",
       "      <td>-0.080192</td>\n",
       "      <td>-0.94526</td>\n",
       "      <td>-0.295270</td>\n",
       "      <td>-0.99872</td>\n",
       "      <td>0.075548</td>\n",
       "      <td>-0.36811</td>\n",
       "      <td>0.174130</td>\n",
       "      <td>-0.91368</td>\n",
       "      <td>-0.434570</td>\n",
       "      <td>0.005599</td>\n",
       "      <td>-0.112690</td>\n",
       "      <td>-0.88804</td>\n",
       "      <td>-0.373220</td>\n",
       "      <td>-0.273200</td>\n",
       "      <td>0.94308</td>\n",
       "      <td>0.56609</td>\n",
       "      <td>-0.83534</td>\n",
       "      <td>-0.493930</td>\n",
       "      <td>-0.20025</td>\n",
       "      <td>0.26091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>-0.961840</td>\n",
       "      <td>-0.99159</td>\n",
       "      <td>-0.605760</td>\n",
       "      <td>-0.95529</td>\n",
       "      <td>-0.598010</td>\n",
       "      <td>-0.75308</td>\n",
       "      <td>0.348640</td>\n",
       "      <td>-0.90054</td>\n",
       "      <td>-0.419870</td>\n",
       "      <td>-0.428130</td>\n",
       "      <td>-0.443980</td>\n",
       "      <td>-0.36786</td>\n",
       "      <td>0.294750</td>\n",
       "      <td>-0.461840</td>\n",
       "      <td>0.72774</td>\n",
       "      <td>-0.56634</td>\n",
       "      <td>-0.66900</td>\n",
       "      <td>0.228160</td>\n",
       "      <td>0.46553</td>\n",
       "      <td>0.51279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4250</th>\n",
       "      <td>-0.993120</td>\n",
       "      <td>-0.99833</td>\n",
       "      <td>-0.666740</td>\n",
       "      <td>-0.99603</td>\n",
       "      <td>-0.823690</td>\n",
       "      <td>-0.97822</td>\n",
       "      <td>0.179510</td>\n",
       "      <td>-0.99067</td>\n",
       "      <td>-0.908980</td>\n",
       "      <td>-0.220640</td>\n",
       "      <td>-0.222060</td>\n",
       "      <td>-0.55032</td>\n",
       "      <td>-0.246870</td>\n",
       "      <td>-0.238240</td>\n",
       "      <td>0.85393</td>\n",
       "      <td>-0.79750</td>\n",
       "      <td>-0.96242</td>\n",
       "      <td>0.086704</td>\n",
       "      <td>0.80464</td>\n",
       "      <td>-0.04497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4251</th>\n",
       "      <td>-0.876390</td>\n",
       "      <td>-0.91202</td>\n",
       "      <td>0.099425</td>\n",
       "      <td>-0.99068</td>\n",
       "      <td>-0.555830</td>\n",
       "      <td>-0.77128</td>\n",
       "      <td>0.145430</td>\n",
       "      <td>-0.70946</td>\n",
       "      <td>0.052827</td>\n",
       "      <td>-0.168870</td>\n",
       "      <td>-0.171460</td>\n",
       "      <td>-0.46310</td>\n",
       "      <td>0.087885</td>\n",
       "      <td>-0.184020</td>\n",
       "      <td>0.84474</td>\n",
       "      <td>-0.41651</td>\n",
       "      <td>-0.72850</td>\n",
       "      <td>-0.194380</td>\n",
       "      <td>0.76881</td>\n",
       "      <td>0.69839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4252 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           302      476        69       57       509      330       560  \\\n",
       "0    -0.812300 -0.79845  0.183860 -0.98223 -0.357970 -0.59567  0.155710   \n",
       "1    -0.859070 -0.89067 -0.317770 -0.93661 -0.360600 -0.76223  0.046078   \n",
       "2    -0.779560 -0.97099 -0.676350 -0.99624 -0.498650 -0.61913  0.063412   \n",
       "3    -0.999950 -0.99989 -0.275170 -0.98303 -0.984400 -0.99924 -0.008889   \n",
       "4    -0.908560 -0.99994 -0.943640 -0.97048 -0.867150 -0.99979  0.024504   \n",
       "...        ...      ...       ...      ...       ...      ...       ...   \n",
       "4247 -0.999680 -0.99989 -0.324440 -0.36750 -0.973970 -0.99957 -0.615220   \n",
       "4248 -0.080192 -0.94526 -0.295270 -0.99872  0.075548 -0.36811  0.174130   \n",
       "4249 -0.961840 -0.99159 -0.605760 -0.95529 -0.598010 -0.75308  0.348640   \n",
       "4250 -0.993120 -0.99833 -0.666740 -0.99603 -0.823690 -0.97822  0.179510   \n",
       "4251 -0.876390 -0.91202  0.099425 -0.99068 -0.555830 -0.77128  0.145430   \n",
       "\n",
       "          303       139        51        42       65        37        54  \\\n",
       "0    -0.67741  0.043679 -0.170100 -0.185710 -0.28371  0.060380 -0.196020   \n",
       "1    -0.85571 -0.492890 -0.006883 -0.031985 -0.62215 -0.142340 -0.050869   \n",
       "2    -0.94452 -0.396320 -0.050900 -0.055481 -0.64045  0.175420 -0.065852   \n",
       "3    -0.99994 -0.967240  0.040792  0.041905 -0.29605  0.202020  0.037238   \n",
       "4    -0.99151 -0.959710 -0.006231 -0.003264 -1.00400 -0.766310 -0.003740   \n",
       "...       ...       ...       ...       ...      ...       ...       ...   \n",
       "4247 -0.99971 -0.990710  0.731970  0.742610 -0.38181  0.087882  0.740840   \n",
       "4248 -0.91368 -0.434570  0.005599 -0.112690 -0.88804 -0.373220 -0.273200   \n",
       "4249 -0.90054 -0.419870 -0.428130 -0.443980 -0.36786  0.294750 -0.461840   \n",
       "4250 -0.99067 -0.908980 -0.220640 -0.222060 -0.55032 -0.246870 -0.238240   \n",
       "4251 -0.70946  0.052827 -0.168870 -0.171460 -0.46310  0.087885 -0.184020   \n",
       "\n",
       "           49      504      181        63       56      102  \n",
       "0     0.85742 -0.24683 -0.34622 -0.192380  0.75438  0.66965  \n",
       "1     0.79233 -0.26603 -0.57348 -1.000000  0.60876  0.53391  \n",
       "2     0.83856 -0.38402 -0.65520 -0.966820  0.68541  0.74086  \n",
       "3     0.81959 -0.97698 -0.98761 -1.000000  0.71984 -0.60071  \n",
       "4     0.92372 -0.58337 -0.99146 -1.000000  0.66802 -0.62306  \n",
       "...       ...      ...      ...       ...      ...      ...  \n",
       "4247 -0.22874 -0.97070 -0.99403 -1.000000 -0.91100 -0.43793  \n",
       "4248  0.94308  0.56609 -0.83534 -0.493930 -0.20025  0.26091  \n",
       "4249  0.72774 -0.56634 -0.66900  0.228160  0.46553  0.51279  \n",
       "4250  0.85393 -0.79750 -0.96242  0.086704  0.80464 -0.04497  \n",
       "4251  0.84474 -0.41651 -0.72850 -0.194380  0.76881  0.69839  \n",
       "\n",
       "[4252 rows x 20 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_after_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "13c87ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       2\n",
       "2       2\n",
       "3       5\n",
       "4       5\n",
       "       ..\n",
       "4247    6\n",
       "4248    1\n",
       "4249    2\n",
       "4250    5\n",
       "4251    1\n",
       "Name: class, Length: 4252, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_for_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b27d89c",
   "metadata": {},
   "source": [
    "# Apply ML Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6aff76ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best: 0.000000 using {'C': 5, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[140  13  19   0   0   0]\n",
      " [ 12  87  16   1   0   6]\n",
      " [ 17  15  97   0   0   0]\n",
      " [  1   1   0 112  48   9]\n",
      " [  1   1   0  36 113   0]\n",
      " [  0   5   0   0   0 101]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.81      0.82       172\n",
      "           2       0.71      0.71      0.71       122\n",
      "           3       0.73      0.75      0.74       129\n",
      "           4       0.75      0.65      0.70       171\n",
      "           5       0.70      0.75      0.72       151\n",
      "           6       0.87      0.95      0.91       106\n",
      "\n",
      "    accuracy                           0.76       851\n",
      "   macro avg       0.77      0.77      0.77       851\n",
      "weighted avg       0.76      0.76      0.76       851\n",
      "\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SImple Algorithm\n",
    "# splitting dataset into training and testing part\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    solvers = [\"lbfgs\",\"liblinear\"]\n",
    "    penalty = ['l2']\n",
    "    c_values = [5]\n",
    "    # define grid search\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0,)\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "#     print(\"Training set score for logreg_model: %f\" % grid_search.score(X_train , y_train))\n",
    "#     print(\"Testing  set score for logreg_model: %f\" % grid_search.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4b46c1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[26  8  2  0  0  0]\n",
      " [ 4 25  0  0  0  1]\n",
      " [ 7  6 26  0  0  0]\n",
      " [ 0  1  0 21  8  2]\n",
      " [ 0  0  0  6 45  0]\n",
      " [ 0  1  0  0  0 24]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.70      0.72      0.71        36\n",
      "           2       0.61      0.83      0.70        30\n",
      "           3       0.93      0.67      0.78        39\n",
      "           4       0.78      0.66      0.71        32\n",
      "           5       0.85      0.88      0.87        51\n",
      "           6       0.89      0.96      0.92        25\n",
      "\n",
      "    accuracy                           0.78       213\n",
      "   macro avg       0.79      0.79      0.78       213\n",
      "weighted avg       0.80      0.78      0.78       213\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (425,)\n",
      "Shape of testing output Y_test: (425, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[46 13  7  0  0  0]\n",
      " [14 39  4  0  0  4]\n",
      " [ 9 13 58  0  0  0]\n",
      " [ 0  3  0 61 32  3]\n",
      " [ 0  1  1 12 55  0]\n",
      " [ 0  0  0  2  0 48]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.70      0.68        66\n",
      "           2       0.57      0.64      0.60        61\n",
      "           3       0.83      0.72      0.77        80\n",
      "           4       0.81      0.62      0.70        99\n",
      "           5       0.63      0.80      0.71        69\n",
      "           6       0.87      0.96      0.91        50\n",
      "\n",
      "    accuracy                           0.72       425\n",
      "   macro avg       0.73      0.74      0.73       425\n",
      "weighted avg       0.74      0.72      0.72       425\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (638,)\n",
      "Shape of testing output Y_test: (638, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[71 16 22  0  0  1]\n",
      " [17 71 15  1  0  2]\n",
      " [11 12 66  0  0  0]\n",
      " [ 1  0  0 81 42  3]\n",
      " [ 0  0  0 21 97  1]\n",
      " [ 0  3  0  0  0 84]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.65      0.68       110\n",
      "           2       0.70      0.67      0.68       106\n",
      "           3       0.64      0.74      0.69        89\n",
      "           4       0.79      0.64      0.70       127\n",
      "           5       0.70      0.82      0.75       119\n",
      "           6       0.92      0.97      0.94        87\n",
      "\n",
      "    accuracy                           0.74       638\n",
      "   macro avg       0.74      0.75      0.74       638\n",
      "weighted avg       0.74      0.74      0.74       638\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (850,)\n",
      "Shape of testing output Y_test: (850, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[128   9  12   0   0   0]\n",
      " [ 22  84  19   1   0   1]\n",
      " [ 21  13 106   1   2   0]\n",
      " [  1   5   0 113  46   5]\n",
      " [  2   0   1  41 102   0]\n",
      " [  0   5   0   5   0 105]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.86      0.79       149\n",
      "           2       0.72      0.66      0.69       127\n",
      "           3       0.77      0.74      0.75       143\n",
      "           4       0.70      0.66      0.68       170\n",
      "           5       0.68      0.70      0.69       146\n",
      "           6       0.95      0.91      0.93       115\n",
      "\n",
      "    accuracy                           0.75       850\n",
      "   macro avg       0.76      0.76      0.76       850\n",
      "weighted avg       0.75      0.75      0.75       850\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[179   7  19   0   0   0]\n",
      " [ 17 115  24   3   0   5]\n",
      " [ 32  25 119   1   1   0]\n",
      " [  1   0   1 125  49   7]\n",
      " [  2   0   0  56 138   0]\n",
      " [  0   1   0   6   0 130]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.87      0.82       205\n",
      "           2       0.78      0.70      0.74       164\n",
      "           3       0.73      0.67      0.70       178\n",
      "           4       0.65      0.68      0.67       183\n",
      "           5       0.73      0.70      0.72       196\n",
      "           6       0.92      0.95      0.93       137\n",
      "\n",
      "    accuracy                           0.76      1063\n",
      "   macro avg       0.76      0.76      0.76      1063\n",
      "weighted avg       0.76      0.76      0.76      1063\n",
      "\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    solvers = [\"lbfgs\",\"liblinear\"]\n",
    "    penalty = ['l2','l1']\n",
    "    c_values = [10]\n",
    "    # define grid search\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=2, cv=cv, scoring='f1',error_score=0,)\n",
    "    grid_result = grid_search.fit(X_train, Y_train)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = grid_search.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170ccd98",
   "metadata": {},
   "source": [
    "# ****KNN Classifer ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "480dacab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8765042757190983 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[155   6   7   0   0   0]\n",
      " [  8 112   4   0   0   0]\n",
      " [  7   6 128   0   0   0]\n",
      " [  0   2   2 121  29   3]\n",
      " [  1   1   2  16 127   0]\n",
      " [  0   2   0   2   0 110]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.92      0.91       168\n",
      "           2       0.87      0.90      0.89       124\n",
      "           3       0.90      0.91      0.90       141\n",
      "           4       0.87      0.77      0.82       157\n",
      "           5       0.81      0.86      0.84       147\n",
      "           6       0.97      0.96      0.97       114\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.89      0.89       851\n",
      "weighted avg       0.89      0.88      0.88       851\n",
      "\n",
      "Training set score for knn_model: 0.939724\n",
      "Testing  set score for knn_model: 0.884841\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = KNeighborsClassifier()\n",
    "    params_grid = [{'n_neighbors': [2,3,4], 'weights' :['uniform'],'leaf_size':[4,5,6,7,8,9],'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "    knn_model = GridSearchCV(model, params_grid, cv=5)\n",
    "    knn_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', knn_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best #neighbors:',knn_model.best_estimator_.n_neighbors,\"\\n\") \n",
    "    print('Best weights:',knn_model.best_estimator_.weights,\"\\n\")\n",
    "    print('Best leaf_size:',knn_model.best_estimator_.leaf_size,\"\\n\")\n",
    "    print('Best algorithm:',knn_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = knn_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for knn_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for knn_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "acc79f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.7132890365448505 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[29  1  3  0  0  0]\n",
      " [ 7 24  5  0  0  0]\n",
      " [ 6  4 21  0  0  0]\n",
      " [ 0  2  0 37 11  0]\n",
      " [ 2  0  0  8 32  0]\n",
      " [ 0  3  0  0  0 18]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.92      0.91       168\n",
      "           2       0.87      0.90      0.89       124\n",
      "           3       0.90      0.91      0.90       141\n",
      "           4       0.87      0.77      0.82       157\n",
      "           5       0.81      0.86      0.84       147\n",
      "           6       0.97      0.96      0.97       114\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.89      0.89       851\n",
      "weighted avg       0.89      0.88      0.88       851\n",
      "\n",
      "Training set score for knn_model: 0.863850\n",
      "Testing  set score for knn_model: 0.755869\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (425,)\n",
      "Shape of testing output Y_test: (425, 1)\n",
      "Best score for training data: 0.7670588235294117 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[71  3  5  0  0  0]\n",
      " [ 9 43  6  0  0  2]\n",
      " [ 6  7 69  0  0  0]\n",
      " [ 0  2  0 44 19  2]\n",
      " [ 0  4  0 11 70  0]\n",
      " [ 0  0  0  0  2 50]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.92      0.91       168\n",
      "           2       0.87      0.90      0.89       124\n",
      "           3       0.90      0.91      0.90       141\n",
      "           4       0.87      0.77      0.82       157\n",
      "           5       0.81      0.86      0.84       147\n",
      "           6       0.97      0.96      0.97       114\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.89      0.89       851\n",
      "weighted avg       0.89      0.88      0.88       851\n",
      "\n",
      "Training set score for knn_model: 0.884706\n",
      "Testing  set score for knn_model: 0.816471\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (638,)\n",
      "Shape of testing output Y_test: (638, 1)\n",
      "Best score for training data: 0.7883858267716535 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[109   4  16   0   0   0]\n",
      " [ 14  61   9   0   0   0]\n",
      " [  8   4  87   0   0   0]\n",
      " [  0   6   2  76  35   3]\n",
      " [  1   3   2  20 100   0]\n",
      " [  0   1   0   3   0  74]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.92      0.91       168\n",
      "           2       0.87      0.90      0.89       124\n",
      "           3       0.90      0.91      0.90       141\n",
      "           4       0.87      0.77      0.82       157\n",
      "           5       0.81      0.86      0.84       147\n",
      "           6       0.97      0.96      0.97       114\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.89      0.89       851\n",
      "weighted avg       0.89      0.88      0.88       851\n",
      "\n",
      "Training set score for knn_model: 0.901254\n",
      "Testing  set score for knn_model: 0.794671\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (850,)\n",
      "Shape of testing output Y_test: (850, 1)\n",
      "Best score for training data: 0.8058823529411765 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[146   3   4   0   0   0]\n",
      " [  9 109   4   3   0   3]\n",
      " [ 19   7 110   0   0   0]\n",
      " [  1   2   1 133  31   1]\n",
      " [  2   2   1  41 115   0]\n",
      " [  1   2   0   4   0  96]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.92      0.91       168\n",
      "           2       0.87      0.90      0.89       124\n",
      "           3       0.90      0.91      0.90       141\n",
      "           4       0.87      0.77      0.82       157\n",
      "           5       0.81      0.86      0.84       147\n",
      "           6       0.97      0.96      0.97       114\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.89      0.89       851\n",
      "weighted avg       0.89      0.88      0.88       851\n",
      "\n",
      "Training set score for knn_model: 0.920000\n",
      "Testing  set score for knn_model: 0.834118\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.8081229515457526 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[176   2  15   0   0   0]\n",
      " [ 13 140   8   0   0   2]\n",
      " [ 23   5 151   0   0   0]\n",
      " [  1   4   0 145  40   2]\n",
      " [  1   2   0  43 142   0]\n",
      " [  0   7   0   3   0 138]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.92      0.91       168\n",
      "           2       0.87      0.90      0.89       124\n",
      "           3       0.90      0.91      0.90       141\n",
      "           4       0.87      0.77      0.82       157\n",
      "           5       0.81      0.86      0.84       147\n",
      "           6       0.97      0.96      0.97       114\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.89      0.89       851\n",
      "weighted avg       0.89      0.88      0.88       851\n",
      "\n",
      "Training set score for knn_model: 0.917215\n",
      "Testing  set score for knn_model: 0.839135\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    model = KNeighborsClassifier()\n",
    "    params_grid = [{'n_neighbors': [2,3,4], 'weights' :['uniform'],'leaf_size':[4,5,6,7,8,9],'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "    knn_model = GridSearchCV(model, params_grid, cv=5)\n",
    "    knn_model.fit(X_train,Y_train)\n",
    "    print('Best score for training data:', knn_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best #neighbors:',knn_model.best_estimator_.n_neighbors,\"\\n\") \n",
    "    print('Best weights:',knn_model.best_estimator_.weights,\"\\n\")\n",
    "    print('Best leaf_size:',knn_model.best_estimator_.leaf_size,\"\\n\")\n",
    "    print('Best algorithm:',knn_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = knn_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for knn_model: %f\" % final_model.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for knn_model: %f\" % final_model.score(X_test  , Y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5541e508",
   "metadata": {},
   "source": [
    "# ** Support Vector Machine **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7c82f4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.7785934549688595 \n",
      "\n",
      "Best C: 2 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[150   8  19   0   0   0]\n",
      " [ 29  94  12   2   0   2]\n",
      " [ 21  11 111   0   1   0]\n",
      " [  0   1   0  79  63   5]\n",
      " [  0   1   0  23 122   0]\n",
      " [  0   3   0   1   0  93]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.85      0.80       177\n",
      "           2       0.80      0.68      0.73       139\n",
      "           3       0.78      0.77      0.78       144\n",
      "           4       0.75      0.53      0.62       148\n",
      "           5       0.66      0.84      0.73       146\n",
      "           6       0.93      0.96      0.94        97\n",
      "\n",
      "    accuracy                           0.76       851\n",
      "   macro avg       0.78      0.77      0.77       851\n",
      "weighted avg       0.77      0.76      0.76       851\n",
      "\n",
      "Training set score for svm_model: 0.794472\n",
      "Testing  set score for svm_model: 0.762632\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = SVC()\n",
    "    params_grid = [{'kernel': ['linear'], 'C': [2]}]\n",
    "    svm_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    svm_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "    print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "    print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "    final_model = svm_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for svm_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for svm_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2ef4eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.6713615023474179 \n",
      "\n",
      "Best C: 100 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[32  6  0  0  0  0]\n",
      " [ 3 15  5  0  0  0]\n",
      " [ 8  5 30  0  0  0]\n",
      " [ 0  0  0 20 18  2]\n",
      " [ 0  0  0  3 35  0]\n",
      " [ 0  3  0  0  0 28]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.84      0.79        38\n",
      "           2       0.52      0.65      0.58        23\n",
      "           3       0.86      0.70      0.77        43\n",
      "           4       0.87      0.50      0.63        40\n",
      "           5       0.66      0.92      0.77        38\n",
      "           6       0.93      0.90      0.92        31\n",
      "\n",
      "    accuracy                           0.75       213\n",
      "   macro avg       0.76      0.75      0.74       213\n",
      "weighted avg       0.78      0.75      0.75       213\n",
      "\n",
      "Training set score for svm_model: 0.868545\n",
      "Testing  set score for svm_model: 0.751174\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.7411846968334831 \n",
      "\n",
      "Best C: 100 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[68  8  7  0  0  0]\n",
      " [ 8 41  8  1  0  0]\n",
      " [ 6  4 54  0  0  0]\n",
      " [ 0  2  1 45 23  1]\n",
      " [ 0  0  0 20 63  0]\n",
      " [ 0  4  0  2  0 59]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.83      0.82      0.82        83\n",
      "           2       0.69      0.71      0.70        58\n",
      "           3       0.77      0.84      0.81        64\n",
      "           4       0.66      0.62      0.64        72\n",
      "           5       0.73      0.76      0.75        83\n",
      "           6       0.98      0.91      0.94        65\n",
      "\n",
      "    accuracy                           0.78       425\n",
      "   macro avg       0.78      0.78      0.78       425\n",
      "weighted avg       0.78      0.78      0.78       425\n",
      "\n",
      "Training set score for svm_model: 0.861176\n",
      "Testing  set score for svm_model: 0.776471\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.7586441078335843 \n",
      "\n",
      "Best C: 100 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[94 14 12  0  0  0]\n",
      " [16 65  7  0  0  2]\n",
      " [11  7 86  0  1  0]\n",
      " [ 0  1  2 74 43  2]\n",
      " [ 2  0  1 19 84  0]\n",
      " [ 0  1  0  1  0 93]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.78      0.77       120\n",
      "           2       0.74      0.72      0.73        90\n",
      "           3       0.80      0.82      0.81       105\n",
      "           4       0.79      0.61      0.69       122\n",
      "           5       0.66      0.79      0.72       106\n",
      "           6       0.96      0.98      0.97        95\n",
      "\n",
      "    accuracy                           0.78       638\n",
      "   macro avg       0.78      0.78      0.78       638\n",
      "weighted avg       0.78      0.78      0.78       638\n",
      "\n",
      "Training set score for svm_model: 0.849530\n",
      "Testing  set score for svm_model: 0.777429\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.750593075532109 \n",
      "\n",
      "Best C: 100 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[136   7   9   0   0   0]\n",
      " [ 22  92   2   1   1   0]\n",
      " [ 26  12 107   0   1   0]\n",
      " [  1   1   1 114  46   1]\n",
      " [  0   0   0  35 119   0]\n",
      " [  0   7   0   5   1 103]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.89      0.81       152\n",
      "           2       0.77      0.78      0.78       118\n",
      "           3       0.90      0.73      0.81       146\n",
      "           4       0.74      0.70      0.71       164\n",
      "           5       0.71      0.77      0.74       154\n",
      "           6       0.99      0.89      0.94       116\n",
      "\n",
      "    accuracy                           0.79       850\n",
      "   macro avg       0.81      0.79      0.80       850\n",
      "weighted avg       0.80      0.79      0.79       850\n",
      "\n",
      "Training set score for svm_model: 0.821176\n",
      "Testing  set score for svm_model: 0.789412\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.7488342484284236 \n",
      "\n",
      "Best C: 100 \n",
      "\n",
      "Best Kernel: linear \n",
      "\n",
      "Best Gamma: scale \n",
      "\n",
      "[[193  11   6   0   0   0]\n",
      " [ 22 108  22   2   0   2]\n",
      " [ 21  22 117   0   2   0]\n",
      " [  1   2   2 119  73   0]\n",
      " [  2   0   2  45 149   1]\n",
      " [  0   5   0   5   0 129]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.92      0.86       210\n",
      "           2       0.73      0.69      0.71       156\n",
      "           3       0.79      0.72      0.75       162\n",
      "           4       0.70      0.60      0.65       197\n",
      "           5       0.67      0.75      0.70       199\n",
      "           6       0.98      0.93      0.95       139\n",
      "\n",
      "    accuracy                           0.77      1063\n",
      "   macro avg       0.78      0.77      0.77      1063\n",
      "weighted avg       0.77      0.77      0.77      1063\n",
      "\n",
      "Training set score for svm_model: 0.811853\n",
      "Testing  set score for svm_model: 0.766698\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = SVC()\n",
    "    params_grid = [{'kernel': ['linear'], 'C': [100,150]}]\n",
    "    svm_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    svm_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "    print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "    print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "    final_model = svm_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for svm_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for svm_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865b2853",
   "metadata": {},
   "source": [
    "# ** Decision Tree **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d1543568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8462191649893915 \n",
      "\n",
      "Best depth: 11 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[153   6   9   0   0   0]\n",
      " [ 10  94   8   0   0   0]\n",
      " [ 10  14 114   1   0   1]\n",
      " [  0   0   0 153  14   1]\n",
      " [  1   0   0   9 145   0]\n",
      " [  0   2   1   0   0 105]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.88      0.91      0.89       168\n",
      "           2       0.81      0.84      0.82       112\n",
      "           3       0.86      0.81      0.84       140\n",
      "           4       0.94      0.91      0.92       168\n",
      "           5       0.91      0.94      0.92       155\n",
      "           6       0.98      0.97      0.98       108\n",
      "\n",
      "    accuracy                           0.90       851\n",
      "   macro avg       0.90      0.90      0.90       851\n",
      "weighted avg       0.90      0.90      0.90       851\n",
      "\n",
      "Training set score for dc_model: 0.982358\n",
      "Testing  set score for dc_model: 0.897767\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = DecisionTreeClassifier()\n",
    "    params_grid = [{'max_depth': [8,9,10,11,12],'random_state':[42]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "37ebe3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "Best score for training data: 0.7230046948356806 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[19  6  3  0  0  0]\n",
      " [ 3 16  5  0  0  1]\n",
      " [ 7 16 24  0  0  0]\n",
      " [ 0  0  0 29  8  1]\n",
      " [ 1  0  0 14 32  0]\n",
      " [ 0  6  0  1  0 21]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.68      0.66        28\n",
      "           2       0.36      0.64      0.46        25\n",
      "           3       0.75      0.51      0.61        47\n",
      "           4       0.66      0.76      0.71        38\n",
      "           5       0.80      0.68      0.74        47\n",
      "           6       0.91      0.75      0.82        28\n",
      "\n",
      "    accuracy                           0.66       213\n",
      "   macro avg       0.69      0.67      0.67       213\n",
      "weighted avg       0.71      0.66      0.67       213\n",
      "\n",
      "Training set score for dc_model: 0.990610\n",
      "Testing  set score for dc_model: 0.661972\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (425,)\n",
      "Shape of testing output Y_test: (425, 1)\n",
      "Best score for training data: 0.729464255985083 \n",
      "\n",
      "Best depth: 10 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[56 15  5  0  0  0]\n",
      " [13 31  6  0  1  4]\n",
      " [16 16 38  0  0  0]\n",
      " [ 1  2  0 62 19  3]\n",
      " [ 0  0  0 17 61  0]\n",
      " [ 0  3  1  1  0 54]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.74      0.69        76\n",
      "           2       0.46      0.56      0.51        55\n",
      "           3       0.76      0.54      0.63        70\n",
      "           4       0.78      0.71      0.74        87\n",
      "           5       0.75      0.78      0.77        78\n",
      "           6       0.89      0.92      0.90        59\n",
      "\n",
      "    accuracy                           0.71       425\n",
      "   macro avg       0.71      0.71      0.71       425\n",
      "weighted avg       0.72      0.71      0.71       425\n",
      "\n",
      "Training set score for dc_model: 0.995294\n",
      "Testing  set score for dc_model: 0.710588\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (638,)\n",
      "Shape of testing output Y_test: (638, 1)\n",
      "Best score for training data: 0.7397983287566067 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[79 17 24  0  0  0]\n",
      " [16 59 13  0  0  3]\n",
      " [ 8  9 85  0  0  1]\n",
      " [ 0  0  1 87 22  0]\n",
      " [ 1  0  0 22 97  0]\n",
      " [ 0  3  0  3  0 88]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.66      0.71       120\n",
      "           2       0.67      0.65      0.66        91\n",
      "           3       0.69      0.83      0.75       103\n",
      "           4       0.78      0.79      0.78       110\n",
      "           5       0.82      0.81      0.81       120\n",
      "           6       0.96      0.94      0.95        94\n",
      "\n",
      "    accuracy                           0.78       638\n",
      "   macro avg       0.78      0.78      0.78       638\n",
      "weighted avg       0.78      0.78      0.78       638\n",
      "\n",
      "Training set score for dc_model: 0.960815\n",
      "Testing  set score for dc_model: 0.775862\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (850,)\n",
      "Shape of testing output Y_test: (850, 1)\n",
      "Best score for training data: 0.7623591964033909 \n",
      "\n",
      "Best depth: 12 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[119  18  15   0   0   0]\n",
      " [ 19  98  17   3   1   0]\n",
      " [ 20  18  75   0   2   0]\n",
      " [  0   2   1 157  14   4]\n",
      " [  1   0   0  25 144   0]\n",
      " [  0   1   0   6   3  87]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.78      0.77       152\n",
      "           2       0.72      0.71      0.71       138\n",
      "           3       0.69      0.65      0.67       115\n",
      "           4       0.82      0.88      0.85       178\n",
      "           5       0.88      0.85      0.86       170\n",
      "           6       0.96      0.90      0.93        97\n",
      "\n",
      "    accuracy                           0.80       850\n",
      "   macro avg       0.80      0.80      0.80       850\n",
      "weighted avg       0.80      0.80      0.80       850\n",
      "\n",
      "Training set score for dc_model: 0.995294\n",
      "Testing  set score for dc_model: 0.800000\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7892814514203867 \n",
      "\n",
      "Best depth: 11 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[139  24  26   1   0   0]\n",
      " [ 19 110  27   4   0   2]\n",
      " [ 16  15 129   1   0   0]\n",
      " [  0   2   1 182  22   0]\n",
      " [  1   0   1  21 166   0]\n",
      " [  0   8   1   4   0 141]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.73      0.76       190\n",
      "           2       0.69      0.68      0.69       162\n",
      "           3       0.70      0.80      0.75       161\n",
      "           4       0.85      0.88      0.87       207\n",
      "           5       0.88      0.88      0.88       189\n",
      "           6       0.99      0.92      0.95       154\n",
      "\n",
      "    accuracy                           0.82      1063\n",
      "   macro avg       0.82      0.81      0.81      1063\n",
      "weighted avg       0.82      0.82      0.82      1063\n",
      "\n",
      "Training set score for dc_model: 0.994356\n",
      "Testing  set score for dc_model: 0.815616\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = DecisionTreeClassifier()\n",
    "    params_grid = [{'max_depth': [8,9,10,11,12],'random_state':[42]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,Y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , Y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0b5b6",
   "metadata": {},
   "source": [
    "<h1> **************************************************************************************************************</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9e22d6",
   "metadata": {},
   "source": [
    " <h1> AdaBoost </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "18f300b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.3\n",
      "Best score for training data: 0.8316532258064516 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[186  12  16   0   0   0]\n",
      " [ 21 148  22   0   0   2]\n",
      " [ 26  19 170   0   0   2]\n",
      " [  1   2   1 207  38   2]\n",
      " [  3   0   0  13 205   1]\n",
      " [  0   0   0   6   1 172]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.87      0.82       214\n",
      "           2       0.82      0.77      0.79       193\n",
      "           3       0.81      0.78      0.80       217\n",
      "           4       0.92      0.82      0.87       251\n",
      "           5       0.84      0.92      0.88       222\n",
      "           6       0.96      0.96      0.96       179\n",
      "\n",
      "    accuracy                           0.85      1276\n",
      "   macro avg       0.86      0.85      0.85      1276\n",
      "weighted avg       0.85      0.85      0.85      1276\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.852665\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.25\n",
      "Best score for training data: 0.8425838820947006 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[141   9  23   1   1   0]\n",
      " [ 17 111  15   3   3   0]\n",
      " [ 12  12 153   0   1   0]\n",
      " [  0   2   1 200  12   5]\n",
      " [  1   0   1  20 186   1]\n",
      " [  0   2   1   4   0 125]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.81      0.82       175\n",
      "           2       0.82      0.74      0.78       149\n",
      "           3       0.79      0.86      0.82       178\n",
      "           4       0.88      0.91      0.89       220\n",
      "           5       0.92      0.89      0.90       209\n",
      "           6       0.95      0.95      0.95       132\n",
      "\n",
      "    accuracy                           0.86      1063\n",
      "   macro avg       0.86      0.86      0.86      1063\n",
      "weighted avg       0.86      0.86      0.86      1063\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.861712\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8462243537755944 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[126   8  13   0   1   0]\n",
      " [ 21  97   9   0   0   5]\n",
      " [ 20  14 100   1   1   0]\n",
      " [  0   2   0 161   9   3]\n",
      " [  1   0   0  13 143   0]\n",
      " [  0   2   0   1   0 100]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.85      0.80       148\n",
      "           2       0.79      0.73      0.76       132\n",
      "           3       0.82      0.74      0.78       136\n",
      "           4       0.91      0.92      0.92       175\n",
      "           5       0.93      0.91      0.92       157\n",
      "           6       0.93      0.97      0.95       103\n",
      "\n",
      "    accuracy                           0.85       851\n",
      "   macro avg       0.85      0.85      0.85       851\n",
      "weighted avg       0.86      0.85      0.85       851\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.854289\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.15\n",
      "Best score for training data: 0.8563915578776141 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[ 95   6  17   0   1   0]\n",
      " [ 14  72  12   0   0   1]\n",
      " [ 18   9  84   0   1   0]\n",
      " [  0   0   0 123   4   3]\n",
      " [  0   0   0   9  94   0]\n",
      " [  0   0   0   1   2  72]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.75      0.80      0.77       119\n",
      "           2       0.83      0.73      0.77        99\n",
      "           3       0.74      0.75      0.75       112\n",
      "           4       0.92      0.95      0.94       130\n",
      "           5       0.92      0.91      0.92       103\n",
      "           6       0.95      0.96      0.95        75\n",
      "\n",
      "    accuracy                           0.85       638\n",
      "   macro avg       0.85      0.85      0.85       638\n",
      "weighted avg       0.85      0.85      0.85       638\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.846395\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.1\n",
      "Best score for training data: 0.8507582928678263 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[66  3  6  0  0  0]\n",
      " [ 4 42  7  0  0  0]\n",
      " [10  3 64  0  0  0]\n",
      " [ 0  0  0 88  3  0]\n",
      " [ 0  0  2  4 64  0]\n",
      " [ 0  1  0  0  0 59]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.82      0.88      0.85        75\n",
      "           2       0.86      0.79      0.82        53\n",
      "           3       0.81      0.83      0.82        77\n",
      "           4       0.96      0.97      0.96        91\n",
      "           5       0.96      0.91      0.93        70\n",
      "           6       1.00      0.98      0.99        60\n",
      "\n",
      "    accuracy                           0.90       426\n",
      "   macro avg       0.90      0.89      0.90       426\n",
      "weighted avg       0.90      0.90      0.90       426\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.899061\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.05\n",
      "Best score for training data: 0.8606052449759946 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 0.01 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[26  1  4  0  1  0]\n",
      " [ 3 33  2  0  0  1]\n",
      " [ 2  3 28  0  0  0]\n",
      " [ 0  0  0 38  1  1]\n",
      " [ 0  0  0  6 37  0]\n",
      " [ 0  0  0  0  1 25]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.81      0.83        32\n",
      "           2       0.89      0.85      0.87        39\n",
      "           3       0.82      0.85      0.84        33\n",
      "           4       0.86      0.95      0.90        40\n",
      "           5       0.93      0.86      0.89        43\n",
      "           6       0.93      0.96      0.94        26\n",
      "\n",
      "    accuracy                           0.88       213\n",
      "   macro avg       0.88      0.88      0.88       213\n",
      "weighted avg       0.88      0.88      0.88       213\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.877934\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.3,0.25,0.2,0.15,0.1,0.05]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = DecisionTreeClassifier(max_features=None)\n",
    "    params_grid = [{'n_estimators':[2,4],'learning_rate':[1.0,1e-1,1e-2],'algorithm':['SAMME.R','SAMME']}]\n",
    "    model_ada=AdaBoostClassifier(model,)\n",
    "    dc_model = GridSearchCV(model_ada, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best estimator:',dc_model.best_estimator_.n_estimators,\"\\n\") \n",
    "    print('Best #learning rate:',dc_model.best_estimator_.learning_rate,\"\\n\")\n",
    "    print('Best algorithm:',dc_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d4501268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7417840375586855 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[27  9  8  0  0  0]\n",
      " [ 2 23  8  1  0  1]\n",
      " [ 5  9 20  0  0  0]\n",
      " [ 0  0  0 32  8  1]\n",
      " [ 0  1  0  6 30  0]\n",
      " [ 0  3  0  0  0 19]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.61      0.69        44\n",
      "           2       0.51      0.66      0.57        35\n",
      "           3       0.56      0.59      0.57        34\n",
      "           4       0.82      0.78      0.80        41\n",
      "           5       0.79      0.81      0.80        37\n",
      "           6       0.90      0.86      0.88        22\n",
      "\n",
      "    accuracy                           0.71       213\n",
      "   macro avg       0.73      0.72      0.72       213\n",
      "weighted avg       0.73      0.71      0.71       213\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.708920\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7388872240535411 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[51 11 12  0  0  0]\n",
      " [10 39  7  0  0  3]\n",
      " [17 16 45  0  0  1]\n",
      " [ 0  4  0 60 19  2]\n",
      " [ 0  0  1 21 47  0]\n",
      " [ 0  3  2  1  0 53]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.69      0.67        74\n",
      "           2       0.53      0.66      0.59        59\n",
      "           3       0.67      0.57      0.62        79\n",
      "           4       0.73      0.71      0.72        85\n",
      "           5       0.71      0.68      0.70        69\n",
      "           6       0.90      0.90      0.90        59\n",
      "\n",
      "    accuracy                           0.69       425\n",
      "   macro avg       0.70      0.70      0.70       425\n",
      "weighted avg       0.70      0.69      0.69       425\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.694118\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7664835976023858 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.01 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[ 71  11  21   0   0   0]\n",
      " [ 13  60  15   2   0   3]\n",
      " [ 10  21  64   0   0   0]\n",
      " [  0   1   0 100  25   2]\n",
      " [  0   2   1  19  97   0]\n",
      " [  0   4   0   2   0  94]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.76      0.69      0.72       103\n",
      "           2       0.61      0.65      0.62        93\n",
      "           3       0.63      0.67      0.65        95\n",
      "           4       0.81      0.78      0.80       128\n",
      "           5       0.80      0.82      0.80       119\n",
      "           6       0.95      0.94      0.94       100\n",
      "\n",
      "    accuracy                           0.76       638\n",
      "   macro avg       0.76      0.76      0.76       638\n",
      "weighted avg       0.76      0.76      0.76       638\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.761755\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7623674911660778 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[103  14  14   0   1   0]\n",
      " [ 12 100  17   2   0   0]\n",
      " [ 29  16  95   0   1   0]\n",
      " [  0   3   3 139  17   2]\n",
      " [  0   0   1  25 138   0]\n",
      " [  0   0   1  12   3 102]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.78      0.75       132\n",
      "           2       0.75      0.76      0.76       131\n",
      "           3       0.73      0.67      0.70       141\n",
      "           4       0.78      0.85      0.81       164\n",
      "           5       0.86      0.84      0.85       164\n",
      "           6       0.98      0.86      0.92       118\n",
      "\n",
      "    accuracy                           0.80       850\n",
      "   macro avg       0.80      0.80      0.80       850\n",
      "weighted avg       0.80      0.80      0.80       850\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.796471\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7892628842736267 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[125  26  36   1   0   0]\n",
      " [ 11 109  32   3   0   5]\n",
      " [ 19  14 133   0   1   1]\n",
      " [  0   2   0 176  21   1]\n",
      " [  1   0   0  26 189   0]\n",
      " [  0   5   0   6   1 119]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.66      0.73       188\n",
      "           2       0.70      0.68      0.69       160\n",
      "           3       0.66      0.79      0.72       168\n",
      "           4       0.83      0.88      0.85       200\n",
      "           5       0.89      0.88      0.88       216\n",
      "           6       0.94      0.91      0.93       131\n",
      "\n",
      "    accuracy                           0.80      1063\n",
      "   macro avg       0.80      0.80      0.80      1063\n",
      "weighted avg       0.81      0.80      0.80      1063\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.800564\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = DecisionTreeClassifier(max_features=None)\n",
    "    params_grid = [{'n_estimators':[2,4],'learning_rate':[1.0,1e-1,1e-2],'algorithm':['SAMME.R','SAMME']}]\n",
    "    model_ada=AdaBoostClassifier(model,)\n",
    "    dc_model = GridSearchCV(model_ada, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best estimator:',dc_model.best_estimator_.n_estimators,\"\\n\") \n",
    "    print('Best #learning rate:',dc_model.best_estimator_.learning_rate,\"\\n\")\n",
    "    print('Best algorithm:',dc_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f013bf29",
   "metadata": {},
   "source": [
    "# ** Ensemble Learning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8c91abd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "[[134  11  14   0   0   0]\n",
      " [ 15  81  14   1   1   3]\n",
      " [ 18  12  92   0   0   0]\n",
      " [  0   3   0 121  56   2]\n",
      " [  3   1   2  24 140   0]\n",
      " [  0   2   0   0   0 101]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.84      0.81       159\n",
      "           2       0.74      0.70      0.72       115\n",
      "           3       0.75      0.75      0.75       122\n",
      "           4       0.83      0.66      0.74       182\n",
      "           5       0.71      0.82      0.76       170\n",
      "           6       0.95      0.98      0.97       103\n",
      "\n",
      "    accuracy                           0.79       851\n",
      "   macro avg       0.80      0.80      0.79       851\n",
      "weighted avg       0.79      0.79      0.78       851\n",
      "\n",
      "Training set score for EL: 0.790650\n",
      "Testing  set score for EL: 0.786134\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data_ratio=[0.20]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    estimator = []\n",
    "    # make different combination in the ensemble classfier\n",
    "    estimator.append(('lr',LogisticRegression(solver='saga',penalty='l1',C=1,n_jobs=2)))\n",
    "    estimator.append(('SVC', SVC(gamma ='scale',probability=True,C=100,kernel='linear',)))\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=14,criterion='gini', splitter='best', min_samples_split=17,random_state=42)\n",
    "    model = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=250 ,learning_rate=1,algorithm='SAMME',random_state=7)\n",
    "    estimator.append(('DTC', model))\n",
    "    #estimator.append(('svc_rbf',SVC(gamma ='auto',C=4,kernel='rbf',)))\n",
    "\n",
    "\n",
    "    vot_hard = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "    vot_hard.fit(X_train,y_train)\n",
    "    Y_pred = vot_hard.predict(X_test)\n",
    "    \n",
    "    print(confusion_matrix(y_test,Y_pred))\n",
    "    print(classification_report(y_test,Y_pred))\n",
    "    print(\"Training set score for EL: %f\" % vot_hard.score(X_train , y_train))\n",
    "    print(\"Testing  set score for EL: %f\" % vot_hard.score(X_test  , y_test ))\n",
    "    \n",
    "\n",
    "    score = accuracy_score(y_test, Y_pred)\n",
    "    print(\"Hard Voting Score % d\" % score)\n",
    "    \n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a56fad3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (213,)\n",
      "Shape of testing output Y_test: (213, 1)\n",
      "[[28  8  1  0  0  0]\n",
      " [ 2 21  2  0  0  1]\n",
      " [ 5 10 26  0  0  0]\n",
      " [ 0  1  0 31 15  1]\n",
      " [ 0  0  0  4 27  0]\n",
      " [ 0  1  0  0  0 29]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.76      0.78        37\n",
      "           2       0.51      0.81      0.63        26\n",
      "           3       0.90      0.63      0.74        41\n",
      "           4       0.89      0.65      0.75        48\n",
      "           5       0.64      0.87      0.74        31\n",
      "           6       0.94      0.97      0.95        30\n",
      "\n",
      "    accuracy                           0.76       213\n",
      "   macro avg       0.78      0.78      0.76       213\n",
      "weighted avg       0.80      0.76      0.76       213\n",
      "\n",
      "Training set score for EL: 0.779343\n",
      "Testing  set score for EL: 0.760563\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (425,)\n",
      "Shape of testing output Y_test: (425, 1)\n",
      "[[63  6  7  0  0  0]\n",
      " [11 44 10  1  0  2]\n",
      " [10  6 54  0  0  0]\n",
      " [ 0  1  0 51 29  4]\n",
      " [ 1  0  0 14 63  0]\n",
      " [ 0  0  0  0  0 48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.83      0.78        76\n",
      "           2       0.77      0.65      0.70        68\n",
      "           3       0.76      0.77      0.77        70\n",
      "           4       0.77      0.60      0.68        85\n",
      "           5       0.68      0.81      0.74        78\n",
      "           6       0.89      1.00      0.94        48\n",
      "\n",
      "    accuracy                           0.76       425\n",
      "   macro avg       0.77      0.78      0.77       425\n",
      "weighted avg       0.76      0.76      0.76       425\n",
      "\n",
      "Training set score for EL: 0.825882\n",
      "Testing  set score for EL: 0.760000\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (638,)\n",
      "Shape of testing output Y_test: (638, 1)\n",
      "[[ 90  11  13   0   0   0]\n",
      " [ 12  72   6   0   1   3]\n",
      " [ 11   9  72   1   0   0]\n",
      " [  1   2   0  76  47   3]\n",
      " [  0   1   0  23 106   0]\n",
      " [  0   0   0   1   0  77]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.79      0.79       114\n",
      "           2       0.76      0.77      0.76        94\n",
      "           3       0.79      0.77      0.78        93\n",
      "           4       0.75      0.59      0.66       129\n",
      "           5       0.69      0.82      0.75       130\n",
      "           6       0.93      0.99      0.96        78\n",
      "\n",
      "    accuracy                           0.77       638\n",
      "   macro avg       0.78      0.79      0.78       638\n",
      "weighted avg       0.77      0.77      0.77       638\n",
      "\n",
      "Training set score for EL: 0.838558\n",
      "Testing  set score for EL: 0.772727\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (850,)\n",
      "Shape of testing output Y_test: (850, 1)\n",
      "[[138  10   6   0   0   1]\n",
      " [ 18  80  15   3   0   2]\n",
      " [ 23   9 106   1   0   0]\n",
      " [  0   2   1 113  47   2]\n",
      " [  1   0   0  38 112   1]\n",
      " [  0   5   0   4   0 112]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.89      0.82       155\n",
      "           2       0.75      0.68      0.71       118\n",
      "           3       0.83      0.76      0.79       139\n",
      "           4       0.71      0.68      0.70       165\n",
      "           5       0.70      0.74      0.72       152\n",
      "           6       0.95      0.93      0.94       121\n",
      "\n",
      "    accuracy                           0.78       850\n",
      "   macro avg       0.79      0.78      0.78       850\n",
      "weighted avg       0.78      0.78      0.78       850\n",
      "\n",
      "Training set score for EL: 0.802353\n",
      "Testing  set score for EL: 0.777647\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "[[187   5  18   0   0   0]\n",
      " [ 29  89  18   2   0   7]\n",
      " [ 21  10 124   1   0   0]\n",
      " [  0   2   1 123  71   7]\n",
      " [  0   1   0  63 137   0]\n",
      " [  0   3   0   4   0 140]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.89      0.84       210\n",
      "           2       0.81      0.61      0.70       145\n",
      "           3       0.77      0.79      0.78       156\n",
      "           4       0.64      0.60      0.62       204\n",
      "           5       0.66      0.68      0.67       201\n",
      "           6       0.91      0.95      0.93       147\n",
      "\n",
      "    accuracy                           0.75      1063\n",
      "   macro avg       0.76      0.76      0.76      1063\n",
      "weighted avg       0.75      0.75      0.75      1063\n",
      "\n",
      "Training set score for EL: 0.787394\n",
      "Testing  set score for EL: 0.752587\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    estimator = []\n",
    "    # make different combination in the ensemble classfier\n",
    "    estimator.append(('lr',LogisticRegression(solver='saga',penalty='l1',C=1,n_jobs=2)))\n",
    "    estimator.append(('SVC', SVC(gamma ='scale',probability=True,C=100,kernel='linear',)))\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=14,criterion='gini', splitter='best', min_samples_split=17,random_state=42)\n",
    "    model = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=250 ,learning_rate=1,algorithm='SAMME',random_state=7)\n",
    "    estimator.append(('DTC', model))\n",
    "    #estimator.append(('svc_rbf',SVC(gamma ='auto',C=4,kernel='rbf',)))\n",
    "\n",
    "\n",
    "    vot_hard = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "    vot_hard.fit(X_train,Y_train)\n",
    "    Y_pred = vot_hard.predict(X_test)\n",
    "    \n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "    print(\"Training set score for EL: %f\" % vot_hard.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for EL: %f\" % vot_hard.score(X_test  , Y_test ))\n",
    "    \n",
    "\n",
    "    score = accuracy_score(Y_test, Y_pred)\n",
    "    print(\"Hard Voting Score % d\" % score)\n",
    "    \n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0b5b6",
   "metadata": {},
   "source": [
    "# ** Random Forest **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0c9587e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8691587887920141 \n",
      "\n",
      "Best depth: 7 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 10 \n",
      "\n",
      "[[139   7   9   0   0   0]\n",
      " [ 18 115   6   1   0   0]\n",
      " [ 15   7 129   0   0   0]\n",
      " [  0   2   0 132  20   0]\n",
      " [  1   0   0  18 136   0]\n",
      " [  0   0   0   0   0  96]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.90      0.85       155\n",
      "           2       0.88      0.82      0.85       140\n",
      "           3       0.90      0.85      0.87       151\n",
      "           4       0.87      0.86      0.87       154\n",
      "           5       0.87      0.88      0.87       155\n",
      "           6       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           0.88       851\n",
      "   macro avg       0.89      0.88      0.89       851\n",
      "weighted avg       0.88      0.88      0.88       851\n",
      "\n",
      "Training set score for dc_model: 0.935901\n",
      "Testing  set score for dc_model: 0.877791\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = RandomForestClassifier()\n",
    "    params_grid = [{'max_depth': [5,6,7],'random_state':[42],'n_jobs':[10,15,20]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #estimators:',dc_model.best_estimator_.n_estimators,\"\\n\")\n",
    "    print('Best jobs:',dc_model.best_estimator_.n_jobs,\"\\n\") \n",
    "    #print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "eb66d8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (213, 20)\n",
      "Shape of testing input X_test: (213, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.7887323943661971 \n",
      "\n",
      "Best depth: 6 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[31  5  1  0  0  0]\n",
      " [ 9 19  4  0  0  0]\n",
      " [13  7 21  1  0  0]\n",
      " [ 0  2  0 32  7  1]\n",
      " [ 0  1  0  3 26  0]\n",
      " [ 0  7  0  1  0 22]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.84      0.69        37\n",
      "           2       0.46      0.59      0.52        32\n",
      "           3       0.81      0.50      0.62        42\n",
      "           4       0.86      0.76      0.81        42\n",
      "           5       0.79      0.87      0.83        30\n",
      "           6       0.96      0.73      0.83        30\n",
      "\n",
      "    accuracy                           0.71       213\n",
      "   macro avg       0.74      0.72      0.72       213\n",
      "weighted avg       0.75      0.71      0.71       213\n",
      "\n",
      "Training set score for dc_model: 0.995305\n",
      "Testing  set score for dc_model: 0.708920\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (425, 20)\n",
      "Shape of testing input X_test: (425, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.8024006925715046 \n",
      "\n",
      "Best depth: 7 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[56  2  5  0  0  0]\n",
      " [ 5 48  3  0  0  5]\n",
      " [11  7 55  0  0  0]\n",
      " [ 0  1  1 59 26  0]\n",
      " [ 1  0  0  6 72  0]\n",
      " [ 0  2  0  0  0 60]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.89      0.82        63\n",
      "           2       0.80      0.79      0.79        61\n",
      "           3       0.86      0.75      0.80        73\n",
      "           4       0.91      0.68      0.78        87\n",
      "           5       0.73      0.91      0.81        79\n",
      "           6       0.92      0.97      0.94        62\n",
      "\n",
      "    accuracy                           0.82       425\n",
      "   macro avg       0.83      0.83      0.83       425\n",
      "weighted avg       0.83      0.82      0.82       425\n",
      "\n",
      "Training set score for dc_model: 0.983529\n",
      "Testing  set score for dc_model: 0.823529\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (638, 20)\n",
      "Shape of testing input X_test: (638, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.8197729353057549 \n",
      "\n",
      "Best depth: 6 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[ 94  11   9   0   0   0]\n",
      " [ 13  66   9   0   0   2]\n",
      " [ 10  10  85   0   0   0]\n",
      " [  0   1   0 104  16   3]\n",
      " [  2   0   0  18 102   0]\n",
      " [  0   3   0   0   0  80]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.79      0.82      0.81       114\n",
      "           2       0.73      0.73      0.73        90\n",
      "           3       0.83      0.81      0.82       105\n",
      "           4       0.85      0.84      0.85       124\n",
      "           5       0.86      0.84      0.85       122\n",
      "           6       0.94      0.96      0.95        83\n",
      "\n",
      "    accuracy                           0.83       638\n",
      "   macro avg       0.83      0.83      0.83       638\n",
      "weighted avg       0.83      0.83      0.83       638\n",
      "\n",
      "Training set score for dc_model: 0.949843\n",
      "Testing  set score for dc_model: 0.832288\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (850, 20)\n",
      "Shape of testing input X_test: (850, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.8258721942965211 \n",
      "\n",
      "Best depth: 7 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[137   8   8   0   0   0]\n",
      " [ 14 103   9   2   0   1]\n",
      " [ 20   4 111   0   0   0]\n",
      " [  0   1   0 126  15   1]\n",
      " [  1   1   0  25 137   1]\n",
      " [  0   6   1   5   0 113]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.90      0.84       153\n",
      "           2       0.84      0.80      0.82       129\n",
      "           3       0.86      0.82      0.84       135\n",
      "           4       0.80      0.88      0.84       143\n",
      "           5       0.90      0.83      0.86       165\n",
      "           6       0.97      0.90      0.94       125\n",
      "\n",
      "    accuracy                           0.86       850\n",
      "   macro avg       0.86      0.86      0.86       850\n",
      "weighted avg       0.86      0.86      0.86       850\n",
      "\n",
      "Training set score for dc_model: 0.972941\n",
      "Testing  set score for dc_model: 0.855294\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (1063, 20)\n",
      "Shape of testing input X_test: (1063, 20)\n",
      "Shape of training output Y_train: (1063,)\n",
      "Shape of testing output Y_test: (1063, 1)\n",
      "Best score for training data: 0.8344340998912495 \n",
      "\n",
      "Best depth: 7 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[186   3   7   0   0   0]\n",
      " [ 24 137  13   0   0   3]\n",
      " [ 21  10 140   1   0   0]\n",
      " [  0   0   0 172  14   1]\n",
      " [  2   0   0  27 150   0]\n",
      " [  0   5   0   3   0 144]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.80      0.95      0.87       196\n",
      "           2       0.88      0.77      0.83       177\n",
      "           3       0.88      0.81      0.84       172\n",
      "           4       0.85      0.92      0.88       187\n",
      "           5       0.91      0.84      0.87       179\n",
      "           6       0.97      0.95      0.96       152\n",
      "\n",
      "    accuracy                           0.87      1063\n",
      "   macro avg       0.88      0.87      0.88      1063\n",
      "weighted avg       0.88      0.87      0.87      1063\n",
      "\n",
      "Training set score for dc_model: 0.954845\n",
      "Testing  set score for dc_model: 0.873942\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    params_grid = [{'max_depth': [5,6,7],'random_state':[42],'n_jobs':[2]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #estimators:',dc_model.best_estimator_.n_estimators,\"\\n\")\n",
    "    print('Best jobs:',dc_model.best_estimator_.n_jobs,\"\\n\") \n",
    "    #print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2fe81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "414dcb54",
   "metadata": {},
   "source": [
    "# ...........THANK YOU.........HAPPY CODING......."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
