{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "476d9e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "from sklearn.model_selection import train_test_split,RepeatedStratifiedKFold,GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8ae230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data=pd.read_csv(r\"Data\\pd_speech_features.csv\",)\n",
    "    data.columns = data.iloc[0]\n",
    "    data= data.iloc[1: , :]\n",
    "    data=data.drop('id',axis=1)\n",
    "    data=data.sample(frac=1)\n",
    "    labels=data['class']\n",
    "    data=data.drop('class',axis=1)\n",
    "    \n",
    "    return data,labels\n",
    "\n",
    "data,labels=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91151582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0</td>\n",
       "      <td>0.78991</td>\n",
       "      <td>0.83561</td>\n",
       "      <td>0.3334</td>\n",
       "      <td>372</td>\n",
       "      <td>371</td>\n",
       "      <td>0.005186792</td>\n",
       "      <td>4.90E-05</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>5.86E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>112.2577</td>\n",
       "      <td>61.1446</td>\n",
       "      <td>20.2593</td>\n",
       "      <td>6.4921</td>\n",
       "      <td>6.5002</td>\n",
       "      <td>9.5002</td>\n",
       "      <td>9.8331</td>\n",
       "      <td>8.0799</td>\n",
       "      <td>6.9204</td>\n",
       "      <td>2.6948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>1</td>\n",
       "      <td>0.49231</td>\n",
       "      <td>0.75583</td>\n",
       "      <td>0.8498</td>\n",
       "      <td>112</td>\n",
       "      <td>107</td>\n",
       "      <td>0.009241015</td>\n",
       "      <td>0.000589142</td>\n",
       "      <td>0.02775</td>\n",
       "      <td>0.00025648</td>\n",
       "      <td>...</td>\n",
       "      <td>11.8991</td>\n",
       "      <td>1.9327</td>\n",
       "      <td>2.1889</td>\n",
       "      <td>29.3002</td>\n",
       "      <td>28.1956</td>\n",
       "      <td>27.2019</td>\n",
       "      <td>43.3029</td>\n",
       "      <td>53.5871</td>\n",
       "      <td>40.2744</td>\n",
       "      <td>93.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0.79196</td>\n",
       "      <td>0.66429</td>\n",
       "      <td>0.69379</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>0.009104241</td>\n",
       "      <td>0.000111466</td>\n",
       "      <td>0.00354</td>\n",
       "      <td>3.22E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2994</td>\n",
       "      <td>2.151</td>\n",
       "      <td>2.119</td>\n",
       "      <td>4.3379</td>\n",
       "      <td>5.9496</td>\n",
       "      <td>4.1898</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>2.9994</td>\n",
       "      <td>2.9446</td>\n",
       "      <td>3.8043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>1</td>\n",
       "      <td>0.24164</td>\n",
       "      <td>0.78342</td>\n",
       "      <td>0.63352</td>\n",
       "      <td>239</td>\n",
       "      <td>238</td>\n",
       "      <td>0.00809496</td>\n",
       "      <td>0.000158005</td>\n",
       "      <td>0.00214</td>\n",
       "      <td>1.73E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5769</td>\n",
       "      <td>1.6122</td>\n",
       "      <td>14.7791</td>\n",
       "      <td>20.4438</td>\n",
       "      <td>5.5828</td>\n",
       "      <td>4.4942</td>\n",
       "      <td>6.0828</td>\n",
       "      <td>14.0307</td>\n",
       "      <td>10.646</td>\n",
       "      <td>10.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>1</td>\n",
       "      <td>0.86364</td>\n",
       "      <td>0.83083</td>\n",
       "      <td>0.55985</td>\n",
       "      <td>250</td>\n",
       "      <td>249</td>\n",
       "      <td>0.007711596</td>\n",
       "      <td>9.04E-05</td>\n",
       "      <td>0.00369</td>\n",
       "      <td>2.85E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5707</td>\n",
       "      <td>1.744</td>\n",
       "      <td>95.1695</td>\n",
       "      <td>25.3594</td>\n",
       "      <td>4.0593</td>\n",
       "      <td>3.2636</td>\n",
       "      <td>3.913</td>\n",
       "      <td>3.4838</td>\n",
       "      <td>3.0588</td>\n",
       "      <td>3.2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80665</td>\n",
       "      <td>0.61493</td>\n",
       "      <td>0.6002</td>\n",
       "      <td>242</td>\n",
       "      <td>241</td>\n",
       "      <td>0.007971983</td>\n",
       "      <td>6.86E-05</td>\n",
       "      <td>0.00162</td>\n",
       "      <td>1.29E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8661</td>\n",
       "      <td>1.8574</td>\n",
       "      <td>3.7109</td>\n",
       "      <td>2.9855</td>\n",
       "      <td>2.6307</td>\n",
       "      <td>2.6897</td>\n",
       "      <td>3.8939</td>\n",
       "      <td>6.5859</td>\n",
       "      <td>8.7967</td>\n",
       "      <td>14.1727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0</td>\n",
       "      <td>0.79225</td>\n",
       "      <td>0.72175</td>\n",
       "      <td>0.45346</td>\n",
       "      <td>346</td>\n",
       "      <td>345</td>\n",
       "      <td>0.00557753</td>\n",
       "      <td>5.28E-05</td>\n",
       "      <td>0.00098</td>\n",
       "      <td>5.44E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2307</td>\n",
       "      <td>5.3425</td>\n",
       "      <td>6.3238</td>\n",
       "      <td>7.2737</td>\n",
       "      <td>4.2224</td>\n",
       "      <td>3.1236</td>\n",
       "      <td>2.7313</td>\n",
       "      <td>18.7789</td>\n",
       "      <td>22.4631</td>\n",
       "      <td>83.3482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80765</td>\n",
       "      <td>0.7747</td>\n",
       "      <td>0.48003</td>\n",
       "      <td>272</td>\n",
       "      <td>271</td>\n",
       "      <td>0.007086226</td>\n",
       "      <td>7.20E-05</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>1.07E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5455</td>\n",
       "      <td>2.5053</td>\n",
       "      <td>2.8389</td>\n",
       "      <td>3.3534</td>\n",
       "      <td>3.4437</td>\n",
       "      <td>4.561</td>\n",
       "      <td>5.0762</td>\n",
       "      <td>3.484</td>\n",
       "      <td>3.4902</td>\n",
       "      <td>4.2894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>1</td>\n",
       "      <td>0.83405</td>\n",
       "      <td>0.76165</td>\n",
       "      <td>0.44345</td>\n",
       "      <td>285</td>\n",
       "      <td>284</td>\n",
       "      <td>0.006772009</td>\n",
       "      <td>5.71E-05</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>8.34E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7055</td>\n",
       "      <td>15.2594</td>\n",
       "      <td>36.4588</td>\n",
       "      <td>13.5266</td>\n",
       "      <td>4.8854</td>\n",
       "      <td>3.7817</td>\n",
       "      <td>5.6088</td>\n",
       "      <td>34.6387</td>\n",
       "      <td>34.4628</td>\n",
       "      <td>81.7748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "      <td>0.82028</td>\n",
       "      <td>0.71591</td>\n",
       "      <td>0.54311</td>\n",
       "      <td>310</td>\n",
       "      <td>309</td>\n",
       "      <td>0.006210673</td>\n",
       "      <td>7.18E-05</td>\n",
       "      <td>0.00164</td>\n",
       "      <td>1.02E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.105</td>\n",
       "      <td>33.3424</td>\n",
       "      <td>12.1173</td>\n",
       "      <td>7.3881</td>\n",
       "      <td>4.7971</td>\n",
       "      <td>4.4347</td>\n",
       "      <td>6.7973</td>\n",
       "      <td>26.7017</td>\n",
       "      <td>24.6198</td>\n",
       "      <td>61.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0   gender      PPE      DFA     RPDE numPulses numPeriodsPulses  \\\n",
       "411      0  0.78991  0.83561   0.3334       372              371   \n",
       "426      1  0.49231  0.75583   0.8498       112              107   \n",
       "31       1  0.79196  0.66429  0.69379       212              211   \n",
       "538      1  0.24164  0.78342  0.63352       239              238   \n",
       "468      1  0.86364  0.83083  0.55985       250              249   \n",
       "..     ...      ...      ...      ...       ...              ...   \n",
       "171      1  0.80665  0.61493   0.6002       242              241   \n",
       "515      0  0.79225  0.72175  0.45346       346              345   \n",
       "622      1  0.80765   0.7747  0.48003       272              271   \n",
       "687      1  0.83405  0.76165  0.44345       285              284   \n",
       "187      0  0.82028  0.71591  0.54311       310              309   \n",
       "\n",
       "0   meanPeriodPulses stdDevPeriodPulses locPctJitter locAbsJitter  ...  \\\n",
       "411      0.005186792           4.90E-05      0.00113     5.86E-06  ...   \n",
       "426      0.009241015        0.000589142      0.02775   0.00025648  ...   \n",
       "31       0.009104241        0.000111466      0.00354     3.22E-05  ...   \n",
       "538       0.00809496        0.000158005      0.00214     1.73E-05  ...   \n",
       "468      0.007711596           9.04E-05      0.00369     2.85E-05  ...   \n",
       "..               ...                ...          ...          ...  ...   \n",
       "171      0.007971983           6.86E-05      0.00162     1.29E-05  ...   \n",
       "515       0.00557753           5.28E-05      0.00098     5.44E-06  ...   \n",
       "622      0.007086226           7.20E-05      0.00151     1.07E-05  ...   \n",
       "687      0.006772009           5.71E-05      0.00123     8.34E-06  ...   \n",
       "187      0.006210673           7.18E-05      0.00164     1.02E-05  ...   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_27 tqwt_kurtosisValue_dec_28  \\\n",
       "411                  112.2577                   61.1446   \n",
       "426                   11.8991                    1.9327   \n",
       "31                     2.2994                     2.151   \n",
       "538                    1.5769                    1.6122   \n",
       "468                    1.5707                     1.744   \n",
       "..                        ...                       ...   \n",
       "171                    1.8661                    1.8574   \n",
       "515                    6.2307                    5.3425   \n",
       "622                    1.5455                    2.5053   \n",
       "687                    1.7055                   15.2594   \n",
       "187                     2.105                   33.3424   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_29 tqwt_kurtosisValue_dec_30  \\\n",
       "411                   20.2593                    6.4921   \n",
       "426                    2.1889                   29.3002   \n",
       "31                      2.119                    4.3379   \n",
       "538                   14.7791                   20.4438   \n",
       "468                   95.1695                   25.3594   \n",
       "..                        ...                       ...   \n",
       "171                    3.7109                    2.9855   \n",
       "515                    6.3238                    7.2737   \n",
       "622                    2.8389                    3.3534   \n",
       "687                   36.4588                   13.5266   \n",
       "187                   12.1173                    7.3881   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_31 tqwt_kurtosisValue_dec_32  \\\n",
       "411                    6.5002                    9.5002   \n",
       "426                   28.1956                   27.2019   \n",
       "31                     5.9496                    4.1898   \n",
       "538                    5.5828                    4.4942   \n",
       "468                    4.0593                    3.2636   \n",
       "..                        ...                       ...   \n",
       "171                    2.6307                    2.6897   \n",
       "515                    4.2224                    3.1236   \n",
       "622                    3.4437                     4.561   \n",
       "687                    4.8854                    3.7817   \n",
       "187                    4.7971                    4.4347   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_33 tqwt_kurtosisValue_dec_34  \\\n",
       "411                    9.8331                    8.0799   \n",
       "426                   43.3029                   53.5871   \n",
       "31                     3.1736                    2.9994   \n",
       "538                    6.0828                   14.0307   \n",
       "468                     3.913                    3.4838   \n",
       "..                        ...                       ...   \n",
       "171                    3.8939                    6.5859   \n",
       "515                    2.7313                   18.7789   \n",
       "622                    5.0762                     3.484   \n",
       "687                    5.6088                   34.6387   \n",
       "187                    6.7973                   26.7017   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_35 tqwt_kurtosisValue_dec_36  \n",
       "411                    6.9204                    2.6948  \n",
       "426                   40.2744                    93.142  \n",
       "31                     2.9446                    3.8043  \n",
       "538                    10.646                   10.9042  \n",
       "468                    3.0588                    3.2383  \n",
       "..                        ...                       ...  \n",
       "171                    8.7967                   14.1727  \n",
       "515                   22.4631                   83.3482  \n",
       "622                    3.4902                    4.2894  \n",
       "687                   34.4628                   81.7748  \n",
       "187                   24.6198                     61.04  \n",
       "\n",
       "[756 rows x 753 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc3bf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>...</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>740</td>\n",
       "      <td>745</td>\n",
       "      <td>748</td>\n",
       "      <td>315</td>\n",
       "      <td>319</td>\n",
       "      <td>755</td>\n",
       "      <td>646</td>\n",
       "      <td>358</td>\n",
       "      <td>543</td>\n",
       "      <td>...</td>\n",
       "      <td>750</td>\n",
       "      <td>749</td>\n",
       "      <td>755</td>\n",
       "      <td>752</td>\n",
       "      <td>753</td>\n",
       "      <td>749</td>\n",
       "      <td>752</td>\n",
       "      <td>753</td>\n",
       "      <td>753</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1</td>\n",
       "      <td>0.82273</td>\n",
       "      <td>0.67772</td>\n",
       "      <td>0.34552</td>\n",
       "      <td>237</td>\n",
       "      <td>236</td>\n",
       "      <td>0.006004477</td>\n",
       "      <td>5.72E-05</td>\n",
       "      <td>0.00076</td>\n",
       "      <td>1.39E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5769</td>\n",
       "      <td>1.7726</td>\n",
       "      <td>4.0251</td>\n",
       "      <td>2.8454</td>\n",
       "      <td>3.8369</td>\n",
       "      <td>2.6829</td>\n",
       "      <td>3.1761</td>\n",
       "      <td>3.1854</td>\n",
       "      <td>4.6983</td>\n",
       "      <td>3.2941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>390</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0      gender      PPE      DFA     RPDE numPulses numPeriodsPulses  \\\n",
       "count     756      756      756      756       756              756   \n",
       "unique      2      740      745      748       315              319   \n",
       "top         1  0.82273  0.67772  0.34552       237              236   \n",
       "freq      390        3        2        2         9                8   \n",
       "\n",
       "0      meanPeriodPulses stdDevPeriodPulses locPctJitter locAbsJitter  ...  \\\n",
       "count               756                756          756          756  ...   \n",
       "unique              755                646          358          543  ...   \n",
       "top         0.006004477           5.72E-05      0.00076     1.39E-05  ...   \n",
       "freq                  2                  3            9           10  ...   \n",
       "\n",
       "0      tqwt_kurtosisValue_dec_27 tqwt_kurtosisValue_dec_28  \\\n",
       "count                        756                       756   \n",
       "unique                       750                       749   \n",
       "top                       1.5769                    1.7726   \n",
       "freq                           2                         2   \n",
       "\n",
       "0      tqwt_kurtosisValue_dec_29 tqwt_kurtosisValue_dec_30  \\\n",
       "count                        756                       756   \n",
       "unique                       755                       752   \n",
       "top                       4.0251                    2.8454   \n",
       "freq                           2                         2   \n",
       "\n",
       "0      tqwt_kurtosisValue_dec_31 tqwt_kurtosisValue_dec_32  \\\n",
       "count                        756                       756   \n",
       "unique                       753                       749   \n",
       "top                       3.8369                    2.6829   \n",
       "freq                           2                         2   \n",
       "\n",
       "0      tqwt_kurtosisValue_dec_33 tqwt_kurtosisValue_dec_34  \\\n",
       "count                        756                       756   \n",
       "unique                       752                       753   \n",
       "top                       3.1761                    3.1854   \n",
       "freq                           2                         2   \n",
       "\n",
       "0      tqwt_kurtosisValue_dec_35 tqwt_kurtosisValue_dec_36  \n",
       "count                        756                       756  \n",
       "unique                       753                       754  \n",
       "top                       4.6983                    3.2941  \n",
       "freq                           2                         2  \n",
       "\n",
       "[4 rows x 753 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1ba2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "gender                       0\n",
       "PPE                          0\n",
       "DFA                          0\n",
       "RPDE                         0\n",
       "numPulses                    0\n",
       "                            ..\n",
       "tqwt_kurtosisValue_dec_32    0\n",
       "tqwt_kurtosisValue_dec_33    0\n",
       "tqwt_kurtosisValue_dec_34    0\n",
       "tqwt_kurtosisValue_dec_35    0\n",
       "tqwt_kurtosisValue_dec_36    0\n",
       "Length: 753, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09db083b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.032266</td>\n",
       "      <td>0.257864</td>\n",
       "      <td>1.940467</td>\n",
       "      <td>-1.133279</td>\n",
       "      <td>0.484378</td>\n",
       "      <td>0.486441</td>\n",
       "      <td>-0.642778</td>\n",
       "      <td>-0.459234</td>\n",
       "      <td>-0.454801</td>\n",
       "      <td>-0.475130</td>\n",
       "      <td>...</td>\n",
       "      <td>2.088255</td>\n",
       "      <td>0.827330</td>\n",
       "      <td>-0.079161</td>\n",
       "      <td>-0.473962</td>\n",
       "      <td>-0.367988</td>\n",
       "      <td>-0.152981</td>\n",
       "      <td>-0.155671</td>\n",
       "      <td>-0.427653</td>\n",
       "      <td>-0.542947</td>\n",
       "      <td>-0.841500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.968742</td>\n",
       "      <td>-1.501193</td>\n",
       "      <td>0.795386</td>\n",
       "      <td>2.626417</td>\n",
       "      <td>-2.137821</td>\n",
       "      <td>-2.171186</td>\n",
       "      <td>1.578448</td>\n",
       "      <td>0.283569</td>\n",
       "      <td>9.681276</td>\n",
       "      <td>10.475578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208912</td>\n",
       "      <td>-0.576036</td>\n",
       "      <td>-0.633385</td>\n",
       "      <td>0.419752</td>\n",
       "      <td>0.715008</td>\n",
       "      <td>0.843071</td>\n",
       "      <td>1.893812</td>\n",
       "      <td>2.468662</td>\n",
       "      <td>1.769540</td>\n",
       "      <td>1.802510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.968742</td>\n",
       "      <td>0.269981</td>\n",
       "      <td>-0.518486</td>\n",
       "      <td>1.490572</td>\n",
       "      <td>-1.129283</td>\n",
       "      <td>-1.124242</td>\n",
       "      <td>1.503512</td>\n",
       "      <td>-0.373331</td>\n",
       "      <td>0.462853</td>\n",
       "      <td>0.675783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.428646</td>\n",
       "      <td>-0.570862</td>\n",
       "      <td>-0.635529</td>\n",
       "      <td>-0.558373</td>\n",
       "      <td>-0.395473</td>\n",
       "      <td>-0.451790</td>\n",
       "      <td>-0.563457</td>\n",
       "      <td>-0.751003</td>\n",
       "      <td>-0.818596</td>\n",
       "      <td>-0.809066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.968742</td>\n",
       "      <td>-2.982856</td>\n",
       "      <td>1.191385</td>\n",
       "      <td>1.051771</td>\n",
       "      <td>-0.856977</td>\n",
       "      <td>-0.852440</td>\n",
       "      <td>0.950548</td>\n",
       "      <td>-0.309330</td>\n",
       "      <td>-0.070224</td>\n",
       "      <td>0.024735</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445183</td>\n",
       "      <td>-0.583632</td>\n",
       "      <td>-0.247240</td>\n",
       "      <td>0.072722</td>\n",
       "      <td>-0.413783</td>\n",
       "      <td>-0.434662</td>\n",
       "      <td>-0.385316</td>\n",
       "      <td>-0.048913</td>\n",
       "      <td>-0.284645</td>\n",
       "      <td>-0.601517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.968742</td>\n",
       "      <td>0.693668</td>\n",
       "      <td>1.871860</td>\n",
       "      <td>0.515410</td>\n",
       "      <td>-0.746038</td>\n",
       "      <td>-0.741705</td>\n",
       "      <td>0.740511</td>\n",
       "      <td>-0.402301</td>\n",
       "      <td>0.519968</td>\n",
       "      <td>0.514113</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445325</td>\n",
       "      <td>-0.580508</td>\n",
       "      <td>2.218354</td>\n",
       "      <td>0.265335</td>\n",
       "      <td>-0.489833</td>\n",
       "      <td>-0.503906</td>\n",
       "      <td>-0.518181</td>\n",
       "      <td>-0.720173</td>\n",
       "      <td>-0.810678</td>\n",
       "      <td>-0.825612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>0.968742</td>\n",
       "      <td>0.356811</td>\n",
       "      <td>-1.226950</td>\n",
       "      <td>0.809182</td>\n",
       "      <td>-0.826721</td>\n",
       "      <td>-0.822239</td>\n",
       "      <td>0.883171</td>\n",
       "      <td>-0.432280</td>\n",
       "      <td>-0.268224</td>\n",
       "      <td>-0.167521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.438564</td>\n",
       "      <td>-0.577821</td>\n",
       "      <td>-0.586705</td>\n",
       "      <td>-0.611365</td>\n",
       "      <td>-0.561147</td>\n",
       "      <td>-0.536198</td>\n",
       "      <td>-0.519350</td>\n",
       "      <td>-0.522739</td>\n",
       "      <td>-0.412860</td>\n",
       "      <td>-0.505970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>-1.032266</td>\n",
       "      <td>0.271695</td>\n",
       "      <td>0.306236</td>\n",
       "      <td>-0.259171</td>\n",
       "      <td>0.222159</td>\n",
       "      <td>0.224705</td>\n",
       "      <td>-0.428701</td>\n",
       "      <td>-0.454008</td>\n",
       "      <td>-0.511916</td>\n",
       "      <td>-0.493481</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.338660</td>\n",
       "      <td>-0.495221</td>\n",
       "      <td>-0.506566</td>\n",
       "      <td>-0.443336</td>\n",
       "      <td>-0.481692</td>\n",
       "      <td>-0.511783</td>\n",
       "      <td>-0.590541</td>\n",
       "      <td>0.253287</td>\n",
       "      <td>0.534654</td>\n",
       "      <td>1.516212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0.968742</td>\n",
       "      <td>0.362721</td>\n",
       "      <td>1.066227</td>\n",
       "      <td>-0.065726</td>\n",
       "      <td>-0.524160</td>\n",
       "      <td>-0.520236</td>\n",
       "      <td>0.397883</td>\n",
       "      <td>-0.427604</td>\n",
       "      <td>-0.310109</td>\n",
       "      <td>-0.263649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445902</td>\n",
       "      <td>-0.562465</td>\n",
       "      <td>-0.613449</td>\n",
       "      <td>-0.596949</td>\n",
       "      <td>-0.520563</td>\n",
       "      <td>-0.430903</td>\n",
       "      <td>-0.446954</td>\n",
       "      <td>-0.720160</td>\n",
       "      <td>-0.780768</td>\n",
       "      <td>-0.794885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>0.968742</td>\n",
       "      <td>0.518767</td>\n",
       "      <td>0.878920</td>\n",
       "      <td>-0.332050</td>\n",
       "      <td>-0.393050</td>\n",
       "      <td>-0.389368</td>\n",
       "      <td>0.225730</td>\n",
       "      <td>-0.448095</td>\n",
       "      <td>-0.416724</td>\n",
       "      <td>-0.366767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.442240</td>\n",
       "      <td>-0.260183</td>\n",
       "      <td>0.417682</td>\n",
       "      <td>-0.198322</td>\n",
       "      <td>-0.448596</td>\n",
       "      <td>-0.474753</td>\n",
       "      <td>-0.414341</td>\n",
       "      <td>1.262687</td>\n",
       "      <td>1.366613</td>\n",
       "      <td>1.470217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>-1.032266</td>\n",
       "      <td>0.437375</td>\n",
       "      <td>0.222415</td>\n",
       "      <td>0.393533</td>\n",
       "      <td>-0.140915</td>\n",
       "      <td>-0.137699</td>\n",
       "      <td>-0.081814</td>\n",
       "      <td>-0.427879</td>\n",
       "      <td>-0.260609</td>\n",
       "      <td>-0.285496</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.433095</td>\n",
       "      <td>0.168397</td>\n",
       "      <td>-0.328878</td>\n",
       "      <td>-0.438853</td>\n",
       "      <td>-0.453004</td>\n",
       "      <td>-0.438010</td>\n",
       "      <td>-0.341564</td>\n",
       "      <td>0.757535</td>\n",
       "      <td>0.684181</td>\n",
       "      <td>0.864084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0      gender       PPE       DFA      RPDE  numPulses  numPeriodsPulses  \\\n",
       "0   -1.032266  0.257864  1.940467 -1.133279   0.484378          0.486441   \n",
       "1    0.968742 -1.501193  0.795386  2.626417  -2.137821         -2.171186   \n",
       "2    0.968742  0.269981 -0.518486  1.490572  -1.129283         -1.124242   \n",
       "3    0.968742 -2.982856  1.191385  1.051771  -0.856977         -0.852440   \n",
       "4    0.968742  0.693668  1.871860  0.515410  -0.746038         -0.741705   \n",
       "..        ...       ...       ...       ...        ...               ...   \n",
       "751  0.968742  0.356811 -1.226950  0.809182  -0.826721         -0.822239   \n",
       "752 -1.032266  0.271695  0.306236 -0.259171   0.222159          0.224705   \n",
       "753  0.968742  0.362721  1.066227 -0.065726  -0.524160         -0.520236   \n",
       "754  0.968742  0.518767  0.878920 -0.332050  -0.393050         -0.389368   \n",
       "755 -1.032266  0.437375  0.222415  0.393533  -0.140915         -0.137699   \n",
       "\n",
       "0    meanPeriodPulses  stdDevPeriodPulses  locPctJitter  locAbsJitter  ...  \\\n",
       "0           -0.642778           -0.459234     -0.454801     -0.475130  ...   \n",
       "1            1.578448            0.283569      9.681276     10.475578  ...   \n",
       "2            1.503512           -0.373331      0.462853      0.675783  ...   \n",
       "3            0.950548           -0.309330     -0.070224      0.024735  ...   \n",
       "4            0.740511           -0.402301      0.519968      0.514113  ...   \n",
       "..                ...                 ...           ...           ...  ...   \n",
       "751          0.883171           -0.432280     -0.268224     -0.167521  ...   \n",
       "752         -0.428701           -0.454008     -0.511916     -0.493481  ...   \n",
       "753          0.397883           -0.427604     -0.310109     -0.263649  ...   \n",
       "754          0.225730           -0.448095     -0.416724     -0.366767  ...   \n",
       "755         -0.081814           -0.427879     -0.260609     -0.285496  ...   \n",
       "\n",
       "0    tqwt_kurtosisValue_dec_27  tqwt_kurtosisValue_dec_28  \\\n",
       "0                     2.088255                   0.827330   \n",
       "1                    -0.208912                  -0.576036   \n",
       "2                    -0.428646                  -0.570862   \n",
       "3                    -0.445183                  -0.583632   \n",
       "4                    -0.445325                  -0.580508   \n",
       "..                         ...                        ...   \n",
       "751                  -0.438564                  -0.577821   \n",
       "752                  -0.338660                  -0.495221   \n",
       "753                  -0.445902                  -0.562465   \n",
       "754                  -0.442240                  -0.260183   \n",
       "755                  -0.433095                   0.168397   \n",
       "\n",
       "0    tqwt_kurtosisValue_dec_29  tqwt_kurtosisValue_dec_30  \\\n",
       "0                    -0.079161                  -0.473962   \n",
       "1                    -0.633385                   0.419752   \n",
       "2                    -0.635529                  -0.558373   \n",
       "3                    -0.247240                   0.072722   \n",
       "4                     2.218354                   0.265335   \n",
       "..                         ...                        ...   \n",
       "751                  -0.586705                  -0.611365   \n",
       "752                  -0.506566                  -0.443336   \n",
       "753                  -0.613449                  -0.596949   \n",
       "754                   0.417682                  -0.198322   \n",
       "755                  -0.328878                  -0.438853   \n",
       "\n",
       "0    tqwt_kurtosisValue_dec_31  tqwt_kurtosisValue_dec_32  \\\n",
       "0                    -0.367988                  -0.152981   \n",
       "1                     0.715008                   0.843071   \n",
       "2                    -0.395473                  -0.451790   \n",
       "3                    -0.413783                  -0.434662   \n",
       "4                    -0.489833                  -0.503906   \n",
       "..                         ...                        ...   \n",
       "751                  -0.561147                  -0.536198   \n",
       "752                  -0.481692                  -0.511783   \n",
       "753                  -0.520563                  -0.430903   \n",
       "754                  -0.448596                  -0.474753   \n",
       "755                  -0.453004                  -0.438010   \n",
       "\n",
       "0    tqwt_kurtosisValue_dec_33  tqwt_kurtosisValue_dec_34  \\\n",
       "0                    -0.155671                  -0.427653   \n",
       "1                     1.893812                   2.468662   \n",
       "2                    -0.563457                  -0.751003   \n",
       "3                    -0.385316                  -0.048913   \n",
       "4                    -0.518181                  -0.720173   \n",
       "..                         ...                        ...   \n",
       "751                  -0.519350                  -0.522739   \n",
       "752                  -0.590541                   0.253287   \n",
       "753                  -0.446954                  -0.720160   \n",
       "754                  -0.414341                   1.262687   \n",
       "755                  -0.341564                   0.757535   \n",
       "\n",
       "0    tqwt_kurtosisValue_dec_35  tqwt_kurtosisValue_dec_36  \n",
       "0                    -0.542947                  -0.841500  \n",
       "1                     1.769540                   1.802510  \n",
       "2                    -0.818596                  -0.809066  \n",
       "3                    -0.284645                  -0.601517  \n",
       "4                    -0.810678                  -0.825612  \n",
       "..                         ...                        ...  \n",
       "751                  -0.412860                  -0.505970  \n",
       "752                   0.534654                   1.516212  \n",
       "753                  -0.780768                  -0.794885  \n",
       "754                   1.366613                   1.470217  \n",
       "755                   0.684181                   0.864084  \n",
       "\n",
       "[756 rows x 753 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler()\n",
    "data_scaled = pd.DataFrame(scalar.fit_transform(data), columns=data.columns)\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548865d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension Reduction done\n"
     ]
    }
   ],
   "source": [
    "# Method 1 for dimension reduction\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model1=RandomForestRegressor(random_state=2,max_depth=10)\n",
    "model1.fit(data,labels)\n",
    "print(\"Dimension Reduction done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b6cced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0</td>\n",
       "      <td>0.78991</td>\n",
       "      <td>0.83561</td>\n",
       "      <td>0.3334</td>\n",
       "      <td>372</td>\n",
       "      <td>371</td>\n",
       "      <td>0.005186792</td>\n",
       "      <td>4.90E-05</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>5.86E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>112.2577</td>\n",
       "      <td>61.1446</td>\n",
       "      <td>20.2593</td>\n",
       "      <td>6.4921</td>\n",
       "      <td>6.5002</td>\n",
       "      <td>9.5002</td>\n",
       "      <td>9.8331</td>\n",
       "      <td>8.0799</td>\n",
       "      <td>6.9204</td>\n",
       "      <td>2.6948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>1</td>\n",
       "      <td>0.49231</td>\n",
       "      <td>0.75583</td>\n",
       "      <td>0.8498</td>\n",
       "      <td>112</td>\n",
       "      <td>107</td>\n",
       "      <td>0.009241015</td>\n",
       "      <td>0.000589142</td>\n",
       "      <td>0.02775</td>\n",
       "      <td>0.00025648</td>\n",
       "      <td>...</td>\n",
       "      <td>11.8991</td>\n",
       "      <td>1.9327</td>\n",
       "      <td>2.1889</td>\n",
       "      <td>29.3002</td>\n",
       "      <td>28.1956</td>\n",
       "      <td>27.2019</td>\n",
       "      <td>43.3029</td>\n",
       "      <td>53.5871</td>\n",
       "      <td>40.2744</td>\n",
       "      <td>93.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0.79196</td>\n",
       "      <td>0.66429</td>\n",
       "      <td>0.69379</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>0.009104241</td>\n",
       "      <td>0.000111466</td>\n",
       "      <td>0.00354</td>\n",
       "      <td>3.22E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2994</td>\n",
       "      <td>2.151</td>\n",
       "      <td>2.119</td>\n",
       "      <td>4.3379</td>\n",
       "      <td>5.9496</td>\n",
       "      <td>4.1898</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>2.9994</td>\n",
       "      <td>2.9446</td>\n",
       "      <td>3.8043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>1</td>\n",
       "      <td>0.24164</td>\n",
       "      <td>0.78342</td>\n",
       "      <td>0.63352</td>\n",
       "      <td>239</td>\n",
       "      <td>238</td>\n",
       "      <td>0.00809496</td>\n",
       "      <td>0.000158005</td>\n",
       "      <td>0.00214</td>\n",
       "      <td>1.73E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5769</td>\n",
       "      <td>1.6122</td>\n",
       "      <td>14.7791</td>\n",
       "      <td>20.4438</td>\n",
       "      <td>5.5828</td>\n",
       "      <td>4.4942</td>\n",
       "      <td>6.0828</td>\n",
       "      <td>14.0307</td>\n",
       "      <td>10.646</td>\n",
       "      <td>10.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>1</td>\n",
       "      <td>0.86364</td>\n",
       "      <td>0.83083</td>\n",
       "      <td>0.55985</td>\n",
       "      <td>250</td>\n",
       "      <td>249</td>\n",
       "      <td>0.007711596</td>\n",
       "      <td>9.04E-05</td>\n",
       "      <td>0.00369</td>\n",
       "      <td>2.85E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5707</td>\n",
       "      <td>1.744</td>\n",
       "      <td>95.1695</td>\n",
       "      <td>25.3594</td>\n",
       "      <td>4.0593</td>\n",
       "      <td>3.2636</td>\n",
       "      <td>3.913</td>\n",
       "      <td>3.4838</td>\n",
       "      <td>3.0588</td>\n",
       "      <td>3.2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80665</td>\n",
       "      <td>0.61493</td>\n",
       "      <td>0.6002</td>\n",
       "      <td>242</td>\n",
       "      <td>241</td>\n",
       "      <td>0.007971983</td>\n",
       "      <td>6.86E-05</td>\n",
       "      <td>0.00162</td>\n",
       "      <td>1.29E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8661</td>\n",
       "      <td>1.8574</td>\n",
       "      <td>3.7109</td>\n",
       "      <td>2.9855</td>\n",
       "      <td>2.6307</td>\n",
       "      <td>2.6897</td>\n",
       "      <td>3.8939</td>\n",
       "      <td>6.5859</td>\n",
       "      <td>8.7967</td>\n",
       "      <td>14.1727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0</td>\n",
       "      <td>0.79225</td>\n",
       "      <td>0.72175</td>\n",
       "      <td>0.45346</td>\n",
       "      <td>346</td>\n",
       "      <td>345</td>\n",
       "      <td>0.00557753</td>\n",
       "      <td>5.28E-05</td>\n",
       "      <td>0.00098</td>\n",
       "      <td>5.44E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2307</td>\n",
       "      <td>5.3425</td>\n",
       "      <td>6.3238</td>\n",
       "      <td>7.2737</td>\n",
       "      <td>4.2224</td>\n",
       "      <td>3.1236</td>\n",
       "      <td>2.7313</td>\n",
       "      <td>18.7789</td>\n",
       "      <td>22.4631</td>\n",
       "      <td>83.3482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80765</td>\n",
       "      <td>0.7747</td>\n",
       "      <td>0.48003</td>\n",
       "      <td>272</td>\n",
       "      <td>271</td>\n",
       "      <td>0.007086226</td>\n",
       "      <td>7.20E-05</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>1.07E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5455</td>\n",
       "      <td>2.5053</td>\n",
       "      <td>2.8389</td>\n",
       "      <td>3.3534</td>\n",
       "      <td>3.4437</td>\n",
       "      <td>4.561</td>\n",
       "      <td>5.0762</td>\n",
       "      <td>3.484</td>\n",
       "      <td>3.4902</td>\n",
       "      <td>4.2894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>1</td>\n",
       "      <td>0.83405</td>\n",
       "      <td>0.76165</td>\n",
       "      <td>0.44345</td>\n",
       "      <td>285</td>\n",
       "      <td>284</td>\n",
       "      <td>0.006772009</td>\n",
       "      <td>5.71E-05</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>8.34E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7055</td>\n",
       "      <td>15.2594</td>\n",
       "      <td>36.4588</td>\n",
       "      <td>13.5266</td>\n",
       "      <td>4.8854</td>\n",
       "      <td>3.7817</td>\n",
       "      <td>5.6088</td>\n",
       "      <td>34.6387</td>\n",
       "      <td>34.4628</td>\n",
       "      <td>81.7748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "      <td>0.82028</td>\n",
       "      <td>0.71591</td>\n",
       "      <td>0.54311</td>\n",
       "      <td>310</td>\n",
       "      <td>309</td>\n",
       "      <td>0.006210673</td>\n",
       "      <td>7.18E-05</td>\n",
       "      <td>0.00164</td>\n",
       "      <td>1.02E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.105</td>\n",
       "      <td>33.3424</td>\n",
       "      <td>12.1173</td>\n",
       "      <td>7.3881</td>\n",
       "      <td>4.7971</td>\n",
       "      <td>4.4347</td>\n",
       "      <td>6.7973</td>\n",
       "      <td>26.7017</td>\n",
       "      <td>24.6198</td>\n",
       "      <td>61.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0   gender      PPE      DFA     RPDE numPulses numPeriodsPulses  \\\n",
       "411      0  0.78991  0.83561   0.3334       372              371   \n",
       "426      1  0.49231  0.75583   0.8498       112              107   \n",
       "31       1  0.79196  0.66429  0.69379       212              211   \n",
       "538      1  0.24164  0.78342  0.63352       239              238   \n",
       "468      1  0.86364  0.83083  0.55985       250              249   \n",
       "..     ...      ...      ...      ...       ...              ...   \n",
       "171      1  0.80665  0.61493   0.6002       242              241   \n",
       "515      0  0.79225  0.72175  0.45346       346              345   \n",
       "622      1  0.80765   0.7747  0.48003       272              271   \n",
       "687      1  0.83405  0.76165  0.44345       285              284   \n",
       "187      0  0.82028  0.71591  0.54311       310              309   \n",
       "\n",
       "0   meanPeriodPulses stdDevPeriodPulses locPctJitter locAbsJitter  ...  \\\n",
       "411      0.005186792           4.90E-05      0.00113     5.86E-06  ...   \n",
       "426      0.009241015        0.000589142      0.02775   0.00025648  ...   \n",
       "31       0.009104241        0.000111466      0.00354     3.22E-05  ...   \n",
       "538       0.00809496        0.000158005      0.00214     1.73E-05  ...   \n",
       "468      0.007711596           9.04E-05      0.00369     2.85E-05  ...   \n",
       "..               ...                ...          ...          ...  ...   \n",
       "171      0.007971983           6.86E-05      0.00162     1.29E-05  ...   \n",
       "515       0.00557753           5.28E-05      0.00098     5.44E-06  ...   \n",
       "622      0.007086226           7.20E-05      0.00151     1.07E-05  ...   \n",
       "687      0.006772009           5.71E-05      0.00123     8.34E-06  ...   \n",
       "187      0.006210673           7.18E-05      0.00164     1.02E-05  ...   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_27 tqwt_kurtosisValue_dec_28  \\\n",
       "411                  112.2577                   61.1446   \n",
       "426                   11.8991                    1.9327   \n",
       "31                     2.2994                     2.151   \n",
       "538                    1.5769                    1.6122   \n",
       "468                    1.5707                     1.744   \n",
       "..                        ...                       ...   \n",
       "171                    1.8661                    1.8574   \n",
       "515                    6.2307                    5.3425   \n",
       "622                    1.5455                    2.5053   \n",
       "687                    1.7055                   15.2594   \n",
       "187                     2.105                   33.3424   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_29 tqwt_kurtosisValue_dec_30  \\\n",
       "411                   20.2593                    6.4921   \n",
       "426                    2.1889                   29.3002   \n",
       "31                      2.119                    4.3379   \n",
       "538                   14.7791                   20.4438   \n",
       "468                   95.1695                   25.3594   \n",
       "..                        ...                       ...   \n",
       "171                    3.7109                    2.9855   \n",
       "515                    6.3238                    7.2737   \n",
       "622                    2.8389                    3.3534   \n",
       "687                   36.4588                   13.5266   \n",
       "187                   12.1173                    7.3881   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_31 tqwt_kurtosisValue_dec_32  \\\n",
       "411                    6.5002                    9.5002   \n",
       "426                   28.1956                   27.2019   \n",
       "31                     5.9496                    4.1898   \n",
       "538                    5.5828                    4.4942   \n",
       "468                    4.0593                    3.2636   \n",
       "..                        ...                       ...   \n",
       "171                    2.6307                    2.6897   \n",
       "515                    4.2224                    3.1236   \n",
       "622                    3.4437                     4.561   \n",
       "687                    4.8854                    3.7817   \n",
       "187                    4.7971                    4.4347   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_33 tqwt_kurtosisValue_dec_34  \\\n",
       "411                    9.8331                    8.0799   \n",
       "426                   43.3029                   53.5871   \n",
       "31                     3.1736                    2.9994   \n",
       "538                    6.0828                   14.0307   \n",
       "468                     3.913                    3.4838   \n",
       "..                        ...                       ...   \n",
       "171                    3.8939                    6.5859   \n",
       "515                    2.7313                   18.7789   \n",
       "622                    5.0762                     3.484   \n",
       "687                    5.6088                   34.6387   \n",
       "187                    6.7973                   26.7017   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_35 tqwt_kurtosisValue_dec_36  \n",
       "411                    6.9204                    2.6948  \n",
       "426                   40.2744                    93.142  \n",
       "31                     2.9446                    3.8043  \n",
       "538                    10.646                   10.9042  \n",
       "468                    3.0588                    3.2383  \n",
       "..                        ...                       ...  \n",
       "171                    8.7967                   14.1727  \n",
       "515                   22.4631                   83.3482  \n",
       "622                    3.4902                    4.2894  \n",
       "687                   34.4628                   81.7748  \n",
       "187                   24.6198                     61.04  \n",
       "\n",
       "[756 rows x 753 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e99794fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features=data.columns\n",
    "importances=model1.feature_importances_\n",
    "indices = np.argsort(importances)[-20:]  # top 80 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "selected_features=[features[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5abfd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_after_RF=data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf3d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tqwt_skewnessValue_dec_24</th>\n",
       "      <th>std_6th_delta</th>\n",
       "      <th>tqwt_entropy_log_dec_27</th>\n",
       "      <th>std_9th_delta_delta</th>\n",
       "      <th>tqwt_entropy_shannon_dec_12</th>\n",
       "      <th>mean_MFCC_6th_coef</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>tqwt_energy_dec_18</th>\n",
       "      <th>std_10th_delta_delta</th>\n",
       "      <th>std_6th_delta_delta</th>\n",
       "      <th>std_7th_delta_delta</th>\n",
       "      <th>tqwt_stdValue_dec_12</th>\n",
       "      <th>tqwt_TKEO_std_dec_11</th>\n",
       "      <th>tqwt_energy_dec_27</th>\n",
       "      <th>std_delta_log_energy</th>\n",
       "      <th>tqwt_TKEO_mean_dec_12</th>\n",
       "      <th>mean_MFCC_2nd_coef</th>\n",
       "      <th>tqwt_entropy_log_dec_12</th>\n",
       "      <th>tqwt_TKEO_std_dec_12</th>\n",
       "      <th>std_delta_delta_log_energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>-0.00029958</td>\n",
       "      <td>0.032073</td>\n",
       "      <td>-10726.3095</td>\n",
       "      <td>0.015058</td>\n",
       "      <td>48.4839</td>\n",
       "      <td>-0.19598</td>\n",
       "      <td>5.86E-06</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.013091</td>\n",
       "      <td>0.014143</td>\n",
       "      <td>0.017356</td>\n",
       "      <td>0.021192</td>\n",
       "      <td>0.00019547</td>\n",
       "      <td>0.00016759</td>\n",
       "      <td>0.018759</td>\n",
       "      <td>0.00053895</td>\n",
       "      <td>2.3489</td>\n",
       "      <td>-145042.4744</td>\n",
       "      <td>0.00075748</td>\n",
       "      <td>0.0051358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>-1.20E-18</td>\n",
       "      <td>0.040426</td>\n",
       "      <td>-9176.9057</td>\n",
       "      <td>0.015722</td>\n",
       "      <td>8.4419</td>\n",
       "      <td>-1.0409</td>\n",
       "      <td>0.00025648</td>\n",
       "      <td>0.083219</td>\n",
       "      <td>0.017352</td>\n",
       "      <td>0.018588</td>\n",
       "      <td>0.020494</td>\n",
       "      <td>0.0076739</td>\n",
       "      <td>2.18E-05</td>\n",
       "      <td>0.00033607</td>\n",
       "      <td>0.048941</td>\n",
       "      <td>4.14E-05</td>\n",
       "      <td>3.9355</td>\n",
       "      <td>-172557.0039</td>\n",
       "      <td>4.52E-05</td>\n",
       "      <td>0.022452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.32E-18</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>-6369.9783</td>\n",
       "      <td>0.020066</td>\n",
       "      <td>0.086544</td>\n",
       "      <td>-2.8153</td>\n",
       "      <td>3.22E-05</td>\n",
       "      <td>0.21857</td>\n",
       "      <td>0.018476</td>\n",
       "      <td>0.035266</td>\n",
       "      <td>0.024964</td>\n",
       "      <td>0.00063748</td>\n",
       "      <td>6.81E-06</td>\n",
       "      <td>0.067101</td>\n",
       "      <td>0.07242</td>\n",
       "      <td>5.54E-07</td>\n",
       "      <td>2.2431</td>\n",
       "      <td>-261249.66</td>\n",
       "      <td>1.30E-06</td>\n",
       "      <td>0.026327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>-0.0019922</td>\n",
       "      <td>0.041979</td>\n",
       "      <td>17.621</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>16.7259</td>\n",
       "      <td>0.38704</td>\n",
       "      <td>1.73E-05</td>\n",
       "      <td>0.12811</td>\n",
       "      <td>0.015995</td>\n",
       "      <td>0.020189</td>\n",
       "      <td>0.019487</td>\n",
       "      <td>0.011591</td>\n",
       "      <td>0.00024843</td>\n",
       "      <td>0.27013</td>\n",
       "      <td>0.028962</td>\n",
       "      <td>0.00012562</td>\n",
       "      <td>1.6865</td>\n",
       "      <td>-164970.5561</td>\n",
       "      <td>0.00031427</td>\n",
       "      <td>0.011061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>0.00041304</td>\n",
       "      <td>0.050808</td>\n",
       "      <td>549.294</td>\n",
       "      <td>0.017201</td>\n",
       "      <td>137.6285</td>\n",
       "      <td>-1.1413</td>\n",
       "      <td>2.85E-05</td>\n",
       "      <td>0.020056</td>\n",
       "      <td>0.017581</td>\n",
       "      <td>0.023861</td>\n",
       "      <td>0.028446</td>\n",
       "      <td>0.040842</td>\n",
       "      <td>0.0025688</td>\n",
       "      <td>0.41665</td>\n",
       "      <td>0.015464</td>\n",
       "      <td>0.0027555</td>\n",
       "      <td>1.4488</td>\n",
       "      <td>-130406.8761</td>\n",
       "      <td>0.0049282</td>\n",
       "      <td>0.0060511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.0088649</td>\n",
       "      <td>0.061597</td>\n",
       "      <td>-5075.6649</td>\n",
       "      <td>0.02768</td>\n",
       "      <td>26.6079</td>\n",
       "      <td>-1.4697</td>\n",
       "      <td>1.29E-05</td>\n",
       "      <td>0.019566</td>\n",
       "      <td>0.027767</td>\n",
       "      <td>0.033289</td>\n",
       "      <td>0.03063</td>\n",
       "      <td>0.014968</td>\n",
       "      <td>0.00012132</td>\n",
       "      <td>0.059441</td>\n",
       "      <td>0.039684</td>\n",
       "      <td>0.00023119</td>\n",
       "      <td>1.1062</td>\n",
       "      <td>-156747.1144</td>\n",
       "      <td>0.00033258</td>\n",
       "      <td>0.016826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>1.60E-17</td>\n",
       "      <td>0.057036</td>\n",
       "      <td>-14711.0868</td>\n",
       "      <td>0.032378</td>\n",
       "      <td>0.2675</td>\n",
       "      <td>-0.63448</td>\n",
       "      <td>5.44E-06</td>\n",
       "      <td>0.087692</td>\n",
       "      <td>0.025188</td>\n",
       "      <td>0.026885</td>\n",
       "      <td>0.029635</td>\n",
       "      <td>0.0011556</td>\n",
       "      <td>1.39E-06</td>\n",
       "      <td>0.00048897</td>\n",
       "      <td>0.073896</td>\n",
       "      <td>1.45E-06</td>\n",
       "      <td>1.9985</td>\n",
       "      <td>-236364.5077</td>\n",
       "      <td>2.25E-06</td>\n",
       "      <td>0.028994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>1.04E-17</td>\n",
       "      <td>0.067411</td>\n",
       "      <td>-2869.6198</td>\n",
       "      <td>0.026591</td>\n",
       "      <td>1.8727</td>\n",
       "      <td>-0.9291</td>\n",
       "      <td>1.07E-05</td>\n",
       "      <td>0.019044</td>\n",
       "      <td>0.022668</td>\n",
       "      <td>0.032264</td>\n",
       "      <td>0.031133</td>\n",
       "      <td>0.0033371</td>\n",
       "      <td>1.22E-05</td>\n",
       "      <td>0.16015</td>\n",
       "      <td>0.028267</td>\n",
       "      <td>5.85E-06</td>\n",
       "      <td>2.2783</td>\n",
       "      <td>-201009.6316</td>\n",
       "      <td>1.70E-05</td>\n",
       "      <td>0.01019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>-0.0015199</td>\n",
       "      <td>0.066852</td>\n",
       "      <td>-2962.8412</td>\n",
       "      <td>0.024793</td>\n",
       "      <td>5.6792</td>\n",
       "      <td>-2.0149</td>\n",
       "      <td>8.34E-06</td>\n",
       "      <td>0.077782</td>\n",
       "      <td>0.028525</td>\n",
       "      <td>0.033678</td>\n",
       "      <td>0.028338</td>\n",
       "      <td>0.0062376</td>\n",
       "      <td>0.00025729</td>\n",
       "      <td>0.078626</td>\n",
       "      <td>0.028909</td>\n",
       "      <td>4.36E-05</td>\n",
       "      <td>2.3952</td>\n",
       "      <td>-184099.5388</td>\n",
       "      <td>8.91E-05</td>\n",
       "      <td>0.01239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.0036402</td>\n",
       "      <td>0.077145</td>\n",
       "      <td>-6450.1828</td>\n",
       "      <td>0.02583</td>\n",
       "      <td>22.348</td>\n",
       "      <td>0.083614</td>\n",
       "      <td>1.02E-05</td>\n",
       "      <td>0.08688</td>\n",
       "      <td>0.023596</td>\n",
       "      <td>0.035782</td>\n",
       "      <td>0.033279</td>\n",
       "      <td>0.013439</td>\n",
       "      <td>6.26E-05</td>\n",
       "      <td>0.01519</td>\n",
       "      <td>0.04834</td>\n",
       "      <td>0.00015675</td>\n",
       "      <td>0.47468</td>\n",
       "      <td>-157541.325</td>\n",
       "      <td>0.00020727</td>\n",
       "      <td>0.016747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0   tqwt_skewnessValue_dec_24 std_6th_delta tqwt_entropy_log_dec_27  \\\n",
       "411               -0.00029958      0.032073             -10726.3095   \n",
       "426                 -1.20E-18      0.040426              -9176.9057   \n",
       "31                   2.32E-18      0.075811              -6369.9783   \n",
       "538                -0.0019922      0.041979                  17.621   \n",
       "468                0.00041304      0.050808                 549.294   \n",
       "..                        ...           ...                     ...   \n",
       "171                 0.0088649      0.061597              -5075.6649   \n",
       "515                  1.60E-17      0.057036             -14711.0868   \n",
       "622                  1.04E-17      0.067411              -2869.6198   \n",
       "687                -0.0015199      0.066852              -2962.8412   \n",
       "187                 0.0036402      0.077145              -6450.1828   \n",
       "\n",
       "0   std_9th_delta_delta tqwt_entropy_shannon_dec_12 mean_MFCC_6th_coef  \\\n",
       "411            0.015058                     48.4839           -0.19598   \n",
       "426            0.015722                      8.4419            -1.0409   \n",
       "31             0.020066                    0.086544            -2.8153   \n",
       "538            0.016949                     16.7259            0.38704   \n",
       "468            0.017201                    137.6285            -1.1413   \n",
       "..                  ...                         ...                ...   \n",
       "171             0.02768                     26.6079            -1.4697   \n",
       "515            0.032378                      0.2675           -0.63448   \n",
       "622            0.026591                      1.8727            -0.9291   \n",
       "687            0.024793                      5.6792            -2.0149   \n",
       "187             0.02583                      22.348           0.083614   \n",
       "\n",
       "0   locAbsJitter tqwt_energy_dec_18 std_10th_delta_delta std_6th_delta_delta  \\\n",
       "411     5.86E-06           0.005927             0.013091            0.014143   \n",
       "426   0.00025648           0.083219             0.017352            0.018588   \n",
       "31      3.22E-05            0.21857             0.018476            0.035266   \n",
       "538     1.73E-05            0.12811             0.015995            0.020189   \n",
       "468     2.85E-05           0.020056             0.017581            0.023861   \n",
       "..           ...                ...                  ...                 ...   \n",
       "171     1.29E-05           0.019566             0.027767            0.033289   \n",
       "515     5.44E-06           0.087692             0.025188            0.026885   \n",
       "622     1.07E-05           0.019044             0.022668            0.032264   \n",
       "687     8.34E-06           0.077782             0.028525            0.033678   \n",
       "187     1.02E-05            0.08688             0.023596            0.035782   \n",
       "\n",
       "0   std_7th_delta_delta tqwt_stdValue_dec_12 tqwt_TKEO_std_dec_11  \\\n",
       "411            0.017356             0.021192           0.00019547   \n",
       "426            0.020494            0.0076739             2.18E-05   \n",
       "31             0.024964           0.00063748             6.81E-06   \n",
       "538            0.019487             0.011591           0.00024843   \n",
       "468            0.028446             0.040842            0.0025688   \n",
       "..                  ...                  ...                  ...   \n",
       "171             0.03063             0.014968           0.00012132   \n",
       "515            0.029635            0.0011556             1.39E-06   \n",
       "622            0.031133            0.0033371             1.22E-05   \n",
       "687            0.028338            0.0062376           0.00025729   \n",
       "187            0.033279             0.013439             6.26E-05   \n",
       "\n",
       "0   tqwt_energy_dec_27 std_delta_log_energy tqwt_TKEO_mean_dec_12  \\\n",
       "411         0.00016759             0.018759            0.00053895   \n",
       "426         0.00033607             0.048941              4.14E-05   \n",
       "31            0.067101              0.07242              5.54E-07   \n",
       "538            0.27013             0.028962            0.00012562   \n",
       "468            0.41665             0.015464             0.0027555   \n",
       "..                 ...                  ...                   ...   \n",
       "171           0.059441             0.039684            0.00023119   \n",
       "515         0.00048897             0.073896              1.45E-06   \n",
       "622            0.16015             0.028267              5.85E-06   \n",
       "687           0.078626             0.028909              4.36E-05   \n",
       "187            0.01519              0.04834            0.00015675   \n",
       "\n",
       "0   mean_MFCC_2nd_coef tqwt_entropy_log_dec_12 tqwt_TKEO_std_dec_12  \\\n",
       "411             2.3489            -145042.4744           0.00075748   \n",
       "426             3.9355            -172557.0039             4.52E-05   \n",
       "31              2.2431              -261249.66             1.30E-06   \n",
       "538             1.6865            -164970.5561           0.00031427   \n",
       "468             1.4488            -130406.8761            0.0049282   \n",
       "..                 ...                     ...                  ...   \n",
       "171             1.1062            -156747.1144           0.00033258   \n",
       "515             1.9985            -236364.5077             2.25E-06   \n",
       "622             2.2783            -201009.6316             1.70E-05   \n",
       "687             2.3952            -184099.5388             8.91E-05   \n",
       "187            0.47468             -157541.325           0.00020727   \n",
       "\n",
       "0   std_delta_delta_log_energy  \n",
       "411                  0.0051358  \n",
       "426                   0.022452  \n",
       "31                    0.026327  \n",
       "538                   0.011061  \n",
       "468                  0.0060511  \n",
       "..                         ...  \n",
       "171                   0.016826  \n",
       "515                   0.028994  \n",
       "622                    0.01019  \n",
       "687                    0.01239  \n",
       "187                   0.016747  \n",
       "\n",
       "[756 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_after_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f07fd0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411    0\n",
       "426    1\n",
       "31     1\n",
       "538    1\n",
       "468    1\n",
       "      ..\n",
       "171    1\n",
       "515    1\n",
       "622    1\n",
       "687    1\n",
       "187    1\n",
       "Name: class, Length: 756, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd26555",
   "metadata": {},
   "source": [
    "<h1 > Halt and Stop </h1>\n",
    "<h2> PCA Code </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c6e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_1 = PCA(n_components=20)\n",
    "principalComponents = pca_1.fit_transform(data)\n",
    "data_after_RF=pd.DataFrame(principalComponents)\n",
    "data_after_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad11d9",
   "metadata": {},
   "source": [
    "<h2> PCA STOP </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d418924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Percentages are: [5, 10, 15, 20, 25]\n",
      "Splitting Position are: [0, 38, 114, 227, 378, 567]\n"
     ]
    }
   ],
   "source": [
    "# Defining Splitter for the dataset (Common for all)\n",
    "def splitter(len_array,lower_bound, incrementor, start_idx):\n",
    "    final=0\n",
    "    per=[]\n",
    "    sp=[start_idx]\n",
    "    for i in range(lower_bound,100,incrementor):\n",
    "        if final+i>100:\n",
    "            break\n",
    "        per.append(i)\n",
    "        final+=i\n",
    "    for i in per:\n",
    "        len_of_data=round((i/100)*len_array)\n",
    "        sp.append(sp[-1]+len_of_data)\n",
    "    return per,sp\n",
    "\n",
    "lower_bound=5\n",
    "incrementor=5\n",
    "start_idx=0\n",
    "\n",
    "        \n",
    "spliting_percentage, spliting_position=splitter(len(data),lower_bound,incrementor,start_idx)\n",
    "print(\"Splitting Percentages are: {}\".format(spliting_percentage))\n",
    "print(\"Splitting Position are: {}\".format(spliting_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4733af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_experiment=data_after_RF.copy(deep=True)\n",
    "labels_for_experiment=labels.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030a307",
   "metadata": {},
   "source": [
    "# Apply ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e3bc9",
   "metadata": {},
   "source": [
    "# ************************************** Logistic Regression **************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SImple Algorithm\n",
    "# splitting dataset into training and testing part\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    solvers = [\"lbfgs\",\"liblinear\"]\n",
    "    penalty = ['l2']\n",
    "    c_values = [100]\n",
    "    # define grid search\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0,)\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "#     print(\"Training set score for logreg_model: %f\" % grid_search.score(X_train , y_train))\n",
    "#     print(\"Testing  set score for logreg_model: %f\" % grid_search.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4df27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    solvers = [\"lbfgs\",\"liblinear\"]\n",
    "    penalty = ['l2','l1']\n",
    "    c_values = [10]\n",
    "    # define grid search\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=2, cv=cv, scoring='f1',error_score=0,)\n",
    "    grid_result = grid_search.fit(X_train, Y_train)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = grid_search.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201c88f",
   "metadata": {},
   "source": [
    "# ****KNN Classifer ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750a5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into training and testing part\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = KNeighborsClassifier()\n",
    "    params_grid = [{'n_neighbors': [2,3,4], 'weights' :['uniform'],'leaf_size':[4,5,6,7,8,9],'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "    knn_model = GridSearchCV(model, params_grid, cv=5)\n",
    "    knn_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', knn_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best #neighbors:',knn_model.best_estimator_.n_neighbors,\"\\n\") \n",
    "    print('Best weights:',knn_model.best_estimator_.weights,\"\\n\")\n",
    "    print('Best leaf_size:',knn_model.best_estimator_.leaf_size,\"\\n\")\n",
    "    print('Best algorithm:',knn_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = knn_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for knn_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for knn_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23feab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into training and testing part\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    model = KNeighborsClassifier()\n",
    "    params_grid = [{'n_neighbors': [2,3,4], 'weights' :['uniform'],'leaf_size':[4,5,6,7,8,9],'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "    knn_model = GridSearchCV(model, params_grid, cv=5)\n",
    "    knn_model.fit(X_train,Y_train)\n",
    "    print('Best score for training data:', knn_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best #neighbors:',knn_model.best_estimator_.n_neighbors,\"\\n\") \n",
    "    print('Best weights:',knn_model.best_estimator_.weights,\"\\n\")\n",
    "    print('Best leaf_size:',knn_model.best_estimator_.leaf_size,\"\\n\")\n",
    "    print('Best algorithm:',knn_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = knn_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for knn_model: %f\" % final_model.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for knn_model: %f\" % final_model.score(X_test  , Y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b349eed",
   "metadata": {},
   "source": [
    "# ** Support Vector Machine **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fbe345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = SVC()\n",
    "    params_grid = [{'kernel': ['linear'], 'C': [2]}]\n",
    "    svm_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    svm_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "    print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "    print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "    final_model = svm_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for svm_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for svm_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = SVC()\n",
    "    params_grid = [{'kernel': ['linear'], 'C': [100,150]}]\n",
    "    svm_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    svm_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "    print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "    print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "    final_model = svm_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for svm_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for svm_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106200ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6060496",
   "metadata": {},
   "source": [
    "# ** Decision Tree **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53ec404b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8096234996633992 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[29 13]\n",
      " [11 99]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.69      0.71        42\n",
      "           1       0.88      0.90      0.89       110\n",
      "\n",
      "    accuracy                           0.84       152\n",
      "   macro avg       0.80      0.80      0.80       152\n",
      "weighted avg       0.84      0.84      0.84       152\n",
      "\n",
      "Training set score for dc_model: 0.985099\n",
      "Testing  set score for dc_model: 0.842105\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = DecisionTreeClassifier()\n",
    "    params_grid = [{'max_depth': [8,9,10,11,12],'random_state':[42]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47cf0be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (38, 20)\n",
      "Shape of testing input X_test: (38, 20)\n",
      "Shape of training output Y_train: (38,)\n",
      "Shape of testing output Y_test: (38, 1)\n",
      "Best score for training data: 0.7606837606837606 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[ 6  5]\n",
      " [ 6 21]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.55      0.52        11\n",
      "           1       0.81      0.78      0.79        27\n",
      "\n",
      "    accuracy                           0.71        38\n",
      "   macro avg       0.65      0.66      0.66        38\n",
      "weighted avg       0.72      0.71      0.71        38\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.710526\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (76, 20)\n",
      "Shape of testing input X_test: (76, 20)\n",
      "Shape of training output Y_train: (76,)\n",
      "Shape of testing output Y_test: (76, 1)\n",
      "Best score for training data: 0.7112820512820512 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[ 9  6]\n",
      " [11 50]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.60      0.51        15\n",
      "           1       0.89      0.82      0.85        61\n",
      "\n",
      "    accuracy                           0.78        76\n",
      "   macro avg       0.67      0.71      0.68        76\n",
      "weighted avg       0.81      0.78      0.79        76\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.776316\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (113, 20)\n",
      "Shape of testing input X_test: (113, 20)\n",
      "Shape of training output Y_train: (113,)\n",
      "Shape of testing output Y_test: (113, 1)\n",
      "Best score for training data: 0.7524893314366999 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[21 14]\n",
      " [11 67]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.60      0.63        35\n",
      "           1       0.83      0.86      0.84        78\n",
      "\n",
      "    accuracy                           0.78       113\n",
      "   macro avg       0.74      0.73      0.73       113\n",
      "weighted avg       0.77      0.78      0.78       113\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.778761\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (151, 20)\n",
      "Shape of testing input X_test: (151, 20)\n",
      "Shape of training output Y_train: (151,)\n",
      "Shape of testing output Y_test: (151, 1)\n",
      "Best score for training data: 0.7486274509803922 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[26 18]\n",
      " [12 95]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.59      0.63        44\n",
      "           1       0.84      0.89      0.86       107\n",
      "\n",
      "    accuracy                           0.80       151\n",
      "   macro avg       0.76      0.74      0.75       151\n",
      "weighted avg       0.80      0.80      0.80       151\n",
      "\n",
      "Training set score for dc_model: 0.986755\n",
      "Testing  set score for dc_model: 0.801325\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (189, 20)\n",
      "Shape of testing input X_test: (189, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.7513227513227513 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[ 31  24]\n",
      " [ 15 119]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.56      0.61        55\n",
      "           1       0.83      0.89      0.86       134\n",
      "\n",
      "    accuracy                           0.79       189\n",
      "   macro avg       0.75      0.73      0.74       189\n",
      "weighted avg       0.79      0.79      0.79       189\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.793651\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = DecisionTreeClassifier()\n",
    "    params_grid = [{'max_depth': [8,9,10,11,12],'random_state':[42]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,Y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , Y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220e996",
   "metadata": {},
   "source": [
    "<h1> **************************************************************************************************************</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6111fc",
   "metadata": {},
   "source": [
    "<h1> AdaBoost </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88665124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.3\n",
      "Best score for training data: 0.81668378702277 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 0.01 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[ 32  31]\n",
      " [ 20 144]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.51      0.56        63\n",
      "           1       0.82      0.88      0.85       164\n",
      "\n",
      "    accuracy                           0.78       227\n",
      "   macro avg       0.72      0.69      0.70       227\n",
      "weighted avg       0.77      0.78      0.77       227\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.775330\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.25\n",
      "Best score for training data: 0.8059964726631393 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[ 33  19]\n",
      " [ 14 123]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.63      0.67        52\n",
      "           1       0.87      0.90      0.88       137\n",
      "\n",
      "    accuracy                           0.83       189\n",
      "   macro avg       0.78      0.77      0.77       189\n",
      "weighted avg       0.82      0.83      0.82       189\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.825397\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8277917343973203 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[ 22  18]\n",
      " [ 12 100]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.55      0.59        40\n",
      "           1       0.85      0.89      0.87       112\n",
      "\n",
      "    accuracy                           0.80       152\n",
      "   macro avg       0.75      0.72      0.73       152\n",
      "weighted avg       0.79      0.80      0.80       152\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.802632\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.15\n",
      "Best score for training data: 0.792834890965732 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[23 10]\n",
      " [11 70]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69        33\n",
      "           1       0.88      0.86      0.87        81\n",
      "\n",
      "    accuracy                           0.82       114\n",
      "   macro avg       0.78      0.78      0.78       114\n",
      "weighted avg       0.82      0.82      0.82       114\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.815789\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.1\n",
      "Best score for training data: 0.8146985822515042 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[13  5]\n",
      " [ 6 52]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70        18\n",
      "           1       0.91      0.90      0.90        58\n",
      "\n",
      "    accuracy                           0.86        76\n",
      "   macro avg       0.80      0.81      0.80        76\n",
      "weighted avg       0.86      0.86      0.86        76\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.855263\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.05\n",
      "Best score for training data: 0.8008310088331009 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.01 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[ 8  5]\n",
      " [ 3 22]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.62      0.67        13\n",
      "           1       0.81      0.88      0.85        25\n",
      "\n",
      "    accuracy                           0.79        38\n",
      "   macro avg       0.77      0.75      0.76        38\n",
      "weighted avg       0.78      0.79      0.78        38\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.789474\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.3,0.25,0.2,0.15,0.1,0.05]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = DecisionTreeClassifier(max_features=None)\n",
    "    params_grid = [{'n_estimators':[2,4],'learning_rate':[1.0,1e-1,1e-2],'algorithm':['SAMME.R','SAMME']}]\n",
    "    model_ada=AdaBoostClassifier(model,)\n",
    "    dc_model = GridSearchCV(model_ada, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best estimator:',dc_model.best_estimator_.n_estimators,\"\\n\") \n",
    "    print('Best #learning rate:',dc_model.best_estimator_.learning_rate,\"\\n\")\n",
    "    print('Best algorithm:',dc_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75299f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (38, 20)\n",
      "Shape of testing input X_test: (38, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.844017094017094 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[ 4  3]\n",
      " [10 21]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.57      0.38         7\n",
      "           1       0.88      0.68      0.76        31\n",
      "\n",
      "    accuracy                           0.66        38\n",
      "   macro avg       0.58      0.62      0.57        38\n",
      "weighted avg       0.77      0.66      0.69        38\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.657895\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (76, 20)\n",
      "Shape of testing input X_test: (76, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.7251282051282052 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[11  3]\n",
      " [ 7 55]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.79      0.69        14\n",
      "           1       0.95      0.89      0.92        62\n",
      "\n",
      "    accuracy                           0.87        76\n",
      "   macro avg       0.78      0.84      0.80        76\n",
      "weighted avg       0.89      0.87      0.87        76\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.868421\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (113, 20)\n",
      "Shape of testing input X_test: (113, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.7527264106211474 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[19 15]\n",
      " [11 68]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.56      0.59        34\n",
      "           1       0.82      0.86      0.84        79\n",
      "\n",
      "    accuracy                           0.77       113\n",
      "   macro avg       0.73      0.71      0.72       113\n",
      "weighted avg       0.76      0.77      0.77       113\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.769912\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (151, 20)\n",
      "Shape of testing input X_test: (151, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.808235294117647 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.01 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[22 21]\n",
      " [14 94]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.51      0.56        43\n",
      "           1       0.82      0.87      0.84       108\n",
      "\n",
      "    accuracy                           0.77       151\n",
      "   macro avg       0.71      0.69      0.70       151\n",
      "weighted avg       0.76      0.77      0.76       151\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.768212\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (189, 20)\n",
      "Shape of testing input X_test: (189, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8148148148148149 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[ 36  18]\n",
      " [ 23 112]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.67      0.64        54\n",
      "           1       0.86      0.83      0.85       135\n",
      "\n",
      "    accuracy                           0.78       189\n",
      "   macro avg       0.74      0.75      0.74       189\n",
      "weighted avg       0.79      0.78      0.79       189\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.783069\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = DecisionTreeClassifier(max_features=None)\n",
    "    params_grid = [{'n_estimators':[2,4],'learning_rate':[1.0,1e-1,1e-2],'algorithm':['SAMME.R','SAMME']}]\n",
    "    model_ada=AdaBoostClassifier(model,)\n",
    "    dc_model = GridSearchCV(model_ada, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best estimator:',dc_model.best_estimator_.n_estimators,\"\\n\") \n",
    "    print('Best #learning rate:',dc_model.best_estimator_.learning_rate,\"\\n\")\n",
    "    print('Best algorithm:',dc_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d146a76",
   "metadata": {},
   "source": [
    "# ** Ensemble Learning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "660d880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "[[ 18  21]\n",
      " [  3 110]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.46      0.60        39\n",
      "           1       0.84      0.97      0.90       113\n",
      "\n",
      "    accuracy                           0.84       152\n",
      "   macro avg       0.85      0.72      0.75       152\n",
      "weighted avg       0.84      0.84      0.82       152\n",
      "\n",
      "Training set score for EL: 0.847682\n",
      "Testing  set score for EL: 0.842105\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data_ratio=[0.20]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    estimator = []\n",
    "    # make different combination in the ensemble classfier\n",
    "    estimator.append(('lr',LogisticRegression(solver='saga',penalty='l1',C=1,n_jobs=2)))\n",
    "    estimator.append(('SVC', SVC(gamma ='scale',probability=True,C=100,kernel='linear',)))\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=14,criterion='gini', splitter='best', min_samples_split=17,random_state=42)\n",
    "    model = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=250 ,learning_rate=1,algorithm='SAMME',random_state=7)\n",
    "    estimator.append(('DTC', model))\n",
    "    #estimator.append(('svc_rbf',SVC(gamma ='auto',C=4,kernel='rbf',)))\n",
    "\n",
    "\n",
    "    vot_hard = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "    vot_hard.fit(X_train,y_train)\n",
    "    Y_pred = vot_hard.predict(X_test)\n",
    "    \n",
    "    print(confusion_matrix(y_test,Y_pred))\n",
    "    print(classification_report(y_test,Y_pred))\n",
    "    print(\"Training set score for EL: %f\" % vot_hard.score(X_train , y_train))\n",
    "    print(\"Testing  set score for EL: %f\" % vot_hard.score(X_test  , y_test ))\n",
    "    \n",
    "\n",
    "    score = accuracy_score(y_test, Y_pred)\n",
    "    print(\"Hard Voting Score % d\" % score)\n",
    "    \n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4ebb89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (38, 20)\n",
      "Shape of testing input X_test: (38, 20)\n",
      "Shape of training output Y_train: (38,)\n",
      "Shape of testing output Y_test: (38, 1)\n",
      "[[ 0 12]\n",
      " [ 0 26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.68      1.00      0.81        26\n",
      "\n",
      "    accuracy                           0.68        38\n",
      "   macro avg       0.34      0.50      0.41        38\n",
      "weighted avg       0.47      0.68      0.56        38\n",
      "\n",
      "Training set score for EL: 0.789474\n",
      "Testing  set score for EL: 0.684211\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (76, 20)\n",
      "Shape of testing input X_test: (76, 20)\n",
      "Shape of training output Y_train: (76,)\n",
      "Shape of testing output Y_test: (76, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luhar\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\luhar\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\luhar\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 10]\n",
      " [ 0 59]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.41      0.58        17\n",
      "           1       0.86      1.00      0.92        59\n",
      "\n",
      "    accuracy                           0.87        76\n",
      "   macro avg       0.93      0.71      0.75        76\n",
      "weighted avg       0.89      0.87      0.85        76\n",
      "\n",
      "Training set score for EL: 0.921053\n",
      "Testing  set score for EL: 0.868421\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (113, 20)\n",
      "Shape of testing input X_test: (113, 20)\n",
      "Shape of training output Y_train: (113,)\n",
      "Shape of testing output Y_test: (113, 1)\n",
      "[[ 1 38]\n",
      " [ 0 74]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.03      0.05        39\n",
      "           1       0.66      1.00      0.80        74\n",
      "\n",
      "    accuracy                           0.66       113\n",
      "   macro avg       0.83      0.51      0.42       113\n",
      "weighted avg       0.78      0.66      0.54       113\n",
      "\n",
      "Training set score for EL: 0.823009\n",
      "Testing  set score for EL: 0.663717\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (151, 20)\n",
      "Shape of testing input X_test: (151, 20)\n",
      "Shape of training output Y_train: (151,)\n",
      "Shape of testing output Y_test: (151, 1)\n",
      "[[21 21]\n",
      " [10 99]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.50      0.58        42\n",
      "           1       0.82      0.91      0.86       109\n",
      "\n",
      "    accuracy                           0.79       151\n",
      "   macro avg       0.75      0.70      0.72       151\n",
      "weighted avg       0.78      0.79      0.78       151\n",
      "\n",
      "Training set score for EL: 0.854305\n",
      "Testing  set score for EL: 0.794702\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (189, 20)\n",
      "Shape of testing input X_test: (189, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "[[  9  36]\n",
      " [  3 141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.20      0.32        45\n",
      "           1       0.80      0.98      0.88       144\n",
      "\n",
      "    accuracy                           0.79       189\n",
      "   macro avg       0.77      0.59      0.60       189\n",
      "weighted avg       0.79      0.79      0.74       189\n",
      "\n",
      "Training set score for EL: 0.830688\n",
      "Testing  set score for EL: 0.793651\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    estimator = []\n",
    "    # make different combination in the ensemble classfier\n",
    "    estimator.append(('lr',LogisticRegression(solver='saga',penalty='l1',C=1,n_jobs=2)))\n",
    "    estimator.append(('SVC', SVC(gamma ='scale',probability=True,C=100,kernel='linear',)))\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=14,criterion='gini', splitter='best', min_samples_split=17,random_state=42)\n",
    "    model = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=250 ,learning_rate=1,algorithm='SAMME',random_state=7)\n",
    "    estimator.append(('DTC', model))\n",
    "    #estimator.append(('svc_rbf',SVC(gamma ='auto',C=4,kernel='rbf',)))\n",
    "\n",
    "\n",
    "    vot_hard = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "    vot_hard.fit(X_train,Y_train)\n",
    "    Y_pred = vot_hard.predict(X_test)\n",
    "    \n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "    print(\"Training set score for EL: %f\" % vot_hard.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for EL: %f\" % vot_hard.score(X_test  , Y_test ))\n",
    "    \n",
    "\n",
    "    score = accuracy_score(Y_test, Y_pred)\n",
    "    print(\"Hard Voting Score % d\" % score)\n",
    "    \n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdade4a",
   "metadata": {},
   "source": [
    "# ** Random Forest **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e4698ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8559266374398634 \n",
      "\n",
      "Best depth: 6 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 10 \n",
      "\n",
      "[[ 15  23]\n",
      " [  5 109]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.39      0.52        38\n",
      "           1       0.83      0.96      0.89       114\n",
      "\n",
      "    accuracy                           0.82       152\n",
      "   macro avg       0.79      0.68      0.70       152\n",
      "weighted avg       0.81      0.82      0.79       152\n",
      "\n",
      "Training set score for dc_model: 0.950331\n",
      "Testing  set score for dc_model: 0.815789\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = RandomForestClassifier()\n",
    "    params_grid = [{'max_depth': [5,6,7],'random_state':[42],'n_jobs':[10,15,20]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #estimators:',dc_model.best_estimator_.n_estimators,\"\\n\")\n",
    "    print('Best jobs:',dc_model.best_estimator_.n_jobs,\"\\n\") \n",
    "    #print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d30292d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (38, 20)\n",
      "Shape of testing input X_test: (38, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8183760683760685 \n",
      "\n",
      "Best depth: 5 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[ 7  4]\n",
      " [ 1 26]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.64      0.74        11\n",
      "           1       0.87      0.96      0.91        27\n",
      "\n",
      "    accuracy                           0.87        38\n",
      "   macro avg       0.87      0.80      0.82        38\n",
      "weighted avg       0.87      0.87      0.86        38\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.868421\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (76, 20)\n",
      "Shape of testing input X_test: (76, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8948717948717948 \n",
      "\n",
      "Best depth: 5 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[12 16]\n",
      " [ 3 45]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.43      0.56        28\n",
      "           1       0.74      0.94      0.83        48\n",
      "\n",
      "    accuracy                           0.75        76\n",
      "   macro avg       0.77      0.68      0.69        76\n",
      "weighted avg       0.76      0.75      0.73        76\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.750000\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (113, 20)\n",
      "Shape of testing input X_test: (113, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8667614983404457 \n",
      "\n",
      "Best depth: 6 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[11 25]\n",
      " [ 2 75]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.31      0.45        36\n",
      "           1       0.75      0.97      0.85        77\n",
      "\n",
      "    accuracy                           0.76       113\n",
      "   macro avg       0.80      0.64      0.65       113\n",
      "weighted avg       0.78      0.76      0.72       113\n",
      "\n",
      "Training set score for dc_model: 0.982301\n",
      "Testing  set score for dc_model: 0.761062\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (151, 20)\n",
      "Shape of testing input X_test: (151, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8210457516339869 \n",
      "\n",
      "Best depth: 6 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[ 21  10]\n",
      " [  9 111]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.68      0.69        31\n",
      "           1       0.92      0.93      0.92       120\n",
      "\n",
      "    accuracy                           0.87       151\n",
      "   macro avg       0.81      0.80      0.80       151\n",
      "weighted avg       0.87      0.87      0.87       151\n",
      "\n",
      "Training set score for dc_model: 0.993377\n",
      "Testing  set score for dc_model: 0.874172\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (189, 20)\n",
      "Shape of testing input X_test: (189, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8412698412698413 \n",
      "\n",
      "Best depth: 7 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[ 27  19]\n",
      " [  6 137]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.59      0.68        46\n",
      "           1       0.88      0.96      0.92       143\n",
      "\n",
      "    accuracy                           0.87       189\n",
      "   macro avg       0.85      0.77      0.80       189\n",
      "weighted avg       0.86      0.87      0.86       189\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.867725\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    params_grid = [{'max_depth': [5,6,7],'random_state':[42],'n_jobs':[2]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #estimators:',dc_model.best_estimator_.n_estimators,\"\\n\")\n",
    "    print('Best jobs:',dc_model.best_estimator_.n_jobs,\"\\n\") \n",
    "    #print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de26e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75dcf68a",
   "metadata": {},
   "source": [
    "# ...........THANK YOU.........HAPPY CODING......."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
