{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476d9e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "from sklearn.model_selection import train_test_split,RepeatedStratifiedKFold,GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8ae230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data=pd.read_csv(r\"Data\\pd_speech_features.csv\",)\n",
    "    data.columns = data.iloc[0]\n",
    "    data= data.iloc[1: , :]\n",
    "    data=data.drop('id',axis=1)\n",
    "    data=data.sample(frac=1)\n",
    "    labels=data['class']\n",
    "    data=data.drop('class',axis=1)\n",
    "    \n",
    "    return data,labels\n",
    "\n",
    "data,labels=load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91151582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>1</td>\n",
       "      <td>0.79865</td>\n",
       "      <td>0.57376</td>\n",
       "      <td>0.47711</td>\n",
       "      <td>347</td>\n",
       "      <td>346</td>\n",
       "      <td>0.005571637</td>\n",
       "      <td>5.82E-05</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>5.36E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9887</td>\n",
       "      <td>4.1005</td>\n",
       "      <td>3.3895</td>\n",
       "      <td>2.5501</td>\n",
       "      <td>2.6555</td>\n",
       "      <td>2.7239</td>\n",
       "      <td>2.2796</td>\n",
       "      <td>2.576</td>\n",
       "      <td>2.5895</td>\n",
       "      <td>3.4479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0</td>\n",
       "      <td>0.8568</td>\n",
       "      <td>0.80533</td>\n",
       "      <td>0.46087</td>\n",
       "      <td>435</td>\n",
       "      <td>434</td>\n",
       "      <td>0.004438307</td>\n",
       "      <td>5.28E-05</td>\n",
       "      <td>0.00106</td>\n",
       "      <td>4.72E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.024</td>\n",
       "      <td>3.0937</td>\n",
       "      <td>3.2624</td>\n",
       "      <td>3.5665</td>\n",
       "      <td>5.7636</td>\n",
       "      <td>26.5364</td>\n",
       "      <td>30.308</td>\n",
       "      <td>25.0281</td>\n",
       "      <td>24.7547</td>\n",
       "      <td>68.6664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>0</td>\n",
       "      <td>0.84386</td>\n",
       "      <td>0.75649</td>\n",
       "      <td>0.43172</td>\n",
       "      <td>363</td>\n",
       "      <td>362</td>\n",
       "      <td>0.005312216</td>\n",
       "      <td>3.90E-05</td>\n",
       "      <td>0.00078</td>\n",
       "      <td>4.14E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1989</td>\n",
       "      <td>2.2336</td>\n",
       "      <td>2.3275</td>\n",
       "      <td>2.8376</td>\n",
       "      <td>3.1743</td>\n",
       "      <td>2.8949</td>\n",
       "      <td>3.1839</td>\n",
       "      <td>4.1253</td>\n",
       "      <td>4.5805</td>\n",
       "      <td>3.7793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1</td>\n",
       "      <td>0.81634</td>\n",
       "      <td>0.64393</td>\n",
       "      <td>0.57462</td>\n",
       "      <td>273</td>\n",
       "      <td>272</td>\n",
       "      <td>0.007080068</td>\n",
       "      <td>0.000136295</td>\n",
       "      <td>0.00179</td>\n",
       "      <td>1.27E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5963</td>\n",
       "      <td>3.6303</td>\n",
       "      <td>102.503</td>\n",
       "      <td>35.4456</td>\n",
       "      <td>32.6962</td>\n",
       "      <td>23.8895</td>\n",
       "      <td>20.8759</td>\n",
       "      <td>27.5055</td>\n",
       "      <td>20.7775</td>\n",
       "      <td>14.5989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0</td>\n",
       "      <td>0.8276</td>\n",
       "      <td>0.63579</td>\n",
       "      <td>0.24278</td>\n",
       "      <td>404</td>\n",
       "      <td>403</td>\n",
       "      <td>0.004779499</td>\n",
       "      <td>3.06E-05</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>3.08E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>215.4478</td>\n",
       "      <td>175.2876</td>\n",
       "      <td>123.4963</td>\n",
       "      <td>27.7693</td>\n",
       "      <td>5.2906</td>\n",
       "      <td>4.1913</td>\n",
       "      <td>4.3029</td>\n",
       "      <td>3.9654</td>\n",
       "      <td>3.2715</td>\n",
       "      <td>2.9646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0</td>\n",
       "      <td>0.82745</td>\n",
       "      <td>0.69727</td>\n",
       "      <td>0.37036</td>\n",
       "      <td>432</td>\n",
       "      <td>431</td>\n",
       "      <td>0.004468509</td>\n",
       "      <td>4.01E-05</td>\n",
       "      <td>0.00115</td>\n",
       "      <td>5.12E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9954</td>\n",
       "      <td>2.3879</td>\n",
       "      <td>2.7256</td>\n",
       "      <td>7.8633</td>\n",
       "      <td>9.151</td>\n",
       "      <td>10.7618</td>\n",
       "      <td>15.8496</td>\n",
       "      <td>16.579</td>\n",
       "      <td>22.694</td>\n",
       "      <td>89.0699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0</td>\n",
       "      <td>0.83576</td>\n",
       "      <td>0.79762</td>\n",
       "      <td>0.40497</td>\n",
       "      <td>358</td>\n",
       "      <td>357</td>\n",
       "      <td>0.005402546</td>\n",
       "      <td>4.74E-05</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>7.73E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.1359</td>\n",
       "      <td>79.3196</td>\n",
       "      <td>59.4084</td>\n",
       "      <td>40.291</td>\n",
       "      <td>15.7448</td>\n",
       "      <td>7.2957</td>\n",
       "      <td>9.693</td>\n",
       "      <td>11.4446</td>\n",
       "      <td>13.7622</td>\n",
       "      <td>6.557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80456</td>\n",
       "      <td>0.70612</td>\n",
       "      <td>0.33944</td>\n",
       "      <td>257</td>\n",
       "      <td>256</td>\n",
       "      <td>0.007493308</td>\n",
       "      <td>4.28E-05</td>\n",
       "      <td>0.00101</td>\n",
       "      <td>7.58E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5353</td>\n",
       "      <td>1.6125</td>\n",
       "      <td>43.5383</td>\n",
       "      <td>75.2876</td>\n",
       "      <td>76.0753</td>\n",
       "      <td>72.7387</td>\n",
       "      <td>61.8121</td>\n",
       "      <td>55.5402</td>\n",
       "      <td>47.0509</td>\n",
       "      <td>91.8193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>0</td>\n",
       "      <td>0.85971</td>\n",
       "      <td>0.67213</td>\n",
       "      <td>0.36905</td>\n",
       "      <td>393</td>\n",
       "      <td>392</td>\n",
       "      <td>0.004912711</td>\n",
       "      <td>6.14E-05</td>\n",
       "      <td>0.00094</td>\n",
       "      <td>4.64E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7255</td>\n",
       "      <td>1.6737</td>\n",
       "      <td>1.6235</td>\n",
       "      <td>71.3903</td>\n",
       "      <td>36.7387</td>\n",
       "      <td>10.3534</td>\n",
       "      <td>9.1317</td>\n",
       "      <td>18.7334</td>\n",
       "      <td>20.8992</td>\n",
       "      <td>14.5305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>0</td>\n",
       "      <td>0.83876</td>\n",
       "      <td>0.73382</td>\n",
       "      <td>0.44526</td>\n",
       "      <td>368</td>\n",
       "      <td>367</td>\n",
       "      <td>0.005241436</td>\n",
       "      <td>4.69E-05</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5.25E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>5.4031</td>\n",
       "      <td>2.8925</td>\n",
       "      <td>2.5414</td>\n",
       "      <td>2.6769</td>\n",
       "      <td>2.6605</td>\n",
       "      <td>2.2513</td>\n",
       "      <td>2.2612</td>\n",
       "      <td>7.2206</td>\n",
       "      <td>10.612</td>\n",
       "      <td>32.3344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0   gender      PPE      DFA     RPDE numPulses numPeriodsPulses  \\\n",
       "520      1  0.79865  0.57376  0.47711       347              346   \n",
       "254      0   0.8568  0.80533  0.46087       435              434   \n",
       "634      0  0.84386  0.75649  0.43172       363              362   \n",
       "129      1  0.81634  0.64393  0.57462       273              272   \n",
       "90       0   0.8276  0.63579  0.24278       404              403   \n",
       "..     ...      ...      ...      ...       ...              ...   \n",
       "618      0  0.82745  0.69727  0.37036       432              431   \n",
       "77       0  0.83576  0.79762  0.40497       358              357   \n",
       "502      1  0.80456  0.70612  0.33944       257              256   \n",
       "667      0  0.85971  0.67213  0.36905       393              392   \n",
       "357      0  0.83876  0.73382  0.44526       368              367   \n",
       "\n",
       "0   meanPeriodPulses stdDevPeriodPulses locPctJitter locAbsJitter  ...  \\\n",
       "520      0.005571637           5.82E-05      0.00096     5.36E-06  ...   \n",
       "254      0.004438307           5.28E-05      0.00106     4.72E-06  ...   \n",
       "634      0.005312216           3.90E-05      0.00078     4.14E-06  ...   \n",
       "129      0.007080068        0.000136295      0.00179     1.27E-05  ...   \n",
       "90       0.004779499           3.06E-05      0.00064     3.08E-06  ...   \n",
       "..               ...                ...          ...          ...  ...   \n",
       "618      0.004468509           4.01E-05      0.00115     5.12E-06  ...   \n",
       "77       0.005402546           4.74E-05      0.00143     7.73E-06  ...   \n",
       "502      0.007493308           4.28E-05      0.00101     7.58E-06  ...   \n",
       "667      0.004912711           6.14E-05      0.00094     4.64E-06  ...   \n",
       "357      0.005241436           4.69E-05        0.001     5.25E-06  ...   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_27 tqwt_kurtosisValue_dec_28  \\\n",
       "520                    2.9887                    4.1005   \n",
       "254                     8.024                    3.0937   \n",
       "634                    2.1989                    2.2336   \n",
       "129                    1.5963                    3.6303   \n",
       "90                   215.4478                  175.2876   \n",
       "..                        ...                       ...   \n",
       "618                    1.9954                    2.3879   \n",
       "77                     3.1359                   79.3196   \n",
       "502                    1.5353                    1.6125   \n",
       "667                    1.7255                    1.6737   \n",
       "357                    5.4031                    2.8925   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_29 tqwt_kurtosisValue_dec_30  \\\n",
       "520                    3.3895                    2.5501   \n",
       "254                    3.2624                    3.5665   \n",
       "634                    2.3275                    2.8376   \n",
       "129                   102.503                   35.4456   \n",
       "90                   123.4963                   27.7693   \n",
       "..                        ...                       ...   \n",
       "618                    2.7256                    7.8633   \n",
       "77                    59.4084                    40.291   \n",
       "502                   43.5383                   75.2876   \n",
       "667                    1.6235                   71.3903   \n",
       "357                    2.5414                    2.6769   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_31 tqwt_kurtosisValue_dec_32  \\\n",
       "520                    2.6555                    2.7239   \n",
       "254                    5.7636                   26.5364   \n",
       "634                    3.1743                    2.8949   \n",
       "129                   32.6962                   23.8895   \n",
       "90                     5.2906                    4.1913   \n",
       "..                        ...                       ...   \n",
       "618                     9.151                   10.7618   \n",
       "77                    15.7448                    7.2957   \n",
       "502                   76.0753                   72.7387   \n",
       "667                   36.7387                   10.3534   \n",
       "357                    2.6605                    2.2513   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_33 tqwt_kurtosisValue_dec_34  \\\n",
       "520                    2.2796                     2.576   \n",
       "254                    30.308                   25.0281   \n",
       "634                    3.1839                    4.1253   \n",
       "129                   20.8759                   27.5055   \n",
       "90                     4.3029                    3.9654   \n",
       "..                        ...                       ...   \n",
       "618                   15.8496                    16.579   \n",
       "77                      9.693                   11.4446   \n",
       "502                   61.8121                   55.5402   \n",
       "667                    9.1317                   18.7334   \n",
       "357                    2.2612                    7.2206   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_35 tqwt_kurtosisValue_dec_36  \n",
       "520                    2.5895                    3.4479  \n",
       "254                   24.7547                   68.6664  \n",
       "634                    4.5805                    3.7793  \n",
       "129                   20.7775                   14.5989  \n",
       "90                     3.2715                    2.9646  \n",
       "..                        ...                       ...  \n",
       "618                    22.694                   89.0699  \n",
       "77                    13.7622                     6.557  \n",
       "502                   47.0509                   91.8193  \n",
       "667                   20.8992                   14.5305  \n",
       "357                    10.612                   32.3344  \n",
       "\n",
       "[756 rows x 753 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc3bf6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>...</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>740</td>\n",
       "      <td>745</td>\n",
       "      <td>748</td>\n",
       "      <td>315</td>\n",
       "      <td>319</td>\n",
       "      <td>755</td>\n",
       "      <td>646</td>\n",
       "      <td>358</td>\n",
       "      <td>543</td>\n",
       "      <td>...</td>\n",
       "      <td>750</td>\n",
       "      <td>749</td>\n",
       "      <td>755</td>\n",
       "      <td>752</td>\n",
       "      <td>753</td>\n",
       "      <td>749</td>\n",
       "      <td>752</td>\n",
       "      <td>753</td>\n",
       "      <td>753</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>1</td>\n",
       "      <td>0.82273</td>\n",
       "      <td>0.80616</td>\n",
       "      <td>0.34552</td>\n",
       "      <td>237</td>\n",
       "      <td>236</td>\n",
       "      <td>0.006004477</td>\n",
       "      <td>5.83E-05</td>\n",
       "      <td>0.00076</td>\n",
       "      <td>1.39E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6695</td>\n",
       "      <td>11.5115</td>\n",
       "      <td>4.0251</td>\n",
       "      <td>3.0619</td>\n",
       "      <td>3.3603</td>\n",
       "      <td>3.2838</td>\n",
       "      <td>3.1144</td>\n",
       "      <td>12.595</td>\n",
       "      <td>4.2391</td>\n",
       "      <td>3.2941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>390</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0      gender      PPE      DFA     RPDE numPulses numPeriodsPulses  \\\n",
       "count     756      756      756      756       756              756   \n",
       "unique      2      740      745      748       315              319   \n",
       "top         1  0.82273  0.80616  0.34552       237              236   \n",
       "freq      390        3        2        2         9                8   \n",
       "\n",
       "0      meanPeriodPulses stdDevPeriodPulses locPctJitter locAbsJitter  ...  \\\n",
       "count               756                756          756          756  ...   \n",
       "unique              755                646          358          543  ...   \n",
       "top         0.006004477           5.83E-05      0.00076     1.39E-05  ...   \n",
       "freq                  2                  3            9           10  ...   \n",
       "\n",
       "0      tqwt_kurtosisValue_dec_27 tqwt_kurtosisValue_dec_28  \\\n",
       "count                        756                       756   \n",
       "unique                       750                       749   \n",
       "top                       1.6695                   11.5115   \n",
       "freq                           2                         2   \n",
       "\n",
       "0      tqwt_kurtosisValue_dec_29 tqwt_kurtosisValue_dec_30  \\\n",
       "count                        756                       756   \n",
       "unique                       755                       752   \n",
       "top                       4.0251                    3.0619   \n",
       "freq                           2                         2   \n",
       "\n",
       "0      tqwt_kurtosisValue_dec_31 tqwt_kurtosisValue_dec_32  \\\n",
       "count                        756                       756   \n",
       "unique                       753                       749   \n",
       "top                       3.3603                    3.2838   \n",
       "freq                           2                         2   \n",
       "\n",
       "0      tqwt_kurtosisValue_dec_33 tqwt_kurtosisValue_dec_34  \\\n",
       "count                        756                       756   \n",
       "unique                       752                       753   \n",
       "top                       3.1144                    12.595   \n",
       "freq                           2                         2   \n",
       "\n",
       "0      tqwt_kurtosisValue_dec_35 tqwt_kurtosisValue_dec_36  \n",
       "count                        756                       756  \n",
       "unique                       753                       754  \n",
       "top                       4.2391                    3.2941  \n",
       "freq                           2                         2  \n",
       "\n",
       "[4 rows x 753 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1ba2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "gender                       0\n",
       "PPE                          0\n",
       "DFA                          0\n",
       "RPDE                         0\n",
       "numPulses                    0\n",
       "                            ..\n",
       "tqwt_kurtosisValue_dec_32    0\n",
       "tqwt_kurtosisValue_dec_33    0\n",
       "tqwt_kurtosisValue_dec_34    0\n",
       "tqwt_kurtosisValue_dec_35    0\n",
       "tqwt_kurtosisValue_dec_36    0\n",
       "Length: 753, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09db083b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.968742</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>-1.817862</td>\n",
       "      <td>-0.086986</td>\n",
       "      <td>0.232244</td>\n",
       "      <td>0.234771</td>\n",
       "      <td>-0.431929</td>\n",
       "      <td>-0.446582</td>\n",
       "      <td>-0.519532</td>\n",
       "      <td>-0.496977</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.412868</td>\n",
       "      <td>-0.524658</td>\n",
       "      <td>-0.596562</td>\n",
       "      <td>-0.628426</td>\n",
       "      <td>-0.559909</td>\n",
       "      <td>-0.534274</td>\n",
       "      <td>-0.618200</td>\n",
       "      <td>-0.777950</td>\n",
       "      <td>-0.843215</td>\n",
       "      <td>-0.819485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.032266</td>\n",
       "      <td>0.653238</td>\n",
       "      <td>1.505859</td>\n",
       "      <td>-0.205222</td>\n",
       "      <td>1.119757</td>\n",
       "      <td>1.120647</td>\n",
       "      <td>-1.052858</td>\n",
       "      <td>-0.454008</td>\n",
       "      <td>-0.481455</td>\n",
       "      <td>-0.524941</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297612</td>\n",
       "      <td>-0.548519</td>\n",
       "      <td>-0.600460</td>\n",
       "      <td>-0.588599</td>\n",
       "      <td>-0.404758</td>\n",
       "      <td>0.805624</td>\n",
       "      <td>1.098085</td>\n",
       "      <td>0.651019</td>\n",
       "      <td>0.693534</td>\n",
       "      <td>1.087024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.032266</td>\n",
       "      <td>0.576752</td>\n",
       "      <td>0.804859</td>\n",
       "      <td>-0.417451</td>\n",
       "      <td>0.393610</td>\n",
       "      <td>0.395840</td>\n",
       "      <td>-0.574061</td>\n",
       "      <td>-0.472986</td>\n",
       "      <td>-0.588070</td>\n",
       "      <td>-0.550284</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430946</td>\n",
       "      <td>-0.568904</td>\n",
       "      <td>-0.629134</td>\n",
       "      <td>-0.617161</td>\n",
       "      <td>-0.534011</td>\n",
       "      <td>-0.524652</td>\n",
       "      <td>-0.562826</td>\n",
       "      <td>-0.679345</td>\n",
       "      <td>-0.705176</td>\n",
       "      <td>-0.809797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.968742</td>\n",
       "      <td>0.414086</td>\n",
       "      <td>-0.810713</td>\n",
       "      <td>0.622945</td>\n",
       "      <td>-0.514074</td>\n",
       "      <td>-0.510169</td>\n",
       "      <td>0.394509</td>\n",
       "      <td>-0.339186</td>\n",
       "      <td>-0.203493</td>\n",
       "      <td>-0.176260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.444739</td>\n",
       "      <td>-0.535802</td>\n",
       "      <td>2.443274</td>\n",
       "      <td>0.660554</td>\n",
       "      <td>0.939670</td>\n",
       "      <td>0.656686</td>\n",
       "      <td>0.520522</td>\n",
       "      <td>0.808693</td>\n",
       "      <td>0.417788</td>\n",
       "      <td>-0.493511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.032266</td>\n",
       "      <td>0.480642</td>\n",
       "      <td>-0.927546</td>\n",
       "      <td>-1.793046</td>\n",
       "      <td>0.807111</td>\n",
       "      <td>0.808577</td>\n",
       "      <td>-0.865926</td>\n",
       "      <td>-0.484538</td>\n",
       "      <td>-0.641378</td>\n",
       "      <td>-0.596600</td>\n",
       "      <td>...</td>\n",
       "      <td>4.450235</td>\n",
       "      <td>3.532605</td>\n",
       "      <td>3.087144</td>\n",
       "      <td>0.359765</td>\n",
       "      <td>-0.428369</td>\n",
       "      <td>-0.451705</td>\n",
       "      <td>-0.494306</td>\n",
       "      <td>-0.689521</td>\n",
       "      <td>-0.795931</td>\n",
       "      <td>-0.833613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>-1.032266</td>\n",
       "      <td>0.479755</td>\n",
       "      <td>-0.045125</td>\n",
       "      <td>-0.864188</td>\n",
       "      <td>1.089501</td>\n",
       "      <td>1.090447</td>\n",
       "      <td>-1.036311</td>\n",
       "      <td>-0.471473</td>\n",
       "      <td>-0.447185</td>\n",
       "      <td>-0.507464</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.435604</td>\n",
       "      <td>-0.565247</td>\n",
       "      <td>-0.616924</td>\n",
       "      <td>-0.420233</td>\n",
       "      <td>-0.235665</td>\n",
       "      <td>-0.081992</td>\n",
       "      <td>0.212742</td>\n",
       "      <td>0.113274</td>\n",
       "      <td>0.550662</td>\n",
       "      <td>1.683472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>-1.032266</td>\n",
       "      <td>0.528874</td>\n",
       "      <td>1.395197</td>\n",
       "      <td>-0.612207</td>\n",
       "      <td>0.343183</td>\n",
       "      <td>0.345506</td>\n",
       "      <td>-0.524571</td>\n",
       "      <td>-0.461434</td>\n",
       "      <td>-0.340570</td>\n",
       "      <td>-0.393421</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.409498</td>\n",
       "      <td>1.258091</td>\n",
       "      <td>1.121552</td>\n",
       "      <td>0.850416</td>\n",
       "      <td>0.093486</td>\n",
       "      <td>-0.277025</td>\n",
       "      <td>-0.164250</td>\n",
       "      <td>-0.213506</td>\n",
       "      <td>-0.068594</td>\n",
       "      <td>-0.728597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>0.968742</td>\n",
       "      <td>0.344457</td>\n",
       "      <td>0.081899</td>\n",
       "      <td>-1.089304</td>\n",
       "      <td>-0.675440</td>\n",
       "      <td>-0.671238</td>\n",
       "      <td>0.620915</td>\n",
       "      <td>-0.467760</td>\n",
       "      <td>-0.500493</td>\n",
       "      <td>-0.399975</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446135</td>\n",
       "      <td>-0.583625</td>\n",
       "      <td>0.634812</td>\n",
       "      <td>2.221726</td>\n",
       "      <td>3.105077</td>\n",
       "      <td>3.405366</td>\n",
       "      <td>3.027200</td>\n",
       "      <td>2.592968</td>\n",
       "      <td>2.239366</td>\n",
       "      <td>1.763844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>-1.032266</td>\n",
       "      <td>0.670438</td>\n",
       "      <td>-0.405959</td>\n",
       "      <td>-0.873726</td>\n",
       "      <td>0.696171</td>\n",
       "      <td>0.697843</td>\n",
       "      <td>-0.792941</td>\n",
       "      <td>-0.442181</td>\n",
       "      <td>-0.527147</td>\n",
       "      <td>-0.528437</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.441782</td>\n",
       "      <td>-0.582175</td>\n",
       "      <td>-0.650726</td>\n",
       "      <td>2.069014</td>\n",
       "      <td>1.141464</td>\n",
       "      <td>-0.104972</td>\n",
       "      <td>-0.198620</td>\n",
       "      <td>0.250391</td>\n",
       "      <td>0.426226</td>\n",
       "      <td>-0.495511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>755</th>\n",
       "      <td>-1.032266</td>\n",
       "      <td>0.546607</td>\n",
       "      <td>0.479477</td>\n",
       "      <td>-0.318872</td>\n",
       "      <td>0.444037</td>\n",
       "      <td>0.446174</td>\n",
       "      <td>-0.612840</td>\n",
       "      <td>-0.462122</td>\n",
       "      <td>-0.504301</td>\n",
       "      <td>-0.501783</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.357603</td>\n",
       "      <td>-0.553288</td>\n",
       "      <td>-0.622574</td>\n",
       "      <td>-0.623457</td>\n",
       "      <td>-0.559659</td>\n",
       "      <td>-0.560867</td>\n",
       "      <td>-0.619327</td>\n",
       "      <td>-0.482344</td>\n",
       "      <td>-0.287002</td>\n",
       "      <td>0.024944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0      gender       PPE       DFA      RPDE  numPulses  numPeriodsPulses  \\\n",
       "0    0.968742  0.309524 -1.817862 -0.086986   0.232244          0.234771   \n",
       "1   -1.032266  0.653238  1.505859 -0.205222   1.119757          1.120647   \n",
       "2   -1.032266  0.576752  0.804859 -0.417451   0.393610          0.395840   \n",
       "3    0.968742  0.414086 -0.810713  0.622945  -0.514074         -0.510169   \n",
       "4   -1.032266  0.480642 -0.927546 -1.793046   0.807111          0.808577   \n",
       "..        ...       ...       ...       ...        ...               ...   \n",
       "751 -1.032266  0.479755 -0.045125 -0.864188   1.089501          1.090447   \n",
       "752 -1.032266  0.528874  1.395197 -0.612207   0.343183          0.345506   \n",
       "753  0.968742  0.344457  0.081899 -1.089304  -0.675440         -0.671238   \n",
       "754 -1.032266  0.670438 -0.405959 -0.873726   0.696171          0.697843   \n",
       "755 -1.032266  0.546607  0.479477 -0.318872   0.444037          0.446174   \n",
       "\n",
       "0    meanPeriodPulses  stdDevPeriodPulses  locPctJitter  locAbsJitter  ...  \\\n",
       "0           -0.431929           -0.446582     -0.519532     -0.496977  ...   \n",
       "1           -1.052858           -0.454008     -0.481455     -0.524941  ...   \n",
       "2           -0.574061           -0.472986     -0.588070     -0.550284  ...   \n",
       "3            0.394509           -0.339186     -0.203493     -0.176260  ...   \n",
       "4           -0.865926           -0.484538     -0.641378     -0.596600  ...   \n",
       "..                ...                 ...           ...           ...  ...   \n",
       "751         -1.036311           -0.471473     -0.447185     -0.507464  ...   \n",
       "752         -0.524571           -0.461434     -0.340570     -0.393421  ...   \n",
       "753          0.620915           -0.467760     -0.500493     -0.399975  ...   \n",
       "754         -0.792941           -0.442181     -0.527147     -0.528437  ...   \n",
       "755         -0.612840           -0.462122     -0.504301     -0.501783  ...   \n",
       "\n",
       "0    tqwt_kurtosisValue_dec_27  tqwt_kurtosisValue_dec_28  \\\n",
       "0                    -0.412868                  -0.524658   \n",
       "1                    -0.297612                  -0.548519   \n",
       "2                    -0.430946                  -0.568904   \n",
       "3                    -0.444739                  -0.535802   \n",
       "4                     4.450235                   3.532605   \n",
       "..                         ...                        ...   \n",
       "751                  -0.435604                  -0.565247   \n",
       "752                  -0.409498                   1.258091   \n",
       "753                  -0.446135                  -0.583625   \n",
       "754                  -0.441782                  -0.582175   \n",
       "755                  -0.357603                  -0.553288   \n",
       "\n",
       "0    tqwt_kurtosisValue_dec_29  tqwt_kurtosisValue_dec_30  \\\n",
       "0                    -0.596562                  -0.628426   \n",
       "1                    -0.600460                  -0.588599   \n",
       "2                    -0.629134                  -0.617161   \n",
       "3                     2.443274                   0.660554   \n",
       "4                     3.087144                   0.359765   \n",
       "..                         ...                        ...   \n",
       "751                  -0.616924                  -0.420233   \n",
       "752                   1.121552                   0.850416   \n",
       "753                   0.634812                   2.221726   \n",
       "754                  -0.650726                   2.069014   \n",
       "755                  -0.622574                  -0.623457   \n",
       "\n",
       "0    tqwt_kurtosisValue_dec_31  tqwt_kurtosisValue_dec_32  \\\n",
       "0                    -0.559909                  -0.534274   \n",
       "1                    -0.404758                   0.805624   \n",
       "2                    -0.534011                  -0.524652   \n",
       "3                     0.939670                   0.656686   \n",
       "4                    -0.428369                  -0.451705   \n",
       "..                         ...                        ...   \n",
       "751                  -0.235665                  -0.081992   \n",
       "752                   0.093486                  -0.277025   \n",
       "753                   3.105077                   3.405366   \n",
       "754                   1.141464                  -0.104972   \n",
       "755                  -0.559659                  -0.560867   \n",
       "\n",
       "0    tqwt_kurtosisValue_dec_33  tqwt_kurtosisValue_dec_34  \\\n",
       "0                    -0.618200                  -0.777950   \n",
       "1                     1.098085                   0.651019   \n",
       "2                    -0.562826                  -0.679345   \n",
       "3                     0.520522                   0.808693   \n",
       "4                    -0.494306                  -0.689521   \n",
       "..                         ...                        ...   \n",
       "751                   0.212742                   0.113274   \n",
       "752                  -0.164250                  -0.213506   \n",
       "753                   3.027200                   2.592968   \n",
       "754                  -0.198620                   0.250391   \n",
       "755                  -0.619327                  -0.482344   \n",
       "\n",
       "0    tqwt_kurtosisValue_dec_35  tqwt_kurtosisValue_dec_36  \n",
       "0                    -0.843215                  -0.819485  \n",
       "1                     0.693534                   1.087024  \n",
       "2                    -0.705176                  -0.809797  \n",
       "3                     0.417788                  -0.493511  \n",
       "4                    -0.795931                  -0.833613  \n",
       "..                         ...                        ...  \n",
       "751                   0.550662                   1.683472  \n",
       "752                  -0.068594                  -0.728597  \n",
       "753                   2.239366                   1.763844  \n",
       "754                   0.426226                  -0.495511  \n",
       "755                  -0.287002                   0.024944  \n",
       "\n",
       "[756 rows x 753 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar = StandardScaler()\n",
    "data_scaled = pd.DataFrame(scalar.fit_transform(data), columns=data.columns)\n",
    "data_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548865d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension Reduction done\n"
     ]
    }
   ],
   "source": [
    "# Method 1 for dimension reduction\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model1=RandomForestRegressor(random_state=2,max_depth=10)\n",
    "model1.fit(data,labels)\n",
    "print(\"Dimension Reduction done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b6cced4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>PPE</th>\n",
       "      <th>DFA</th>\n",
       "      <th>RPDE</th>\n",
       "      <th>numPulses</th>\n",
       "      <th>numPeriodsPulses</th>\n",
       "      <th>meanPeriodPulses</th>\n",
       "      <th>stdDevPeriodPulses</th>\n",
       "      <th>locPctJitter</th>\n",
       "      <th>locAbsJitter</th>\n",
       "      <th>...</th>\n",
       "      <th>tqwt_kurtosisValue_dec_27</th>\n",
       "      <th>tqwt_kurtosisValue_dec_28</th>\n",
       "      <th>tqwt_kurtosisValue_dec_29</th>\n",
       "      <th>tqwt_kurtosisValue_dec_30</th>\n",
       "      <th>tqwt_kurtosisValue_dec_31</th>\n",
       "      <th>tqwt_kurtosisValue_dec_32</th>\n",
       "      <th>tqwt_kurtosisValue_dec_33</th>\n",
       "      <th>tqwt_kurtosisValue_dec_34</th>\n",
       "      <th>tqwt_kurtosisValue_dec_35</th>\n",
       "      <th>tqwt_kurtosisValue_dec_36</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0</td>\n",
       "      <td>0.78991</td>\n",
       "      <td>0.83561</td>\n",
       "      <td>0.3334</td>\n",
       "      <td>372</td>\n",
       "      <td>371</td>\n",
       "      <td>0.005186792</td>\n",
       "      <td>4.90E-05</td>\n",
       "      <td>0.00113</td>\n",
       "      <td>5.86E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>112.2577</td>\n",
       "      <td>61.1446</td>\n",
       "      <td>20.2593</td>\n",
       "      <td>6.4921</td>\n",
       "      <td>6.5002</td>\n",
       "      <td>9.5002</td>\n",
       "      <td>9.8331</td>\n",
       "      <td>8.0799</td>\n",
       "      <td>6.9204</td>\n",
       "      <td>2.6948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>1</td>\n",
       "      <td>0.49231</td>\n",
       "      <td>0.75583</td>\n",
       "      <td>0.8498</td>\n",
       "      <td>112</td>\n",
       "      <td>107</td>\n",
       "      <td>0.009241015</td>\n",
       "      <td>0.000589142</td>\n",
       "      <td>0.02775</td>\n",
       "      <td>0.00025648</td>\n",
       "      <td>...</td>\n",
       "      <td>11.8991</td>\n",
       "      <td>1.9327</td>\n",
       "      <td>2.1889</td>\n",
       "      <td>29.3002</td>\n",
       "      <td>28.1956</td>\n",
       "      <td>27.2019</td>\n",
       "      <td>43.3029</td>\n",
       "      <td>53.5871</td>\n",
       "      <td>40.2744</td>\n",
       "      <td>93.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0.79196</td>\n",
       "      <td>0.66429</td>\n",
       "      <td>0.69379</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>0.009104241</td>\n",
       "      <td>0.000111466</td>\n",
       "      <td>0.00354</td>\n",
       "      <td>3.22E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2994</td>\n",
       "      <td>2.151</td>\n",
       "      <td>2.119</td>\n",
       "      <td>4.3379</td>\n",
       "      <td>5.9496</td>\n",
       "      <td>4.1898</td>\n",
       "      <td>3.1736</td>\n",
       "      <td>2.9994</td>\n",
       "      <td>2.9446</td>\n",
       "      <td>3.8043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>1</td>\n",
       "      <td>0.24164</td>\n",
       "      <td>0.78342</td>\n",
       "      <td>0.63352</td>\n",
       "      <td>239</td>\n",
       "      <td>238</td>\n",
       "      <td>0.00809496</td>\n",
       "      <td>0.000158005</td>\n",
       "      <td>0.00214</td>\n",
       "      <td>1.73E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5769</td>\n",
       "      <td>1.6122</td>\n",
       "      <td>14.7791</td>\n",
       "      <td>20.4438</td>\n",
       "      <td>5.5828</td>\n",
       "      <td>4.4942</td>\n",
       "      <td>6.0828</td>\n",
       "      <td>14.0307</td>\n",
       "      <td>10.646</td>\n",
       "      <td>10.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>1</td>\n",
       "      <td>0.86364</td>\n",
       "      <td>0.83083</td>\n",
       "      <td>0.55985</td>\n",
       "      <td>250</td>\n",
       "      <td>249</td>\n",
       "      <td>0.007711596</td>\n",
       "      <td>9.04E-05</td>\n",
       "      <td>0.00369</td>\n",
       "      <td>2.85E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5707</td>\n",
       "      <td>1.744</td>\n",
       "      <td>95.1695</td>\n",
       "      <td>25.3594</td>\n",
       "      <td>4.0593</td>\n",
       "      <td>3.2636</td>\n",
       "      <td>3.913</td>\n",
       "      <td>3.4838</td>\n",
       "      <td>3.0588</td>\n",
       "      <td>3.2383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80665</td>\n",
       "      <td>0.61493</td>\n",
       "      <td>0.6002</td>\n",
       "      <td>242</td>\n",
       "      <td>241</td>\n",
       "      <td>0.007971983</td>\n",
       "      <td>6.86E-05</td>\n",
       "      <td>0.00162</td>\n",
       "      <td>1.29E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.8661</td>\n",
       "      <td>1.8574</td>\n",
       "      <td>3.7109</td>\n",
       "      <td>2.9855</td>\n",
       "      <td>2.6307</td>\n",
       "      <td>2.6897</td>\n",
       "      <td>3.8939</td>\n",
       "      <td>6.5859</td>\n",
       "      <td>8.7967</td>\n",
       "      <td>14.1727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>0</td>\n",
       "      <td>0.79225</td>\n",
       "      <td>0.72175</td>\n",
       "      <td>0.45346</td>\n",
       "      <td>346</td>\n",
       "      <td>345</td>\n",
       "      <td>0.00557753</td>\n",
       "      <td>5.28E-05</td>\n",
       "      <td>0.00098</td>\n",
       "      <td>5.44E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2307</td>\n",
       "      <td>5.3425</td>\n",
       "      <td>6.3238</td>\n",
       "      <td>7.2737</td>\n",
       "      <td>4.2224</td>\n",
       "      <td>3.1236</td>\n",
       "      <td>2.7313</td>\n",
       "      <td>18.7789</td>\n",
       "      <td>22.4631</td>\n",
       "      <td>83.3482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>1</td>\n",
       "      <td>0.80765</td>\n",
       "      <td>0.7747</td>\n",
       "      <td>0.48003</td>\n",
       "      <td>272</td>\n",
       "      <td>271</td>\n",
       "      <td>0.007086226</td>\n",
       "      <td>7.20E-05</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>1.07E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5455</td>\n",
       "      <td>2.5053</td>\n",
       "      <td>2.8389</td>\n",
       "      <td>3.3534</td>\n",
       "      <td>3.4437</td>\n",
       "      <td>4.561</td>\n",
       "      <td>5.0762</td>\n",
       "      <td>3.484</td>\n",
       "      <td>3.4902</td>\n",
       "      <td>4.2894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>1</td>\n",
       "      <td>0.83405</td>\n",
       "      <td>0.76165</td>\n",
       "      <td>0.44345</td>\n",
       "      <td>285</td>\n",
       "      <td>284</td>\n",
       "      <td>0.006772009</td>\n",
       "      <td>5.71E-05</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>8.34E-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7055</td>\n",
       "      <td>15.2594</td>\n",
       "      <td>36.4588</td>\n",
       "      <td>13.5266</td>\n",
       "      <td>4.8854</td>\n",
       "      <td>3.7817</td>\n",
       "      <td>5.6088</td>\n",
       "      <td>34.6387</td>\n",
       "      <td>34.4628</td>\n",
       "      <td>81.7748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0</td>\n",
       "      <td>0.82028</td>\n",
       "      <td>0.71591</td>\n",
       "      <td>0.54311</td>\n",
       "      <td>310</td>\n",
       "      <td>309</td>\n",
       "      <td>0.006210673</td>\n",
       "      <td>7.18E-05</td>\n",
       "      <td>0.00164</td>\n",
       "      <td>1.02E-05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.105</td>\n",
       "      <td>33.3424</td>\n",
       "      <td>12.1173</td>\n",
       "      <td>7.3881</td>\n",
       "      <td>4.7971</td>\n",
       "      <td>4.4347</td>\n",
       "      <td>6.7973</td>\n",
       "      <td>26.7017</td>\n",
       "      <td>24.6198</td>\n",
       "      <td>61.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 753 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0   gender      PPE      DFA     RPDE numPulses numPeriodsPulses  \\\n",
       "411      0  0.78991  0.83561   0.3334       372              371   \n",
       "426      1  0.49231  0.75583   0.8498       112              107   \n",
       "31       1  0.79196  0.66429  0.69379       212              211   \n",
       "538      1  0.24164  0.78342  0.63352       239              238   \n",
       "468      1  0.86364  0.83083  0.55985       250              249   \n",
       "..     ...      ...      ...      ...       ...              ...   \n",
       "171      1  0.80665  0.61493   0.6002       242              241   \n",
       "515      0  0.79225  0.72175  0.45346       346              345   \n",
       "622      1  0.80765   0.7747  0.48003       272              271   \n",
       "687      1  0.83405  0.76165  0.44345       285              284   \n",
       "187      0  0.82028  0.71591  0.54311       310              309   \n",
       "\n",
       "0   meanPeriodPulses stdDevPeriodPulses locPctJitter locAbsJitter  ...  \\\n",
       "411      0.005186792           4.90E-05      0.00113     5.86E-06  ...   \n",
       "426      0.009241015        0.000589142      0.02775   0.00025648  ...   \n",
       "31       0.009104241        0.000111466      0.00354     3.22E-05  ...   \n",
       "538       0.00809496        0.000158005      0.00214     1.73E-05  ...   \n",
       "468      0.007711596           9.04E-05      0.00369     2.85E-05  ...   \n",
       "..               ...                ...          ...          ...  ...   \n",
       "171      0.007971983           6.86E-05      0.00162     1.29E-05  ...   \n",
       "515       0.00557753           5.28E-05      0.00098     5.44E-06  ...   \n",
       "622      0.007086226           7.20E-05      0.00151     1.07E-05  ...   \n",
       "687      0.006772009           5.71E-05      0.00123     8.34E-06  ...   \n",
       "187      0.006210673           7.18E-05      0.00164     1.02E-05  ...   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_27 tqwt_kurtosisValue_dec_28  \\\n",
       "411                  112.2577                   61.1446   \n",
       "426                   11.8991                    1.9327   \n",
       "31                     2.2994                     2.151   \n",
       "538                    1.5769                    1.6122   \n",
       "468                    1.5707                     1.744   \n",
       "..                        ...                       ...   \n",
       "171                    1.8661                    1.8574   \n",
       "515                    6.2307                    5.3425   \n",
       "622                    1.5455                    2.5053   \n",
       "687                    1.7055                   15.2594   \n",
       "187                     2.105                   33.3424   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_29 tqwt_kurtosisValue_dec_30  \\\n",
       "411                   20.2593                    6.4921   \n",
       "426                    2.1889                   29.3002   \n",
       "31                      2.119                    4.3379   \n",
       "538                   14.7791                   20.4438   \n",
       "468                   95.1695                   25.3594   \n",
       "..                        ...                       ...   \n",
       "171                    3.7109                    2.9855   \n",
       "515                    6.3238                    7.2737   \n",
       "622                    2.8389                    3.3534   \n",
       "687                   36.4588                   13.5266   \n",
       "187                   12.1173                    7.3881   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_31 tqwt_kurtosisValue_dec_32  \\\n",
       "411                    6.5002                    9.5002   \n",
       "426                   28.1956                   27.2019   \n",
       "31                     5.9496                    4.1898   \n",
       "538                    5.5828                    4.4942   \n",
       "468                    4.0593                    3.2636   \n",
       "..                        ...                       ...   \n",
       "171                    2.6307                    2.6897   \n",
       "515                    4.2224                    3.1236   \n",
       "622                    3.4437                     4.561   \n",
       "687                    4.8854                    3.7817   \n",
       "187                    4.7971                    4.4347   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_33 tqwt_kurtosisValue_dec_34  \\\n",
       "411                    9.8331                    8.0799   \n",
       "426                   43.3029                   53.5871   \n",
       "31                     3.1736                    2.9994   \n",
       "538                    6.0828                   14.0307   \n",
       "468                     3.913                    3.4838   \n",
       "..                        ...                       ...   \n",
       "171                    3.8939                    6.5859   \n",
       "515                    2.7313                   18.7789   \n",
       "622                    5.0762                     3.484   \n",
       "687                    5.6088                   34.6387   \n",
       "187                    6.7973                   26.7017   \n",
       "\n",
       "0   tqwt_kurtosisValue_dec_35 tqwt_kurtosisValue_dec_36  \n",
       "411                    6.9204                    2.6948  \n",
       "426                   40.2744                    93.142  \n",
       "31                     2.9446                    3.8043  \n",
       "538                    10.646                   10.9042  \n",
       "468                    3.0588                    3.2383  \n",
       "..                        ...                       ...  \n",
       "171                    8.7967                   14.1727  \n",
       "515                   22.4631                   83.3482  \n",
       "622                    3.4902                    4.2894  \n",
       "687                   34.4628                   81.7748  \n",
       "187                   24.6198                     61.04  \n",
       "\n",
       "[756 rows x 753 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e99794fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features=data.columns\n",
    "importances=model1.feature_importances_\n",
    "indices = np.argsort(importances)[-20:]  # top 80 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "selected_features=[features[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5abfd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_after_RF=data[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf3d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tqwt_entropy_log_dec_35</th>\n",
       "      <th>tqwt_entropy_shannon_dec_25</th>\n",
       "      <th>std_6th_delta_delta</th>\n",
       "      <th>std_9th_delta_delta</th>\n",
       "      <th>tqwt_TKEO_std_dec_11</th>\n",
       "      <th>tqwt_energy_dec_12</th>\n",
       "      <th>tqwt_stdValue_dec_11</th>\n",
       "      <th>tqwt_energy_dec_27</th>\n",
       "      <th>tqwt_entropy_shannon_dec_12</th>\n",
       "      <th>mean_MFCC_6th_coef</th>\n",
       "      <th>tqwt_minValue_dec_17</th>\n",
       "      <th>tqwt_TKEO_std_dec_19</th>\n",
       "      <th>tqwt_skewnessValue_dec_24</th>\n",
       "      <th>tqwt_energy_dec_18</th>\n",
       "      <th>mean_MFCC_2nd_coef</th>\n",
       "      <th>std_delta_log_energy</th>\n",
       "      <th>tqwt_TKEO_std_dec_12</th>\n",
       "      <th>tqwt_entropy_log_dec_12</th>\n",
       "      <th>tqwt_TKEO_mean_dec_12</th>\n",
       "      <th>std_delta_delta_log_energy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>-2309.7719</td>\n",
       "      <td>189.3635</td>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.01284</td>\n",
       "      <td>0.0028273</td>\n",
       "      <td>0.0073524</td>\n",
       "      <td>0.037692</td>\n",
       "      <td>4.51E-05</td>\n",
       "      <td>157.782</td>\n",
       "      <td>-1.6877</td>\n",
       "      <td>-0.75638</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>0.00047731</td>\n",
       "      <td>0.046431</td>\n",
       "      <td>-1.9717</td>\n",
       "      <td>0.020313</td>\n",
       "      <td>0.0025669</td>\n",
       "      <td>-116365.5415</td>\n",
       "      <td>0.0015677</td>\n",
       "      <td>0.0078168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>-3374.4678</td>\n",
       "      <td>44.7992</td>\n",
       "      <td>0.018965</td>\n",
       "      <td>0.02175</td>\n",
       "      <td>2.85E-06</td>\n",
       "      <td>0.00039806</td>\n",
       "      <td>0.0010009</td>\n",
       "      <td>0.00022453</td>\n",
       "      <td>0.77374</td>\n",
       "      <td>-2.0767</td>\n",
       "      <td>-0.090316</td>\n",
       "      <td>0.00051377</td>\n",
       "      <td>1.13E-18</td>\n",
       "      <td>0.016202</td>\n",
       "      <td>2.8502</td>\n",
       "      <td>0.037457</td>\n",
       "      <td>6.64E-06</td>\n",
       "      <td>-217577.2067</td>\n",
       "      <td>4.02E-06</td>\n",
       "      <td>0.010547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>-1750.1357</td>\n",
       "      <td>193.1674</td>\n",
       "      <td>0.032461</td>\n",
       "      <td>0.020545</td>\n",
       "      <td>4.17E-05</td>\n",
       "      <td>7.90E-05</td>\n",
       "      <td>0.0037313</td>\n",
       "      <td>0.10056</td>\n",
       "      <td>3.5128</td>\n",
       "      <td>-1.8295</td>\n",
       "      <td>-0.19727</td>\n",
       "      <td>0.095021</td>\n",
       "      <td>-0.0011053</td>\n",
       "      <td>0.019801</td>\n",
       "      <td>1.645</td>\n",
       "      <td>0.061864</td>\n",
       "      <td>7.65E-05</td>\n",
       "      <td>-196875.1207</td>\n",
       "      <td>2.71E-05</td>\n",
       "      <td>0.021631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>-2651.5268</td>\n",
       "      <td>162.9883</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.016623</td>\n",
       "      <td>0.00035225</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.011017</td>\n",
       "      <td>0.016544</td>\n",
       "      <td>55.0837</td>\n",
       "      <td>-1.0935</td>\n",
       "      <td>-0.80117</td>\n",
       "      <td>0.30835</td>\n",
       "      <td>-0.0018418</td>\n",
       "      <td>0.3041</td>\n",
       "      <td>0.80108</td>\n",
       "      <td>0.012465</td>\n",
       "      <td>0.00076701</td>\n",
       "      <td>-139538.7389</td>\n",
       "      <td>0.0003785</td>\n",
       "      <td>0.0064803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-1613.738</td>\n",
       "      <td>262.7038</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>0.00051621</td>\n",
       "      <td>0.0048381</td>\n",
       "      <td>0.013267</td>\n",
       "      <td>1.58E-05</td>\n",
       "      <td>128.0801</td>\n",
       "      <td>-2.0709</td>\n",
       "      <td>-0.92812</td>\n",
       "      <td>0.048442</td>\n",
       "      <td>4.23E-05</td>\n",
       "      <td>0.056862</td>\n",
       "      <td>-0.31404</td>\n",
       "      <td>0.018277</td>\n",
       "      <td>0.0012412</td>\n",
       "      <td>-122319.9537</td>\n",
       "      <td>0.00092046</td>\n",
       "      <td>0.0060161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>-2980.4698</td>\n",
       "      <td>192.9394</td>\n",
       "      <td>0.016594</td>\n",
       "      <td>0.01561</td>\n",
       "      <td>0.00024555</td>\n",
       "      <td>0.0022885</td>\n",
       "      <td>0.00974</td>\n",
       "      <td>0.00030573</td>\n",
       "      <td>100.5226</td>\n",
       "      <td>-1.4169</td>\n",
       "      <td>-1.1025</td>\n",
       "      <td>0.13944</td>\n",
       "      <td>3.44E-17</td>\n",
       "      <td>0.28734</td>\n",
       "      <td>1.8894</td>\n",
       "      <td>0.024106</td>\n",
       "      <td>0.0012235</td>\n",
       "      <td>-127009.4995</td>\n",
       "      <td>0.00081241</td>\n",
       "      <td>0.009572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-2404.0607</td>\n",
       "      <td>126.0535</td>\n",
       "      <td>0.01917</td>\n",
       "      <td>0.020261</td>\n",
       "      <td>0.0005164</td>\n",
       "      <td>0.00025798</td>\n",
       "      <td>0.010779</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>12.9105</td>\n",
       "      <td>-0.22431</td>\n",
       "      <td>-0.95856</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>-0.00069716</td>\n",
       "      <td>0.21148</td>\n",
       "      <td>3.0053</td>\n",
       "      <td>0.041855</td>\n",
       "      <td>0.00069316</td>\n",
       "      <td>-173078.0224</td>\n",
       "      <td>0.00011446</td>\n",
       "      <td>0.012932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>-3074.2418</td>\n",
       "      <td>183.054</td>\n",
       "      <td>0.019303</td>\n",
       "      <td>0.016719</td>\n",
       "      <td>5.71E-05</td>\n",
       "      <td>0.00027945</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>0.099148</td>\n",
       "      <td>6.6614</td>\n",
       "      <td>-1.129</td>\n",
       "      <td>-0.51028</td>\n",
       "      <td>0.16404</td>\n",
       "      <td>5.86E-18</td>\n",
       "      <td>0.16844</td>\n",
       "      <td>3.3438</td>\n",
       "      <td>0.030341</td>\n",
       "      <td>0.00012097</td>\n",
       "      <td>-183231.5901</td>\n",
       "      <td>6.10E-05</td>\n",
       "      <td>0.011296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>-3330.9258</td>\n",
       "      <td>59.7806</td>\n",
       "      <td>0.013483</td>\n",
       "      <td>0.012447</td>\n",
       "      <td>7.01E-06</td>\n",
       "      <td>0.0001437</td>\n",
       "      <td>0.0014492</td>\n",
       "      <td>0.15384</td>\n",
       "      <td>1.3815</td>\n",
       "      <td>-1.8186</td>\n",
       "      <td>-0.19089</td>\n",
       "      <td>0.044707</td>\n",
       "      <td>-1.92E-18</td>\n",
       "      <td>0.12229</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.04791</td>\n",
       "      <td>9.98E-06</td>\n",
       "      <td>-207277.2123</td>\n",
       "      <td>5.25E-06</td>\n",
       "      <td>0.016466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>-3524.8058</td>\n",
       "      <td>53.3702</td>\n",
       "      <td>0.030603</td>\n",
       "      <td>0.021715</td>\n",
       "      <td>2.56E-06</td>\n",
       "      <td>0.0024986</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.00059907</td>\n",
       "      <td>1.7404</td>\n",
       "      <td>-1.7861</td>\n",
       "      <td>-0.092289</td>\n",
       "      <td>0.0010992</td>\n",
       "      <td>-0.0012451</td>\n",
       "      <td>0.07818</td>\n",
       "      <td>2.0564</td>\n",
       "      <td>0.047562</td>\n",
       "      <td>1.17E-05</td>\n",
       "      <td>-204267.4228</td>\n",
       "      <td>9.83E-06</td>\n",
       "      <td>0.016568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>756 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0   tqwt_entropy_log_dec_35 tqwt_entropy_shannon_dec_25 std_6th_delta_delta  \\\n",
       "520              -2309.7719                    189.3635            0.018408   \n",
       "254              -3374.4678                     44.7992            0.018965   \n",
       "634              -1750.1357                    193.1674            0.032461   \n",
       "129              -2651.5268                    162.9883            0.020011   \n",
       "90                -1613.738                    262.7038            0.013905   \n",
       "..                      ...                         ...                 ...   \n",
       "618              -2980.4698                    192.9394            0.016594   \n",
       "77               -2404.0607                    126.0535             0.01917   \n",
       "502              -3074.2418                     183.054            0.019303   \n",
       "667              -3330.9258                     59.7806            0.013483   \n",
       "357              -3524.8058                     53.3702            0.030603   \n",
       "\n",
       "0   std_9th_delta_delta tqwt_TKEO_std_dec_11 tqwt_energy_dec_12  \\\n",
       "520             0.01284            0.0028273          0.0073524   \n",
       "254             0.02175             2.85E-06         0.00039806   \n",
       "634            0.020545             4.17E-05           7.90E-05   \n",
       "129            0.016623           0.00035225           0.001609   \n",
       "90             0.012121           0.00051621          0.0048381   \n",
       "..                  ...                  ...                ...   \n",
       "618             0.01561           0.00024555          0.0022885   \n",
       "77             0.020261            0.0005164         0.00025798   \n",
       "502            0.016719             5.71E-05         0.00027945   \n",
       "667            0.012447             7.01E-06          0.0001437   \n",
       "357            0.021715             2.56E-06          0.0024986   \n",
       "\n",
       "0   tqwt_stdValue_dec_11 tqwt_energy_dec_27 tqwt_entropy_shannon_dec_12  \\\n",
       "520             0.037692           4.51E-05                     157.782   \n",
       "254            0.0010009         0.00022453                     0.77374   \n",
       "634            0.0037313            0.10056                      3.5128   \n",
       "129             0.011017           0.016544                     55.0837   \n",
       "90              0.013267           1.58E-05                    128.0801   \n",
       "..                   ...                ...                         ...   \n",
       "618              0.00974         0.00030573                    100.5226   \n",
       "77              0.010779           0.001416                     12.9105   \n",
       "502             0.004917           0.099148                      6.6614   \n",
       "667            0.0014492            0.15384                      1.3815   \n",
       "357             0.001135         0.00059907                      1.7404   \n",
       "\n",
       "0   mean_MFCC_6th_coef tqwt_minValue_dec_17 tqwt_TKEO_std_dec_19  \\\n",
       "520            -1.6877             -0.75638             0.011171   \n",
       "254            -2.0767            -0.090316           0.00051377   \n",
       "634            -1.8295             -0.19727             0.095021   \n",
       "129            -1.0935             -0.80117              0.30835   \n",
       "90             -2.0709             -0.92812             0.048442   \n",
       "..                 ...                  ...                  ...   \n",
       "618            -1.4169              -1.1025              0.13944   \n",
       "77            -0.22431             -0.95856               0.1363   \n",
       "502             -1.129             -0.51028              0.16404   \n",
       "667            -1.8186             -0.19089             0.044707   \n",
       "357            -1.7861            -0.092289            0.0010992   \n",
       "\n",
       "0   tqwt_skewnessValue_dec_24 tqwt_energy_dec_18 mean_MFCC_2nd_coef  \\\n",
       "520                0.00047731           0.046431            -1.9717   \n",
       "254                  1.13E-18           0.016202             2.8502   \n",
       "634                -0.0011053           0.019801              1.645   \n",
       "129                -0.0018418             0.3041            0.80108   \n",
       "90                   4.23E-05           0.056862           -0.31404   \n",
       "..                        ...                ...                ...   \n",
       "618                  3.44E-17            0.28734             1.8894   \n",
       "77                -0.00069716            0.21148             3.0053   \n",
       "502                  5.86E-18            0.16844             3.3438   \n",
       "667                 -1.92E-18            0.12229               1.66   \n",
       "357                -0.0012451            0.07818             2.0564   \n",
       "\n",
       "0   std_delta_log_energy tqwt_TKEO_std_dec_12 tqwt_entropy_log_dec_12  \\\n",
       "520             0.020313            0.0025669            -116365.5415   \n",
       "254             0.037457             6.64E-06            -217577.2067   \n",
       "634             0.061864             7.65E-05            -196875.1207   \n",
       "129             0.012465           0.00076701            -139538.7389   \n",
       "90              0.018277            0.0012412            -122319.9537   \n",
       "..                   ...                  ...                     ...   \n",
       "618             0.024106            0.0012235            -127009.4995   \n",
       "77              0.041855           0.00069316            -173078.0224   \n",
       "502             0.030341           0.00012097            -183231.5901   \n",
       "667              0.04791             9.98E-06            -207277.2123   \n",
       "357             0.047562             1.17E-05            -204267.4228   \n",
       "\n",
       "0   tqwt_TKEO_mean_dec_12 std_delta_delta_log_energy  \n",
       "520             0.0015677                  0.0078168  \n",
       "254              4.02E-06                   0.010547  \n",
       "634              2.71E-05                   0.021631  \n",
       "129             0.0003785                  0.0064803  \n",
       "90             0.00092046                  0.0060161  \n",
       "..                    ...                        ...  \n",
       "618            0.00081241                   0.009572  \n",
       "77             0.00011446                   0.012932  \n",
       "502              6.10E-05                   0.011296  \n",
       "667              5.25E-06                   0.016466  \n",
       "357              9.83E-06                   0.016568  \n",
       "\n",
       "[756 rows x 20 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_after_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f07fd0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520    0\n",
       "254    1\n",
       "634    1\n",
       "129    0\n",
       "90     0\n",
       "      ..\n",
       "618    1\n",
       "77     1\n",
       "502    1\n",
       "667    0\n",
       "357    1\n",
       "Name: class, Length: 756, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd26555",
   "metadata": {},
   "source": [
    "<h1 > Halt and Stop </h1>\n",
    "<h2> PCA Code </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c6e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_1 = PCA(n_components=20)\n",
    "principalComponents = pca_1.fit_transform(data)\n",
    "data_after_RF=pd.DataFrame(principalComponents)\n",
    "data_after_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad11d9",
   "metadata": {},
   "source": [
    "<h2> PCA STOP </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d418924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Percentages are: [5, 10, 15, 20, 25]\n",
      "Splitting Position are: [0, 38, 114, 227, 378, 567]\n"
     ]
    }
   ],
   "source": [
    "# Defining Splitter for the dataset (Common for all)\n",
    "def splitter(len_array,lower_bound, incrementor, start_idx):\n",
    "    final=0\n",
    "    per=[]\n",
    "    sp=[start_idx]\n",
    "    for i in range(lower_bound,100,incrementor):\n",
    "        if final+i>100:\n",
    "            break\n",
    "        per.append(i)\n",
    "        final+=i\n",
    "    for i in per:\n",
    "        len_of_data=round((i/100)*len_array)\n",
    "        sp.append(sp[-1]+len_of_data)\n",
    "    return per,sp\n",
    "\n",
    "lower_bound=5\n",
    "incrementor=5\n",
    "start_idx=0\n",
    "\n",
    "        \n",
    "spliting_percentage, spliting_position=splitter(len(data),lower_bound,incrementor,start_idx)\n",
    "print(\"Splitting Percentages are: {}\".format(spliting_percentage))\n",
    "print(\"Splitting Position are: {}\".format(spliting_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4733af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_experiment=data_after_RF.copy(deep=True)\n",
    "labels_for_experiment=labels.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2030a307",
   "metadata": {},
   "source": [
    "# Apply ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365e3bc9",
   "metadata": {},
   "source": [
    "# ************************************** Logistic Regression **************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c07fb613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best: 0.000000 using {'C': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[  7  35]\n",
      " [  2 108]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.17      0.27        42\n",
      "           1       0.76      0.98      0.85       110\n",
      "\n",
      "    accuracy                           0.76       152\n",
      "   macro avg       0.77      0.57      0.56       152\n",
      "weighted avg       0.76      0.76      0.69       152\n",
      "\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SImple Algorithm\n",
    "# splitting dataset into training and testing part\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    solvers = [\"lbfgs\",\"liblinear\"]\n",
    "    penalty = ['l2']\n",
    "    c_values = [100]\n",
    "    # define grid search\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='f1',error_score=0,)\n",
    "    grid_result = grid_search.fit(X_train, y_train)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "#     print(\"Training set score for logreg_model: %f\" % grid_search.score(X_train , y_train))\n",
    "#     print(\"Testing  set score for logreg_model: %f\" % grid_search.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b4df27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (38, 20)\n",
      "Shape of testing input X_test: (38, 20)\n",
      "Shape of training output Y_train: (38,)\n",
      "Shape of testing output Y_test: (38, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[ 7  3]\n",
      " [ 6 22]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.70      0.61        10\n",
      "           1       0.88      0.79      0.83        28\n",
      "\n",
      "    accuracy                           0.76        38\n",
      "   macro avg       0.71      0.74      0.72        38\n",
      "weighted avg       0.79      0.76      0.77        38\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (76, 20)\n",
      "Shape of testing input X_test: (76, 20)\n",
      "Shape of training output Y_train: (76,)\n",
      "Shape of testing output Y_test: (76, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[ 2 17]\n",
      " [ 0 57]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.11      0.19        19\n",
      "           1       0.77      1.00      0.87        57\n",
      "\n",
      "    accuracy                           0.78        76\n",
      "   macro avg       0.89      0.55      0.53        76\n",
      "weighted avg       0.83      0.78      0.70        76\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (113, 20)\n",
      "Shape of testing input X_test: (113, 20)\n",
      "Shape of training output Y_train: (113,)\n",
      "Shape of testing output Y_test: (113, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[ 1 33]\n",
      " [ 0 79]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.03      0.06        34\n",
      "           1       0.71      1.00      0.83        79\n",
      "\n",
      "    accuracy                           0.71       113\n",
      "   macro avg       0.85      0.51      0.44       113\n",
      "weighted avg       0.79      0.71      0.60       113\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (151, 20)\n",
      "Shape of testing input X_test: (151, 20)\n",
      "Shape of training output Y_train: (151,)\n",
      "Shape of testing output Y_test: (151, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[ 10  36]\n",
      " [  3 102]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.22      0.34        46\n",
      "           1       0.74      0.97      0.84       105\n",
      "\n",
      "    accuracy                           0.74       151\n",
      "   macro avg       0.75      0.59      0.59       151\n",
      "weighted avg       0.75      0.74      0.69       151\n",
      "\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (189, 20)\n",
      "Shape of testing input X_test: (189, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best: 0.000000 using {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "[[  9  48]\n",
      " [  1 131]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.16      0.27        57\n",
      "           1       0.73      0.99      0.84       132\n",
      "\n",
      "    accuracy                           0.74       189\n",
      "   macro avg       0.82      0.58      0.56       189\n",
      "weighted avg       0.78      0.74      0.67       189\n",
      "\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    solvers = [\"lbfgs\",\"liblinear\"]\n",
    "    penalty = ['l2','l1']\n",
    "    c_values = [10]\n",
    "    # define grid search\n",
    "    grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=2, cv=cv, scoring='f1',error_score=0,)\n",
    "    grid_result = grid_search.fit(X_train, Y_train)\n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    means = grid_result.cv_results_['mean_test_score']\n",
    "    stds = grid_result.cv_results_['std_test_score']\n",
    "    params = grid_result.cv_results_['params']\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = grid_search.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201c88f",
   "metadata": {},
   "source": [
    "# ****KNN Classifer ****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1750a5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.730068870523416 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[20 24]\n",
      " [15 93]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.45      0.51        44\n",
      "           1       0.79      0.86      0.83       108\n",
      "\n",
      "    accuracy                           0.74       152\n",
      "   macro avg       0.68      0.66      0.67       152\n",
      "weighted avg       0.73      0.74      0.73       152\n",
      "\n",
      "Training set score for knn_model: 0.842715\n",
      "Testing  set score for knn_model: 0.743421\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = KNeighborsClassifier()\n",
    "    params_grid = [{'n_neighbors': [2,3,4], 'weights' :['uniform'],'leaf_size':[4,5,6,7,8,9],'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "    knn_model = GridSearchCV(model, params_grid, cv=5)\n",
    "    knn_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', knn_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best #neighbors:',knn_model.best_estimator_.n_neighbors,\"\\n\") \n",
    "    print('Best weights:',knn_model.best_estimator_.weights,\"\\n\")\n",
    "    print('Best leaf_size:',knn_model.best_estimator_.leaf_size,\"\\n\")\n",
    "    print('Best algorithm:',knn_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = knn_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for knn_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for knn_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23feab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (38, 20)\n",
      "Shape of testing input X_test: (38, 20)\n",
      "Shape of training output Y_train: (38,)\n",
      "Shape of testing output Y_test: (38, 1)\n",
      "Best score for training data: 0.7607142857142858 \n",
      "\n",
      "Best #neighbors: 2 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[ 9  7]\n",
      " [ 4 18]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.45      0.51        44\n",
      "           1       0.79      0.86      0.83       108\n",
      "\n",
      "    accuracy                           0.74       152\n",
      "   macro avg       0.68      0.66      0.67       152\n",
      "weighted avg       0.73      0.74      0.73       152\n",
      "\n",
      "Training set score for knn_model: 0.894737\n",
      "Testing  set score for knn_model: 0.710526\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (76, 20)\n",
      "Shape of testing input X_test: (76, 20)\n",
      "Shape of training output Y_train: (76,)\n",
      "Shape of testing output Y_test: (76, 1)\n",
      "Best score for training data: 0.6599999999999999 \n",
      "\n",
      "Best #neighbors: 2 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[11  9]\n",
      " [21 35]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.45      0.51        44\n",
      "           1       0.79      0.86      0.83       108\n",
      "\n",
      "    accuracy                           0.74       152\n",
      "   macro avg       0.68      0.66      0.67       152\n",
      "weighted avg       0.73      0.74      0.73       152\n",
      "\n",
      "Training set score for knn_model: 0.855263\n",
      "Testing  set score for knn_model: 0.605263\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (113, 20)\n",
      "Shape of testing input X_test: (113, 20)\n",
      "Shape of training output Y_train: (113,)\n",
      "Shape of testing output Y_test: (113, 1)\n",
      "Best score for training data: 0.7695652173913043 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[ 8 20]\n",
      " [ 6 79]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.45      0.51        44\n",
      "           1       0.79      0.86      0.83       108\n",
      "\n",
      "    accuracy                           0.74       152\n",
      "   macro avg       0.68      0.66      0.67       152\n",
      "weighted avg       0.73      0.74      0.73       152\n",
      "\n",
      "Training set score for knn_model: 0.876106\n",
      "Testing  set score for knn_model: 0.769912\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (151, 20)\n",
      "Shape of testing input X_test: (151, 20)\n",
      "Shape of training output Y_train: (151,)\n",
      "Shape of testing output Y_test: (151, 1)\n",
      "Best score for training data: 0.787956989247312 \n",
      "\n",
      "Best #neighbors: 3 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[14 33]\n",
      " [ 6 98]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.45      0.51        44\n",
      "           1       0.79      0.86      0.83       108\n",
      "\n",
      "    accuracy                           0.74       152\n",
      "   macro avg       0.68      0.66      0.67       152\n",
      "weighted avg       0.73      0.74      0.73       152\n",
      "\n",
      "Training set score for knn_model: 0.874172\n",
      "Testing  set score for knn_model: 0.741722\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (189, 20)\n",
      "Shape of testing input X_test: (189, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.6987197724039829 \n",
      "\n",
      "Best #neighbors: 4 \n",
      "\n",
      "Best weights: uniform \n",
      "\n",
      "Best leaf_size: 4 \n",
      "\n",
      "Best algorithm: auto \n",
      "\n",
      "[[ 31  19]\n",
      " [ 33 106]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.45      0.51        44\n",
      "           1       0.79      0.86      0.83       108\n",
      "\n",
      "    accuracy                           0.74       152\n",
      "   macro avg       0.68      0.66      0.67       152\n",
      "weighted avg       0.73      0.74      0.73       152\n",
      "\n",
      "Training set score for knn_model: 0.804233\n",
      "Testing  set score for knn_model: 0.724868\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    model = KNeighborsClassifier()\n",
    "    params_grid = [{'n_neighbors': [2,3,4], 'weights' :['uniform'],'leaf_size':[4,5,6,7,8,9],'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "    knn_model = GridSearchCV(model, params_grid, cv=5)\n",
    "    knn_model.fit(X_train,Y_train)\n",
    "    print('Best score for training data:', knn_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best #neighbors:',knn_model.best_estimator_.n_neighbors,\"\\n\") \n",
    "    print('Best weights:',knn_model.best_estimator_.weights,\"\\n\")\n",
    "    print('Best leaf_size:',knn_model.best_estimator_.leaf_size,\"\\n\")\n",
    "    print('Best algorithm:',knn_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = knn_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for knn_model: %f\" % final_model.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for knn_model: %f\" % final_model.score(X_test  , Y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b349eed",
   "metadata": {},
   "source": [
    "# ** Support Vector Machine **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fbe345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = SVC()\n",
    "    params_grid = [{'kernel': ['linear'], 'C': [2]}]\n",
    "    svm_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    svm_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "    print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "    print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "    final_model = svm_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for svm_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for svm_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = SVC()\n",
    "    params_grid = [{'kernel': ['linear'], 'C': [100,150]}]\n",
    "    svm_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    svm_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', svm_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best C:',svm_model.best_estimator_.C,\"\\n\") \n",
    "    print('Best Kernel:',svm_model.best_estimator_.kernel,\"\\n\")\n",
    "    print('Best Gamma:',svm_model.best_estimator_.gamma,\"\\n\")\n",
    "    final_model = svm_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for svm_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for svm_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106200ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6060496",
   "metadata": {},
   "source": [
    "# ** Decision Tree **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53ec404b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8096234996633992 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[29 13]\n",
      " [11 99]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.69      0.71        42\n",
      "           1       0.88      0.90      0.89       110\n",
      "\n",
      "    accuracy                           0.84       152\n",
      "   macro avg       0.80      0.80      0.80       152\n",
      "weighted avg       0.84      0.84      0.84       152\n",
      "\n",
      "Training set score for dc_model: 0.985099\n",
      "Testing  set score for dc_model: 0.842105\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = DecisionTreeClassifier()\n",
    "    params_grid = [{'max_depth': [8,9,10,11,12],'random_state':[42]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47cf0be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (38, 20)\n",
      "Shape of testing input X_test: (38, 20)\n",
      "Shape of training output Y_train: (38,)\n",
      "Shape of testing output Y_test: (38, 1)\n",
      "Best score for training data: 0.7606837606837606 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[ 6  5]\n",
      " [ 6 21]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.55      0.52        11\n",
      "           1       0.81      0.78      0.79        27\n",
      "\n",
      "    accuracy                           0.71        38\n",
      "   macro avg       0.65      0.66      0.66        38\n",
      "weighted avg       0.72      0.71      0.71        38\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.710526\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (76, 20)\n",
      "Shape of testing input X_test: (76, 20)\n",
      "Shape of training output Y_train: (76,)\n",
      "Shape of testing output Y_test: (76, 1)\n",
      "Best score for training data: 0.7112820512820512 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[ 9  6]\n",
      " [11 50]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.60      0.51        15\n",
      "           1       0.89      0.82      0.85        61\n",
      "\n",
      "    accuracy                           0.78        76\n",
      "   macro avg       0.67      0.71      0.68        76\n",
      "weighted avg       0.81      0.78      0.79        76\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.776316\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (113, 20)\n",
      "Shape of testing input X_test: (113, 20)\n",
      "Shape of training output Y_train: (113,)\n",
      "Shape of testing output Y_test: (113, 1)\n",
      "Best score for training data: 0.7524893314366999 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[21 14]\n",
      " [11 67]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.60      0.63        35\n",
      "           1       0.83      0.86      0.84        78\n",
      "\n",
      "    accuracy                           0.78       113\n",
      "   macro avg       0.74      0.73      0.73       113\n",
      "weighted avg       0.77      0.78      0.78       113\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.778761\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (151, 20)\n",
      "Shape of testing input X_test: (151, 20)\n",
      "Shape of training output Y_train: (151,)\n",
      "Shape of testing output Y_test: (151, 1)\n",
      "Best score for training data: 0.7486274509803922 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[26 18]\n",
      " [12 95]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.59      0.63        44\n",
      "           1       0.84      0.89      0.86       107\n",
      "\n",
      "    accuracy                           0.80       151\n",
      "   macro avg       0.76      0.74      0.75       151\n",
      "weighted avg       0.80      0.80      0.80       151\n",
      "\n",
      "Training set score for dc_model: 0.986755\n",
      "Testing  set score for dc_model: 0.801325\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (189, 20)\n",
      "Shape of testing input X_test: (189, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.7513227513227513 \n",
      "\n",
      "Best depth: 8 \n",
      "\n",
      "Best #features: None \n",
      "\n",
      "[[ 31  24]\n",
      " [ 15 119]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.56      0.61        55\n",
      "           1       0.83      0.89      0.86       134\n",
      "\n",
      "    accuracy                           0.79       189\n",
      "   macro avg       0.75      0.73      0.74       189\n",
      "weighted avg       0.79      0.79      0.79       189\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.793651\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = DecisionTreeClassifier()\n",
    "    params_grid = [{'max_depth': [8,9,10,11,12],'random_state':[42]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,Y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    Y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , Y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220e996",
   "metadata": {},
   "source": [
    "<h1> **************************************************************************************************************</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6111fc",
   "metadata": {},
   "source": [
    "<h1> AdaBoost </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88665124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.3\n",
      "Best score for training data: 0.81668378702277 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 0.01 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[ 32  31]\n",
      " [ 20 144]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.51      0.56        63\n",
      "           1       0.82      0.88      0.85       164\n",
      "\n",
      "    accuracy                           0.78       227\n",
      "   macro avg       0.72      0.69      0.70       227\n",
      "weighted avg       0.77      0.78      0.77       227\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.775330\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.25\n",
      "Best score for training data: 0.8059964726631393 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[ 33  19]\n",
      " [ 14 123]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.63      0.67        52\n",
      "           1       0.87      0.90      0.88       137\n",
      "\n",
      "    accuracy                           0.83       189\n",
      "   macro avg       0.78      0.77      0.77       189\n",
      "weighted avg       0.82      0.83      0.82       189\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.825397\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8277917343973203 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[ 22  18]\n",
      " [ 12 100]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.55      0.59        40\n",
      "           1       0.85      0.89      0.87       112\n",
      "\n",
      "    accuracy                           0.80       152\n",
      "   macro avg       0.75      0.72      0.73       152\n",
      "weighted avg       0.79      0.80      0.80       152\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.802632\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.15\n",
      "Best score for training data: 0.792834890965732 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[23 10]\n",
      " [11 70]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.70      0.69        33\n",
      "           1       0.88      0.86      0.87        81\n",
      "\n",
      "    accuracy                           0.82       114\n",
      "   macro avg       0.78      0.78      0.78       114\n",
      "weighted avg       0.82      0.82      0.82       114\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.815789\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.1\n",
      "Best score for training data: 0.8146985822515042 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[13  5]\n",
      " [ 6 52]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70        18\n",
      "           1       0.91      0.90      0.90        58\n",
      "\n",
      "    accuracy                           0.86        76\n",
      "   macro avg       0.80      0.81      0.80        76\n",
      "weighted avg       0.86      0.86      0.86        76\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.855263\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.05\n",
      "Best score for training data: 0.8008310088331009 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.01 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[ 8  5]\n",
      " [ 3 22]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.62      0.67        13\n",
      "           1       0.81      0.88      0.85        25\n",
      "\n",
      "    accuracy                           0.79        38\n",
      "   macro avg       0.77      0.75      0.76        38\n",
      "weighted avg       0.78      0.79      0.78        38\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.789474\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.3,0.25,0.2,0.15,0.1,0.05]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = DecisionTreeClassifier(max_features=None)\n",
    "    params_grid = [{'n_estimators':[2,4],'learning_rate':[1.0,1e-1,1e-2],'algorithm':['SAMME.R','SAMME']}]\n",
    "    model_ada=AdaBoostClassifier(model,)\n",
    "    dc_model = GridSearchCV(model_ada, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best estimator:',dc_model.best_estimator_.n_estimators,\"\\n\") \n",
    "    print('Best #learning rate:',dc_model.best_estimator_.learning_rate,\"\\n\")\n",
    "    print('Best algorithm:',dc_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75299f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (38, 20)\n",
      "Shape of testing input X_test: (38, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.844017094017094 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[ 4  3]\n",
      " [10 21]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.57      0.38         7\n",
      "           1       0.88      0.68      0.76        31\n",
      "\n",
      "    accuracy                           0.66        38\n",
      "   macro avg       0.58      0.62      0.57        38\n",
      "weighted avg       0.77      0.66      0.69        38\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.657895\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (76, 20)\n",
      "Shape of testing input X_test: (76, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.7251282051282052 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[11  3]\n",
      " [ 7 55]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.79      0.69        14\n",
      "           1       0.95      0.89      0.92        62\n",
      "\n",
      "    accuracy                           0.87        76\n",
      "   macro avg       0.78      0.84      0.80        76\n",
      "weighted avg       0.89      0.87      0.87        76\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.868421\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (113, 20)\n",
      "Shape of testing input X_test: (113, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.7527264106211474 \n",
      "\n",
      "Best estimator: 4 \n",
      "\n",
      "Best #learning rate: 0.1 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[19 15]\n",
      " [11 68]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.56      0.59        34\n",
      "           1       0.82      0.86      0.84        79\n",
      "\n",
      "    accuracy                           0.77       113\n",
      "   macro avg       0.73      0.71      0.72       113\n",
      "weighted avg       0.76      0.77      0.77       113\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.769912\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (151, 20)\n",
      "Shape of testing input X_test: (151, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.808235294117647 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 0.01 \n",
      "\n",
      "Best algorithm: SAMME.R \n",
      "\n",
      "[[22 21]\n",
      " [14 94]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.51      0.56        43\n",
      "           1       0.82      0.87      0.84       108\n",
      "\n",
      "    accuracy                           0.77       151\n",
      "   macro avg       0.71      0.69      0.70       151\n",
      "weighted avg       0.76      0.77      0.76       151\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.768212\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (189, 20)\n",
      "Shape of testing input X_test: (189, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8148148148148149 \n",
      "\n",
      "Best estimator: 2 \n",
      "\n",
      "Best #learning rate: 1.0 \n",
      "\n",
      "Best algorithm: SAMME \n",
      "\n",
      "[[ 36  18]\n",
      " [ 23 112]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.67      0.64        54\n",
      "           1       0.86      0.83      0.85       135\n",
      "\n",
      "    accuracy                           0.78       189\n",
      "   macro avg       0.74      0.75      0.74       189\n",
      "weighted avg       0.79      0.78      0.79       189\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.783069\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = DecisionTreeClassifier(max_features=None)\n",
    "    params_grid = [{'n_estimators':[2,4],'learning_rate':[1.0,1e-1,1e-2],'algorithm':['SAMME.R','SAMME']}]\n",
    "    model_ada=AdaBoostClassifier(model,)\n",
    "    dc_model = GridSearchCV(model_ada, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best estimator:',dc_model.best_estimator_.n_estimators,\"\\n\") \n",
    "    print('Best #learning rate:',dc_model.best_estimator_.learning_rate,\"\\n\")\n",
    "    print('Best algorithm:',dc_model.best_estimator_.algorithm,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d146a76",
   "metadata": {},
   "source": [
    "# ** Ensemble Learning **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "660d880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "[[  2  36]\n",
      " [  0 114]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.05      0.10        38\n",
      "           1       0.76      1.00      0.86       114\n",
      "\n",
      "    accuracy                           0.76       152\n",
      "   macro avg       0.88      0.53      0.48       152\n",
      "weighted avg       0.82      0.76      0.67       152\n",
      "\n",
      "Training set score for EL: 0.791391\n",
      "Testing  set score for EL: 0.763158\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data_ratio=[0.20]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    estimator = []\n",
    "    # make different combination in the ensemble classfier\n",
    "    estimator.append(('lr',LogisticRegression(solver='saga',penalty='l1',C=1,n_jobs=2)))\n",
    "    estimator.append(('SVC', SVC(gamma ='scale',probability=True,C=100,kernel='linear',)))\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=14,criterion='gini', splitter='best', min_samples_split=17,random_state=42)\n",
    "    model = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=250 ,learning_rate=1,algorithm='SAMME',random_state=7)\n",
    "    estimator.append(('DTC', model))\n",
    "    #estimator.append(('svc_rbf',SVC(gamma ='auto',C=4,kernel='rbf',)))\n",
    "\n",
    "\n",
    "    vot_hard = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "    vot_hard.fit(X_train,y_train)\n",
    "    Y_pred = vot_hard.predict(X_test)\n",
    "    \n",
    "    print(confusion_matrix(y_test,Y_pred))\n",
    "    print(classification_report(y_test,Y_pred))\n",
    "    print(\"Training set score for EL: %f\" % vot_hard.score(X_train , y_train))\n",
    "    print(\"Testing  set score for EL: %f\" % vot_hard.score(X_test  , y_test ))\n",
    "    \n",
    "\n",
    "    score = accuracy_score(y_test, Y_pred)\n",
    "    print(\"Hard Voting Score % d\" % score)\n",
    "    \n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4ebb89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (38, 20)\n",
      "Shape of testing input X_test: (38, 20)\n",
      "Shape of training output Y_train: (38,)\n",
      "Shape of testing output Y_test: (38, 1)\n",
      "[[ 0 12]\n",
      " [ 0 26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.68      1.00      0.81        26\n",
      "\n",
      "    accuracy                           0.68        38\n",
      "   macro avg       0.34      0.50      0.41        38\n",
      "weighted avg       0.47      0.68      0.56        38\n",
      "\n",
      "Training set score for EL: 0.789474\n",
      "Testing  set score for EL: 0.684211\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (76, 20)\n",
      "Shape of testing input X_test: (76, 20)\n",
      "Shape of training output Y_train: (76,)\n",
      "Shape of testing output Y_test: (76, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luhar\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\luhar\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\luhar\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7 10]\n",
      " [ 0 59]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.41      0.58        17\n",
      "           1       0.86      1.00      0.92        59\n",
      "\n",
      "    accuracy                           0.87        76\n",
      "   macro avg       0.93      0.71      0.75        76\n",
      "weighted avg       0.89      0.87      0.85        76\n",
      "\n",
      "Training set score for EL: 0.921053\n",
      "Testing  set score for EL: 0.868421\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (113, 20)\n",
      "Shape of testing input X_test: (113, 20)\n",
      "Shape of training output Y_train: (113,)\n",
      "Shape of testing output Y_test: (113, 1)\n",
      "[[ 1 38]\n",
      " [ 0 74]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.03      0.05        39\n",
      "           1       0.66      1.00      0.80        74\n",
      "\n",
      "    accuracy                           0.66       113\n",
      "   macro avg       0.83      0.51      0.42       113\n",
      "weighted avg       0.78      0.66      0.54       113\n",
      "\n",
      "Training set score for EL: 0.823009\n",
      "Testing  set score for EL: 0.663717\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (151, 20)\n",
      "Shape of testing input X_test: (151, 20)\n",
      "Shape of training output Y_train: (151,)\n",
      "Shape of testing output Y_test: (151, 1)\n",
      "[[21 21]\n",
      " [10 99]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.50      0.58        42\n",
      "           1       0.82      0.91      0.86       109\n",
      "\n",
      "    accuracy                           0.79       151\n",
      "   macro avg       0.75      0.70      0.72       151\n",
      "weighted avg       0.78      0.79      0.78       151\n",
      "\n",
      "Training set score for EL: 0.854305\n",
      "Testing  set score for EL: 0.794702\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (189, 20)\n",
      "Shape of testing input X_test: (189, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "[[  9  36]\n",
      " [  3 141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.20      0.32        45\n",
      "           1       0.80      0.98      0.88       144\n",
      "\n",
      "    accuracy                           0.79       189\n",
      "   macro avg       0.77      0.59      0.60       189\n",
      "weighted avg       0.79      0.79      0.74       189\n",
      "\n",
      "Training set score for EL: 0.830688\n",
      "Testing  set score for EL: 0.793651\n",
      "Hard Voting Score  0\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    Y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    Y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    estimator = []\n",
    "    # make different combination in the ensemble classfier\n",
    "    estimator.append(('lr',LogisticRegression(solver='saga',penalty='l1',C=1,n_jobs=2)))\n",
    "    estimator.append(('SVC', SVC(gamma ='scale',probability=True,C=100,kernel='linear',)))\n",
    "    base_estimator=DecisionTreeClassifier(max_depth=14,criterion='gini', splitter='best', min_samples_split=17,random_state=42)\n",
    "    model = AdaBoostClassifier(base_estimator=base_estimator,n_estimators=250 ,learning_rate=1,algorithm='SAMME',random_state=7)\n",
    "    estimator.append(('DTC', model))\n",
    "    #estimator.append(('svc_rbf',SVC(gamma ='auto',C=4,kernel='rbf',)))\n",
    "\n",
    "\n",
    "    vot_hard = VotingClassifier(estimators = estimator, voting ='soft')\n",
    "    vot_hard.fit(X_train,Y_train)\n",
    "    Y_pred = vot_hard.predict(X_test)\n",
    "    \n",
    "    print(confusion_matrix(Y_test,Y_pred))\n",
    "    print(classification_report(Y_test,Y_pred))\n",
    "    print(\"Training set score for EL: %f\" % vot_hard.score(X_train , Y_train))\n",
    "    print(\"Testing  set score for EL: %f\" % vot_hard.score(X_test  , Y_test ))\n",
    "    \n",
    "\n",
    "    score = accuracy_score(Y_test, Y_pred)\n",
    "    print(\"Hard Voting Score % d\" % score)\n",
    "    \n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdade4a",
   "metadata": {},
   "source": [
    "# ** Random Forest **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e4698ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "\n",
      "For ratio  0.2\n",
      "Best score for training data: 0.8559266374398634 \n",
      "\n",
      "Best depth: 6 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 10 \n",
      "\n",
      "[[ 15  23]\n",
      " [  5 109]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.39      0.52        38\n",
      "           1       0.83      0.96      0.89       114\n",
      "\n",
      "    accuracy                           0.82       152\n",
      "   macro avg       0.79      0.68      0.70       152\n",
      "weighted avg       0.81      0.82      0.79       152\n",
      "\n",
      "Training set score for dc_model: 0.950331\n",
      "Testing  set score for dc_model: 0.815789\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "test_data_ratio=[0.2]\n",
    "for i in test_data_ratio:\n",
    "    print(\"******************Start of iteration******************\\n\")\n",
    "    print(\"For ratio \",i)\n",
    "    X_train, X_test, y_train, y_test=train_test_split(data_after_RF,labels,test_size=i,shuffle=True)\n",
    "    model = RandomForestClassifier()\n",
    "    params_grid = [{'max_depth': [5,6,7],'random_state':[42],'n_jobs':[10,15,20]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #estimators:',dc_model.best_estimator_.n_estimators,\"\\n\")\n",
    "    print('Best jobs:',dc_model.best_estimator_.n_jobs,\"\\n\") \n",
    "    #print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d30292d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Start of iteration******************\n",
      "Splitting percentage is 5\n",
      "Shape of training input X_train: (38, 20)\n",
      "Shape of testing input X_test: (38, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8183760683760685 \n",
      "\n",
      "Best depth: 5 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[ 7  4]\n",
      " [ 1 26]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.64      0.74        11\n",
      "           1       0.87      0.96      0.91        27\n",
      "\n",
      "    accuracy                           0.87        38\n",
      "   macro avg       0.87      0.80      0.82        38\n",
      "weighted avg       0.87      0.87      0.86        38\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.868421\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 10\n",
      "Shape of training input X_train: (76, 20)\n",
      "Shape of testing input X_test: (76, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8948717948717948 \n",
      "\n",
      "Best depth: 5 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[12 16]\n",
      " [ 3 45]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.43      0.56        28\n",
      "           1       0.74      0.94      0.83        48\n",
      "\n",
      "    accuracy                           0.75        76\n",
      "   macro avg       0.77      0.68      0.69        76\n",
      "weighted avg       0.76      0.75      0.73        76\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.750000\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 15\n",
      "Shape of training input X_train: (113, 20)\n",
      "Shape of testing input X_test: (113, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8667614983404457 \n",
      "\n",
      "Best depth: 6 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[11 25]\n",
      " [ 2 75]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.31      0.45        36\n",
      "           1       0.75      0.97      0.85        77\n",
      "\n",
      "    accuracy                           0.76       113\n",
      "   macro avg       0.80      0.64      0.65       113\n",
      "weighted avg       0.78      0.76      0.72       113\n",
      "\n",
      "Training set score for dc_model: 0.982301\n",
      "Testing  set score for dc_model: 0.761062\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 20\n",
      "Shape of training input X_train: (151, 20)\n",
      "Shape of testing input X_test: (151, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8210457516339869 \n",
      "\n",
      "Best depth: 6 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[ 21  10]\n",
      " [  9 111]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.68      0.69        31\n",
      "           1       0.92      0.93      0.92       120\n",
      "\n",
      "    accuracy                           0.87       151\n",
      "   macro avg       0.81      0.80      0.80       151\n",
      "weighted avg       0.87      0.87      0.87       151\n",
      "\n",
      "Training set score for dc_model: 0.993377\n",
      "Testing  set score for dc_model: 0.874172\n",
      "******************End of iteration******************\n",
      "\n",
      "******************Start of iteration******************\n",
      "Splitting percentage is 25\n",
      "Shape of training input X_train: (189, 20)\n",
      "Shape of testing input X_test: (189, 20)\n",
      "Shape of training output Y_train: (189,)\n",
      "Shape of testing output Y_test: (189, 1)\n",
      "Best score for training data: 0.8412698412698413 \n",
      "\n",
      "Best depth: 7 \n",
      "\n",
      "Best #estimators: 100 \n",
      "\n",
      "Best jobs: 2 \n",
      "\n",
      "[[ 27  19]\n",
      " [  6 137]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.59      0.68        46\n",
      "           1       0.88      0.96      0.92       143\n",
      "\n",
      "    accuracy                           0.87       189\n",
      "   macro avg       0.85      0.77      0.80       189\n",
      "weighted avg       0.86      0.87      0.86       189\n",
      "\n",
      "Training set score for dc_model: 1.000000\n",
      "Testing  set score for dc_model: 0.867725\n",
      "******************End of iteration******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting dataset into training and testing part\n",
    "\n",
    "for i in range(len(spliting_position)-1):\n",
    "    print(\"******************Start of iteration******************\")\n",
    "    print(\"Splitting percentage is {}\".format(spliting_percentage[i]))\n",
    "    \n",
    "    X_train=pd.DataFrame(data_for_experiment.iloc[spliting_position[i]:spliting_position[i+1],:])\n",
    "    y_train=labels_for_experiment.iloc[spliting_position[i]:spliting_position[i+1]]\n",
    "    X_test=data_for_experiment.drop(X_train.index, axis=0).sample(len(X_train))\n",
    "    y_test=np.array(pd.DataFrame(labels_for_experiment,index=X_test.index))\n",
    "    \n",
    "    print(\"Shape of training input X_train: {}\".format(X_train.shape))\n",
    "    print(\"Shape of testing input X_test: {}\".format(X_test.shape))\n",
    "    print(\"Shape of training output Y_train: {}\".format(Y_train.shape))\n",
    "    print(\"Shape of testing output Y_test: {}\".format(Y_test.shape))\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    params_grid = [{'max_depth': [5,6,7],'random_state':[42],'n_jobs':[2]}]\n",
    "    dc_model = GridSearchCV(model, params_grid, cv=3)\n",
    "    dc_model.fit(X_train,y_train)\n",
    "    print('Best score for training data:', dc_model.best_score_,\"\\n\") \n",
    "\n",
    "    # View the best parameters for the model found using grid search\n",
    "    print('Best depth:',dc_model.best_estimator_.max_depth,\"\\n\") \n",
    "    print('Best #estimators:',dc_model.best_estimator_.n_estimators,\"\\n\")\n",
    "    print('Best jobs:',dc_model.best_estimator_.n_jobs,\"\\n\") \n",
    "    #print('Best #features:',dc_model.best_estimator_.max_features,\"\\n\")\n",
    "    #print('Best Gamma:',dc_model.best_estimator_.,\"\\n\")\n",
    "    final_model = dc_model.best_estimator_\n",
    "\n",
    "    \n",
    "    #Predict the response for test dataset\n",
    "    y_pred = final_model.predict(X_test)\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"Training set score for dc_model: %f\" % final_model.score(X_train , y_train))\n",
    "    print(\"Testing  set score for dc_model: %f\" % final_model.score(X_test  , y_test ))\n",
    "\n",
    "    print(\"******************End of iteration******************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de26e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75dcf68a",
   "metadata": {},
   "source": [
    "# ...........THANK YOU.........HAPPY CODING......."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
